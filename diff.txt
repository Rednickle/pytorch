diff --git a/build/aten/src/ATen/CPUByteType.cpp b/build/aten/src/ATen/CPUByteType.cpp
index 7041fc052..acd685df7 100644
--- a/build/aten/src/ATen/CPUByteType.cpp
+++ b/build/aten/src/ATen/CPUByteType.cpp
@@ -1841,27 +1841,6 @@ Tensor CPUByteType::_th_cat(TensorList tensors, int64_t dim) const {
     THByteTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
     return self;
 }
-std::tuple<Tensor,Tensor> CPUByteType::_cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const {
-    AT_ERROR("_cudnn_ctc_loss not supported on CPUByteType");
-}
-Tensor CPUByteType::_cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const {
-    AT_ERROR("_cudnn_rnn_flatten_weight not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUByteType::_cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const {
-    AT_ERROR("_cudnn_rnn not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> CPUByteType::_cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const {
-    AT_ERROR("_cudnn_rnn_backward not supported on CPUByteType");
-}
-Tensor CPUByteType::_cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const {
-    AT_ERROR("_cudnn_init_dropout_state not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor> CPUByteType::_fused_dropout(const Tensor & self, double p, Generator * generator) const {
-    AT_ERROR("_fused_dropout not supported on CPUByteType");
-}
-Tensor CPUByteType::_masked_scale(const Tensor & self, const Tensor & mask, double scale) const {
-    AT_ERROR("_masked_scale not supported on CPUByteType");
-}
 Tensor & CPUByteType::abs_(Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_abs__cpu(/* actuals */ self);
@@ -1966,9 +1945,6 @@ Tensor & CPUByteType::s_copy_(Tensor & self, const Tensor & src, bool non_blocki
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_copy__cpu(/* actuals */ self, src, non_blocking);
 }
-Tensor CPUByteType::_s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const {
-    AT_ERROR("_s_copy_from not supported on CPUByteType");
-}
 void CPUByteType::_copy_same_type_(Tensor & self, const Tensor & src) const {
     const OptionalDeviceGuard device_guard(device_of(self));
  at::native::_copy_same_type__cpu(/* actuals */ self, src);
@@ -1989,54 +1965,6 @@ Tensor & CPUByteType::cosh_out(Tensor & out, const Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_cosh_out_cpu(/* actuals */ out, self);
 }
-Tensor CPUByteType::cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const {
-    AT_ERROR("cudnn_affine_grid_generator not supported on CPUByteType");
-}
-Tensor CPUByteType::cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const {
-    AT_ERROR("cudnn_affine_grid_generator_backward not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUByteType::cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
-    AT_ERROR("cudnn_batch_norm not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUByteType::cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
-    AT_ERROR("cudnn_batch_norm_backward not supported on CPUByteType");
-}
-Tensor CPUByteType::cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution not supported on CPUByteType");
-}
-Tensor CPUByteType::cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_backward_input not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUByteType::cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("cudnn_convolution_backward not supported on CPUByteType");
-}
-Tensor CPUByteType::cudnn_convolution_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("cudnn_convolution_backward_bias not supported on CPUByteType");
-}
-Tensor CPUByteType::cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_backward_weight not supported on CPUByteType");
-}
-Tensor CPUByteType::cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUByteType::cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("cudnn_convolution_transpose_backward not supported on CPUByteType");
-}
-Tensor CPUByteType::cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_bias not supported on CPUByteType");
-}
-Tensor CPUByteType::cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_input not supported on CPUByteType");
-}
-Tensor CPUByteType::cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_weight not supported on CPUByteType");
-}
-Tensor CPUByteType::cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const {
-    AT_ERROR("cudnn_grid_sampler not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor> CPUByteType::cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const {
-    AT_ERROR("cudnn_grid_sampler_backward not supported on CPUByteType");
-}
 std::tuple<Tensor,Tensor> CPUByteType::_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const {
     const OptionalDeviceGuard device_guard(device_of(log_probs));
     return at::native::ctc_loss_cpu(/* actuals */ log_probs, targets, input_lengths, target_lengths, blank, zero_infinity);
@@ -2205,51 +2133,6 @@ Tensor CPUByteType::_log_softmax_backward_data(const Tensor & grad_output, const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::log_softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
 }
-std::tuple<Tensor,Tensor,Tensor> CPUByteType::miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
-    AT_ERROR("miopen_batch_norm not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUByteType::miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
-    AT_ERROR("miopen_batch_norm_backward not supported on CPUByteType");
-}
-Tensor CPUByteType::miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution not supported on CPUByteType");
-}
-Tensor CPUByteType::miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_backward_input not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUByteType::miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_convolution_backward not supported on CPUByteType");
-}
-Tensor CPUByteType::miopen_convolution_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("miopen_convolution_backward_bias not supported on CPUByteType");
-}
-Tensor CPUByteType::miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_backward_weight not supported on CPUByteType");
-}
-Tensor CPUByteType::miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUByteType::miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_convolution_transpose_backward not supported on CPUByteType");
-}
-Tensor CPUByteType::miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose_backward_input not supported on CPUByteType");
-}
-Tensor CPUByteType::miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose_backward_weight not supported on CPUByteType");
-}
-Tensor CPUByteType::miopen_depthwise_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution not supported on CPUByteType");
-}
-Tensor CPUByteType::miopen_depthwise_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution_backward_input not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUByteType::miopen_depthwise_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_depthwise_convolution_backward not supported on CPUByteType");
-}
-Tensor CPUByteType::miopen_depthwise_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution_backward_weight not supported on CPUByteType");
-}
 Tensor CPUByteType::narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::narrow_copy_dense(/* actuals */ self, dim, start, length);
@@ -2258,25 +2141,10 @@ std::tuple<Tensor,Tensor,Tensor> CPUByteType::native_batch_norm(const Tensor & i
     const OptionalDeviceGuard device_guard(device_of(input));
     return at::native::batch_norm_cpu(/* actuals */ input, weight, bias, running_mean, running_var, training, momentum, eps);
 }
-std::tuple<Tensor,Tensor> CPUByteType::batch_norm_stats(const Tensor & input, double eps) const {
-    AT_ERROR("batch_norm_stats not supported on CPUByteType");
-}
-Tensor CPUByteType::batch_norm_elemt(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & mean, const Tensor & invstd, double eps) const {
-    AT_ERROR("batch_norm_elemt not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor> CPUByteType::batch_norm_gather_stats(const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & running_mean, const Tensor & running_var, double momentum, double eps, int64_t count) const {
-    AT_ERROR("batch_norm_gather_stats not supported on CPUByteType");
-}
 std::tuple<Tensor,Tensor,Tensor> CPUByteType::native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const {
     const OptionalDeviceGuard device_guard(device_of(grad_out));
     return at::native::batch_norm_backward_cpu(/* actuals */ grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
 }
-std::tuple<Tensor,Tensor,Tensor,Tensor> CPUByteType::batch_norm_backward_reduce(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, bool input_g, bool weight_g, bool bias_g) const {
-    AT_ERROR("batch_norm_backward_reduce not supported on CPUByteType");
-}
-Tensor CPUByteType::batch_norm_backward_elemt(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & weight, const Tensor & mean_dy, const Tensor & mean_dy_xmu) const {
-    AT_ERROR("batch_norm_backward_elemt not supported on CPUByteType");
-}
 std::tuple<Tensor,Tensor> CPUByteType::batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const {
     const OptionalDeviceGuard device_guard(device_of(input));
     return at::native::batch_norm_update_stats_cpu(/* actuals */ input, running_mean, running_var, momentum);
@@ -2353,28 +2221,10 @@ Tensor CPUByteType::_softmax_backward_data(const Tensor & grad_output, const Ten
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
 }
-Tensor & CPUByteType::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
-    AT_ERROR("_sparse_add_out not supported on CPUByteType");
-}
 Tensor & CPUByteType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::add_out_dense_sparse_cpu(/* actuals */ out, self, other, alpha);
 }
-Tensor & CPUByteType::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_div_zerodim_out not supported on CPUByteType");
-}
-Tensor & CPUByteType::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_div_scalar_out not supported on CPUByteType");
-}
-Tensor & CPUByteType::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_out not supported on CPUByteType");
-}
-Tensor & CPUByteType::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_zerodim_out not supported on CPUByteType");
-}
-Tensor & CPUByteType::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_mul_scalar_out not supported on CPUByteType");
-}
 Tensor & CPUByteType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_sspaddmm_out_only_sparse(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2431,12 +2281,6 @@ Tensor CPUByteType::_s_where(const Tensor & condition, const Tensor & self, cons
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_where_cpu(/* actuals */ condition, self, other);
 }
-std::tuple<Tensor,Tensor> CPUByteType::_weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const {
-    AT_ERROR("_weight_norm_cuda_interface not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor> CPUByteType::_weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const {
-    AT_ERROR("_weight_norm_cuda_interface_backward not supported on CPUByteType");
-}
 Tensor CPUByteType::_standard_gamma_grad(const Tensor & self, const Tensor & output) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_standard_gamma_grad_cpu(/* actuals */ self, output);
@@ -2449,27 +2293,6 @@ Tensor CPUByteType::poisson(const Tensor & self, Generator * generator) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_poisson_cpu(/* actuals */ self, generator);
 }
-Tensor CPUByteType::native_norm(const Tensor & self, Scalar p) const {
-    AT_ERROR("native_norm not supported on CPUByteType");
-}
-Tensor CPUByteType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
-    AT_ERROR("_sparse_sum_backward not supported on CPUByteType");
-}
-Tensor CPUByteType::native_clone(const Tensor & self) const {
-    AT_ERROR("native_clone not supported on CPUByteType");
-}
-Tensor & CPUByteType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
-    AT_ERROR("native_resize_as_ not supported on CPUByteType");
-}
-Tensor & CPUByteType::native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow_out not supported on CPUByteType");
-}
-Tensor CPUByteType::native_pow(const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow not supported on CPUByteType");
-}
-Tensor & CPUByteType::native_zero_(Tensor & self) const {
-    AT_ERROR("native_zero_ not supported on CPUByteType");
-}
 Tensor & CPUByteType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_out_sparse_dense_cpu(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2482,64 +2305,10 @@ Tensor & CPUByteType::s_native_addmm_(Tensor & self, const Tensor & mat1, const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_sparse_dense_cpu_(/* actuals */ self, mat1, mat2, beta, alpha);
 }
-Tensor CPUByteType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CPUByteType");
-}
-Tensor CPUByteType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CPUByteType");
-}
-Tensor & CPUByteType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_ not supported on CPUByteType");
-}
-Tensor & CPUByteType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_and_clear_ not supported on CPUByteType");
-}
 Tensor CPUByteType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::sparse_mask_cpu(/* actuals */ self, mask);
 }
-Tensor CPUByteType::to_dense(const Tensor & self) const {
-    AT_ERROR("to_dense not supported on CPUByteType");
-}
-int64_t CPUByteType::sparse_dim(const Tensor & self) const {
-    AT_ERROR("sparse_dim not supported on CPUByteType");
-}
-int64_t CPUByteType::dense_dim(const Tensor & self) const {
-    AT_ERROR("dense_dim not supported on CPUByteType");
-}
-int64_t CPUByteType::_nnz(const Tensor & self) const {
-    AT_ERROR("_nnz not supported on CPUByteType");
-}
-Tensor CPUByteType::coalesce(const Tensor & self) const {
-    AT_ERROR("coalesce not supported on CPUByteType");
-}
-bool CPUByteType::is_coalesced(const Tensor & self) const {
-    AT_ERROR("is_coalesced not supported on CPUByteType");
-}
-Tensor CPUByteType::_indices(const Tensor & self) const {
-    AT_ERROR("_indices not supported on CPUByteType");
-}
-Tensor CPUByteType::_values(const Tensor & self) const {
-    AT_ERROR("_values not supported on CPUByteType");
-}
-Tensor & CPUByteType::_coalesced_(Tensor & self, bool coalesced) const {
-    AT_ERROR("_coalesced_ not supported on CPUByteType");
-}
-Tensor CPUByteType::indices(const Tensor & self) const {
-    AT_ERROR("indices not supported on CPUByteType");
-}
-Tensor CPUByteType::values(const Tensor & self) const {
-    AT_ERROR("values not supported on CPUByteType");
-}
-Tensor & CPUByteType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm_out not supported on CPUByteType");
-}
-Tensor CPUByteType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm not supported on CPUByteType");
-}
-Tensor & CPUByteType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
-    AT_ERROR("copy_sparse_to_sparse_ not supported on CPUByteType");
-}
 Tensor CPUByteType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
@@ -2552,18 +2321,6 @@ Scalar CPUByteType::_local_scalar_dense(const Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_local_scalar_dense_cpu(/* actuals */ self);
 }
-std::tuple<Tensor,Tensor,Tensor> CPUByteType::_thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const {
-    AT_ERROR("_thnn_fused_lstm_cell not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUByteType::_thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const {
-    AT_ERROR("_thnn_fused_lstm_cell_backward not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor> CPUByteType::_thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const {
-    AT_ERROR("_thnn_fused_gru_cell not supported on CPUByteType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUByteType::_thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const {
-    AT_ERROR("_thnn_fused_gru_cell_backward not supported on CPUByteType");
-}
 Tensor & CPUByteType::tril_(Tensor & self, int64_t diagonal) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::tril_cpu_(/* actuals */ self, diagonal);
diff --git a/build/aten/src/ATen/CPUByteType.h b/build/aten/src/ATen/CPUByteType.h
index a953a92a5..cb98974d6 100644
--- a/build/aten/src/ATen/CPUByteType.h
+++ b/build/aten/src/ATen/CPUByteType.h
@@ -220,13 +220,6 @@ struct CPUByteType final : public CPUTypeDefault {
   Tensor _th_alias(const Tensor & self) const override;
   Tensor & _th_cat_out(Tensor & self, TensorList tensors, int64_t dim) const override;
   Tensor _th_cat(TensorList tensors, int64_t dim) const override;
-  std::tuple<Tensor,Tensor> _cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const override;
-  Tensor _cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const override;
-  std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> _cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const override;
-  Tensor _cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const override;
-  std::tuple<Tensor,Tensor> _fused_dropout(const Tensor & self, double p, Generator * generator) const override;
-  Tensor _masked_scale(const Tensor & self, const Tensor & mask, double scale) const override;
   Tensor & abs_(Tensor & self) const override;
   Tensor & abs_out(Tensor & out, const Tensor & self) const override;
   Tensor & acos_(Tensor & self) const override;
@@ -253,28 +246,11 @@ struct CPUByteType final : public CPUTypeDefault {
   Tensor & clamp_min_(Tensor & self, Scalar min) const override;
   Tensor & clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const override;
   Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const override;
-  Tensor _s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const override;
   void _copy_same_type_(Tensor & self, const Tensor & src) const override;
   Tensor & cos_(Tensor & self) const override;
   Tensor & cos_out(Tensor & out, const Tensor & self) const override;
   Tensor & cosh_(Tensor & self) const override;
   Tensor & cosh_out(Tensor & out, const Tensor & self) const override;
-  Tensor cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const override;
-  Tensor cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const override;
-  Tensor cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor cudnn_convolution_backward_bias(const Tensor & grad_output) const override;
-  Tensor cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const override;
-  Tensor cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const override;
-  std::tuple<Tensor,Tensor> cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const override;
   std::tuple<Tensor,Tensor> _ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const override;
   Tensor _ctc_loss_backward(const Tensor & grad, const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, const Tensor & neg_log_likelihood, const Tensor & log_alpha, int64_t blank, bool zero_infinity) const override;
   Tensor embedding_dense_backward(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) const override;
@@ -317,29 +293,9 @@ struct CPUByteType final : public CPUTypeDefault {
   Tensor & logspace_out(Tensor & out, Scalar start, Scalar end, int64_t steps) const override;
   Tensor _log_softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _log_softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const override;
-  Tensor miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_convolution_backward_bias(const Tensor & grad_output) const override;
-  Tensor miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_depthwise_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_depthwise_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_depthwise_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_depthwise_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
   Tensor narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const override;
   std::tuple<Tensor,Tensor,Tensor> native_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps) const override;
-  std::tuple<Tensor,Tensor> batch_norm_stats(const Tensor & input, double eps) const override;
-  Tensor batch_norm_elemt(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & mean, const Tensor & invstd, double eps) const override;
-  std::tuple<Tensor,Tensor> batch_norm_gather_stats(const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & running_mean, const Tensor & running_var, double momentum, double eps, int64_t count) const override;
   std::tuple<Tensor,Tensor,Tensor> native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor> batch_norm_backward_reduce(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, bool input_g, bool weight_g, bool bias_g) const override;
-  Tensor batch_norm_backward_elemt(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & weight, const Tensor & mean_dy, const Tensor & mean_dy_xmu) const override;
   std::tuple<Tensor,Tensor> batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const override;
   Tensor & randperm_out(Tensor & out, int64_t n, Generator * generator) const override;
   Tensor & range_out(Tensor & out, Scalar start, Scalar end, Scalar step) const override;
@@ -359,13 +315,7 @@ struct CPUByteType final : public CPUTypeDefault {
   Tensor & sinh_out(Tensor & out, const Tensor & self) const override;
   Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  Tensor & _sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const override;
   Tensor & _sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const override;
-  Tensor & _sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
-  Tensor & _sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
   Tensor & sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & sqrt_(Tensor & self) const override;
   Tensor & sqrt_out(Tensor & out, const Tensor & self) const override;
@@ -380,47 +330,16 @@ struct CPUByteType final : public CPUTypeDefault {
   std::tuple<Tensor,Tensor> _unique(const Tensor & self, bool sorted, bool return_inverse) const override;
   std::tuple<Tensor,Tensor> _unique_dim(const Tensor & self, int64_t dim, bool sorted, bool return_inverse) const override;
   Tensor _s_where(const Tensor & condition, const Tensor & self, const Tensor & other) const override;
-  std::tuple<Tensor,Tensor> _weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const override;
-  std::tuple<Tensor,Tensor> _weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const override;
   Tensor _standard_gamma_grad(const Tensor & self, const Tensor & output) const override;
   Tensor _standard_gamma(const Tensor & self, Generator * generator) const override;
   Tensor poisson(const Tensor & self, Generator * generator) const override;
-  Tensor native_norm(const Tensor & self, Scalar p) const override;
-  Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const override;
-  Tensor native_clone(const Tensor & self) const override;
-  Tensor & native_resize_as_(Tensor & self, const Tensor & the_template) const override;
-  Tensor & native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const override;
-  Tensor native_pow(const Tensor & self, Scalar exponent) const override;
-  Tensor & native_zero_(Tensor & self) const override;
   Tensor & s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
-  Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const override;
-  Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const override;
-  Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
-  Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
   Tensor sparse_mask(const Tensor & self, SparseTensorRef mask) const override;
-  Tensor to_dense(const Tensor & self) const override;
-  int64_t sparse_dim(const Tensor & self) const override;
-  int64_t dense_dim(const Tensor & self) const override;
-  int64_t _nnz(const Tensor & self) const override;
-  Tensor coalesce(const Tensor & self) const override;
-  bool is_coalesced(const Tensor & self) const override;
-  Tensor _indices(const Tensor & self) const override;
-  Tensor _values(const Tensor & self) const override;
-  Tensor & _coalesced_(Tensor & self, bool coalesced) const override;
-  Tensor indices(const Tensor & self) const override;
-  Tensor values(const Tensor & self) const override;
-  Tensor & hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor hspmm(const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor to_sparse(const Tensor & self, int64_t sparse_dim) const override;
   Tensor to_sparse(const Tensor & self) const override;
   Scalar _local_scalar_dense(const Tensor & self) const override;
-  std::tuple<Tensor,Tensor,Tensor> _thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const override;
-  std::tuple<Tensor,Tensor> _thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const override;
   Tensor & tril_(Tensor & self, int64_t diagonal) const override;
   Tensor & triu_(Tensor & self, int64_t diagonal) const override;
   Tensor & lerp_(Tensor & self, const Tensor & end, Scalar weight) const override;
diff --git a/build/aten/src/ATen/CPUCharType.cpp b/build/aten/src/ATen/CPUCharType.cpp
index e29c35995..e65d3cacd 100644
--- a/build/aten/src/ATen/CPUCharType.cpp
+++ b/build/aten/src/ATen/CPUCharType.cpp
@@ -1841,27 +1841,6 @@ Tensor CPUCharType::_th_cat(TensorList tensors, int64_t dim) const {
     THCharTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
     return self;
 }
-std::tuple<Tensor,Tensor> CPUCharType::_cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const {
-    AT_ERROR("_cudnn_ctc_loss not supported on CPUCharType");
-}
-Tensor CPUCharType::_cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const {
-    AT_ERROR("_cudnn_rnn_flatten_weight not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUCharType::_cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const {
-    AT_ERROR("_cudnn_rnn not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> CPUCharType::_cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const {
-    AT_ERROR("_cudnn_rnn_backward not supported on CPUCharType");
-}
-Tensor CPUCharType::_cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const {
-    AT_ERROR("_cudnn_init_dropout_state not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor> CPUCharType::_fused_dropout(const Tensor & self, double p, Generator * generator) const {
-    AT_ERROR("_fused_dropout not supported on CPUCharType");
-}
-Tensor CPUCharType::_masked_scale(const Tensor & self, const Tensor & mask, double scale) const {
-    AT_ERROR("_masked_scale not supported on CPUCharType");
-}
 Tensor & CPUCharType::abs_(Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_abs__cpu(/* actuals */ self);
@@ -1966,9 +1945,6 @@ Tensor & CPUCharType::s_copy_(Tensor & self, const Tensor & src, bool non_blocki
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_copy__cpu(/* actuals */ self, src, non_blocking);
 }
-Tensor CPUCharType::_s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const {
-    AT_ERROR("_s_copy_from not supported on CPUCharType");
-}
 void CPUCharType::_copy_same_type_(Tensor & self, const Tensor & src) const {
     const OptionalDeviceGuard device_guard(device_of(self));
  at::native::_copy_same_type__cpu(/* actuals */ self, src);
@@ -1989,54 +1965,6 @@ Tensor & CPUCharType::cosh_out(Tensor & out, const Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_cosh_out_cpu(/* actuals */ out, self);
 }
-Tensor CPUCharType::cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const {
-    AT_ERROR("cudnn_affine_grid_generator not supported on CPUCharType");
-}
-Tensor CPUCharType::cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const {
-    AT_ERROR("cudnn_affine_grid_generator_backward not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUCharType::cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
-    AT_ERROR("cudnn_batch_norm not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUCharType::cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
-    AT_ERROR("cudnn_batch_norm_backward not supported on CPUCharType");
-}
-Tensor CPUCharType::cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution not supported on CPUCharType");
-}
-Tensor CPUCharType::cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_backward_input not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUCharType::cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("cudnn_convolution_backward not supported on CPUCharType");
-}
-Tensor CPUCharType::cudnn_convolution_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("cudnn_convolution_backward_bias not supported on CPUCharType");
-}
-Tensor CPUCharType::cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_backward_weight not supported on CPUCharType");
-}
-Tensor CPUCharType::cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUCharType::cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("cudnn_convolution_transpose_backward not supported on CPUCharType");
-}
-Tensor CPUCharType::cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_bias not supported on CPUCharType");
-}
-Tensor CPUCharType::cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_input not supported on CPUCharType");
-}
-Tensor CPUCharType::cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_weight not supported on CPUCharType");
-}
-Tensor CPUCharType::cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const {
-    AT_ERROR("cudnn_grid_sampler not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor> CPUCharType::cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const {
-    AT_ERROR("cudnn_grid_sampler_backward not supported on CPUCharType");
-}
 std::tuple<Tensor,Tensor> CPUCharType::_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const {
     const OptionalDeviceGuard device_guard(device_of(log_probs));
     return at::native::ctc_loss_cpu(/* actuals */ log_probs, targets, input_lengths, target_lengths, blank, zero_infinity);
@@ -2205,51 +2133,6 @@ Tensor CPUCharType::_log_softmax_backward_data(const Tensor & grad_output, const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::log_softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
 }
-std::tuple<Tensor,Tensor,Tensor> CPUCharType::miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
-    AT_ERROR("miopen_batch_norm not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUCharType::miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
-    AT_ERROR("miopen_batch_norm_backward not supported on CPUCharType");
-}
-Tensor CPUCharType::miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution not supported on CPUCharType");
-}
-Tensor CPUCharType::miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_backward_input not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUCharType::miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_convolution_backward not supported on CPUCharType");
-}
-Tensor CPUCharType::miopen_convolution_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("miopen_convolution_backward_bias not supported on CPUCharType");
-}
-Tensor CPUCharType::miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_backward_weight not supported on CPUCharType");
-}
-Tensor CPUCharType::miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUCharType::miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_convolution_transpose_backward not supported on CPUCharType");
-}
-Tensor CPUCharType::miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose_backward_input not supported on CPUCharType");
-}
-Tensor CPUCharType::miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose_backward_weight not supported on CPUCharType");
-}
-Tensor CPUCharType::miopen_depthwise_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution not supported on CPUCharType");
-}
-Tensor CPUCharType::miopen_depthwise_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution_backward_input not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUCharType::miopen_depthwise_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_depthwise_convolution_backward not supported on CPUCharType");
-}
-Tensor CPUCharType::miopen_depthwise_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution_backward_weight not supported on CPUCharType");
-}
 Tensor CPUCharType::narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::narrow_copy_dense(/* actuals */ self, dim, start, length);
@@ -2258,25 +2141,10 @@ std::tuple<Tensor,Tensor,Tensor> CPUCharType::native_batch_norm(const Tensor & i
     const OptionalDeviceGuard device_guard(device_of(input));
     return at::native::batch_norm_cpu(/* actuals */ input, weight, bias, running_mean, running_var, training, momentum, eps);
 }
-std::tuple<Tensor,Tensor> CPUCharType::batch_norm_stats(const Tensor & input, double eps) const {
-    AT_ERROR("batch_norm_stats not supported on CPUCharType");
-}
-Tensor CPUCharType::batch_norm_elemt(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & mean, const Tensor & invstd, double eps) const {
-    AT_ERROR("batch_norm_elemt not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor> CPUCharType::batch_norm_gather_stats(const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & running_mean, const Tensor & running_var, double momentum, double eps, int64_t count) const {
-    AT_ERROR("batch_norm_gather_stats not supported on CPUCharType");
-}
 std::tuple<Tensor,Tensor,Tensor> CPUCharType::native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const {
     const OptionalDeviceGuard device_guard(device_of(grad_out));
     return at::native::batch_norm_backward_cpu(/* actuals */ grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
 }
-std::tuple<Tensor,Tensor,Tensor,Tensor> CPUCharType::batch_norm_backward_reduce(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, bool input_g, bool weight_g, bool bias_g) const {
-    AT_ERROR("batch_norm_backward_reduce not supported on CPUCharType");
-}
-Tensor CPUCharType::batch_norm_backward_elemt(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & weight, const Tensor & mean_dy, const Tensor & mean_dy_xmu) const {
-    AT_ERROR("batch_norm_backward_elemt not supported on CPUCharType");
-}
 std::tuple<Tensor,Tensor> CPUCharType::batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const {
     const OptionalDeviceGuard device_guard(device_of(input));
     return at::native::batch_norm_update_stats_cpu(/* actuals */ input, running_mean, running_var, momentum);
@@ -2353,28 +2221,10 @@ Tensor CPUCharType::_softmax_backward_data(const Tensor & grad_output, const Ten
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
 }
-Tensor & CPUCharType::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
-    AT_ERROR("_sparse_add_out not supported on CPUCharType");
-}
 Tensor & CPUCharType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::add_out_dense_sparse_cpu(/* actuals */ out, self, other, alpha);
 }
-Tensor & CPUCharType::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_div_zerodim_out not supported on CPUCharType");
-}
-Tensor & CPUCharType::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_div_scalar_out not supported on CPUCharType");
-}
-Tensor & CPUCharType::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_out not supported on CPUCharType");
-}
-Tensor & CPUCharType::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_zerodim_out not supported on CPUCharType");
-}
-Tensor & CPUCharType::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_mul_scalar_out not supported on CPUCharType");
-}
 Tensor & CPUCharType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_sspaddmm_out_only_sparse(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2431,12 +2281,6 @@ Tensor CPUCharType::_s_where(const Tensor & condition, const Tensor & self, cons
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_where_cpu(/* actuals */ condition, self, other);
 }
-std::tuple<Tensor,Tensor> CPUCharType::_weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const {
-    AT_ERROR("_weight_norm_cuda_interface not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor> CPUCharType::_weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const {
-    AT_ERROR("_weight_norm_cuda_interface_backward not supported on CPUCharType");
-}
 Tensor CPUCharType::_standard_gamma_grad(const Tensor & self, const Tensor & output) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_standard_gamma_grad_cpu(/* actuals */ self, output);
@@ -2449,27 +2293,6 @@ Tensor CPUCharType::poisson(const Tensor & self, Generator * generator) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_poisson_cpu(/* actuals */ self, generator);
 }
-Tensor CPUCharType::native_norm(const Tensor & self, Scalar p) const {
-    AT_ERROR("native_norm not supported on CPUCharType");
-}
-Tensor CPUCharType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
-    AT_ERROR("_sparse_sum_backward not supported on CPUCharType");
-}
-Tensor CPUCharType::native_clone(const Tensor & self) const {
-    AT_ERROR("native_clone not supported on CPUCharType");
-}
-Tensor & CPUCharType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
-    AT_ERROR("native_resize_as_ not supported on CPUCharType");
-}
-Tensor & CPUCharType::native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow_out not supported on CPUCharType");
-}
-Tensor CPUCharType::native_pow(const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow not supported on CPUCharType");
-}
-Tensor & CPUCharType::native_zero_(Tensor & self) const {
-    AT_ERROR("native_zero_ not supported on CPUCharType");
-}
 Tensor & CPUCharType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_out_sparse_dense_cpu(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2482,64 +2305,10 @@ Tensor & CPUCharType::s_native_addmm_(Tensor & self, const Tensor & mat1, const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_sparse_dense_cpu_(/* actuals */ self, mat1, mat2, beta, alpha);
 }
-Tensor CPUCharType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CPUCharType");
-}
-Tensor CPUCharType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CPUCharType");
-}
-Tensor & CPUCharType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_ not supported on CPUCharType");
-}
-Tensor & CPUCharType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_and_clear_ not supported on CPUCharType");
-}
 Tensor CPUCharType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::sparse_mask_cpu(/* actuals */ self, mask);
 }
-Tensor CPUCharType::to_dense(const Tensor & self) const {
-    AT_ERROR("to_dense not supported on CPUCharType");
-}
-int64_t CPUCharType::sparse_dim(const Tensor & self) const {
-    AT_ERROR("sparse_dim not supported on CPUCharType");
-}
-int64_t CPUCharType::dense_dim(const Tensor & self) const {
-    AT_ERROR("dense_dim not supported on CPUCharType");
-}
-int64_t CPUCharType::_nnz(const Tensor & self) const {
-    AT_ERROR("_nnz not supported on CPUCharType");
-}
-Tensor CPUCharType::coalesce(const Tensor & self) const {
-    AT_ERROR("coalesce not supported on CPUCharType");
-}
-bool CPUCharType::is_coalesced(const Tensor & self) const {
-    AT_ERROR("is_coalesced not supported on CPUCharType");
-}
-Tensor CPUCharType::_indices(const Tensor & self) const {
-    AT_ERROR("_indices not supported on CPUCharType");
-}
-Tensor CPUCharType::_values(const Tensor & self) const {
-    AT_ERROR("_values not supported on CPUCharType");
-}
-Tensor & CPUCharType::_coalesced_(Tensor & self, bool coalesced) const {
-    AT_ERROR("_coalesced_ not supported on CPUCharType");
-}
-Tensor CPUCharType::indices(const Tensor & self) const {
-    AT_ERROR("indices not supported on CPUCharType");
-}
-Tensor CPUCharType::values(const Tensor & self) const {
-    AT_ERROR("values not supported on CPUCharType");
-}
-Tensor & CPUCharType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm_out not supported on CPUCharType");
-}
-Tensor CPUCharType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm not supported on CPUCharType");
-}
-Tensor & CPUCharType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
-    AT_ERROR("copy_sparse_to_sparse_ not supported on CPUCharType");
-}
 Tensor CPUCharType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
@@ -2552,18 +2321,6 @@ Scalar CPUCharType::_local_scalar_dense(const Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_local_scalar_dense_cpu(/* actuals */ self);
 }
-std::tuple<Tensor,Tensor,Tensor> CPUCharType::_thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const {
-    AT_ERROR("_thnn_fused_lstm_cell not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUCharType::_thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const {
-    AT_ERROR("_thnn_fused_lstm_cell_backward not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor> CPUCharType::_thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const {
-    AT_ERROR("_thnn_fused_gru_cell not supported on CPUCharType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUCharType::_thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const {
-    AT_ERROR("_thnn_fused_gru_cell_backward not supported on CPUCharType");
-}
 Tensor & CPUCharType::tril_(Tensor & self, int64_t diagonal) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::tril_cpu_(/* actuals */ self, diagonal);
diff --git a/build/aten/src/ATen/CPUCharType.h b/build/aten/src/ATen/CPUCharType.h
index 310f2f82c..3787b2233 100644
--- a/build/aten/src/ATen/CPUCharType.h
+++ b/build/aten/src/ATen/CPUCharType.h
@@ -220,13 +220,6 @@ struct CPUCharType final : public CPUTypeDefault {
   Tensor _th_alias(const Tensor & self) const override;
   Tensor & _th_cat_out(Tensor & self, TensorList tensors, int64_t dim) const override;
   Tensor _th_cat(TensorList tensors, int64_t dim) const override;
-  std::tuple<Tensor,Tensor> _cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const override;
-  Tensor _cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const override;
-  std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> _cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const override;
-  Tensor _cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const override;
-  std::tuple<Tensor,Tensor> _fused_dropout(const Tensor & self, double p, Generator * generator) const override;
-  Tensor _masked_scale(const Tensor & self, const Tensor & mask, double scale) const override;
   Tensor & abs_(Tensor & self) const override;
   Tensor & abs_out(Tensor & out, const Tensor & self) const override;
   Tensor & acos_(Tensor & self) const override;
@@ -253,28 +246,11 @@ struct CPUCharType final : public CPUTypeDefault {
   Tensor & clamp_min_(Tensor & self, Scalar min) const override;
   Tensor & clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const override;
   Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const override;
-  Tensor _s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const override;
   void _copy_same_type_(Tensor & self, const Tensor & src) const override;
   Tensor & cos_(Tensor & self) const override;
   Tensor & cos_out(Tensor & out, const Tensor & self) const override;
   Tensor & cosh_(Tensor & self) const override;
   Tensor & cosh_out(Tensor & out, const Tensor & self) const override;
-  Tensor cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const override;
-  Tensor cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const override;
-  Tensor cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor cudnn_convolution_backward_bias(const Tensor & grad_output) const override;
-  Tensor cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const override;
-  Tensor cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const override;
-  std::tuple<Tensor,Tensor> cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const override;
   std::tuple<Tensor,Tensor> _ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const override;
   Tensor _ctc_loss_backward(const Tensor & grad, const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, const Tensor & neg_log_likelihood, const Tensor & log_alpha, int64_t blank, bool zero_infinity) const override;
   Tensor embedding_dense_backward(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) const override;
@@ -317,29 +293,9 @@ struct CPUCharType final : public CPUTypeDefault {
   Tensor & logspace_out(Tensor & out, Scalar start, Scalar end, int64_t steps) const override;
   Tensor _log_softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _log_softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const override;
-  Tensor miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_convolution_backward_bias(const Tensor & grad_output) const override;
-  Tensor miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_depthwise_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_depthwise_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_depthwise_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_depthwise_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
   Tensor narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const override;
   std::tuple<Tensor,Tensor,Tensor> native_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps) const override;
-  std::tuple<Tensor,Tensor> batch_norm_stats(const Tensor & input, double eps) const override;
-  Tensor batch_norm_elemt(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & mean, const Tensor & invstd, double eps) const override;
-  std::tuple<Tensor,Tensor> batch_norm_gather_stats(const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & running_mean, const Tensor & running_var, double momentum, double eps, int64_t count) const override;
   std::tuple<Tensor,Tensor,Tensor> native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor> batch_norm_backward_reduce(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, bool input_g, bool weight_g, bool bias_g) const override;
-  Tensor batch_norm_backward_elemt(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & weight, const Tensor & mean_dy, const Tensor & mean_dy_xmu) const override;
   std::tuple<Tensor,Tensor> batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const override;
   Tensor & randperm_out(Tensor & out, int64_t n, Generator * generator) const override;
   Tensor & range_out(Tensor & out, Scalar start, Scalar end, Scalar step) const override;
@@ -359,13 +315,7 @@ struct CPUCharType final : public CPUTypeDefault {
   Tensor & sinh_out(Tensor & out, const Tensor & self) const override;
   Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  Tensor & _sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const override;
   Tensor & _sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const override;
-  Tensor & _sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
-  Tensor & _sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
   Tensor & sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & sqrt_(Tensor & self) const override;
   Tensor & sqrt_out(Tensor & out, const Tensor & self) const override;
@@ -380,47 +330,16 @@ struct CPUCharType final : public CPUTypeDefault {
   std::tuple<Tensor,Tensor> _unique(const Tensor & self, bool sorted, bool return_inverse) const override;
   std::tuple<Tensor,Tensor> _unique_dim(const Tensor & self, int64_t dim, bool sorted, bool return_inverse) const override;
   Tensor _s_where(const Tensor & condition, const Tensor & self, const Tensor & other) const override;
-  std::tuple<Tensor,Tensor> _weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const override;
-  std::tuple<Tensor,Tensor> _weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const override;
   Tensor _standard_gamma_grad(const Tensor & self, const Tensor & output) const override;
   Tensor _standard_gamma(const Tensor & self, Generator * generator) const override;
   Tensor poisson(const Tensor & self, Generator * generator) const override;
-  Tensor native_norm(const Tensor & self, Scalar p) const override;
-  Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const override;
-  Tensor native_clone(const Tensor & self) const override;
-  Tensor & native_resize_as_(Tensor & self, const Tensor & the_template) const override;
-  Tensor & native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const override;
-  Tensor native_pow(const Tensor & self, Scalar exponent) const override;
-  Tensor & native_zero_(Tensor & self) const override;
   Tensor & s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
-  Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const override;
-  Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const override;
-  Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
-  Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
   Tensor sparse_mask(const Tensor & self, SparseTensorRef mask) const override;
-  Tensor to_dense(const Tensor & self) const override;
-  int64_t sparse_dim(const Tensor & self) const override;
-  int64_t dense_dim(const Tensor & self) const override;
-  int64_t _nnz(const Tensor & self) const override;
-  Tensor coalesce(const Tensor & self) const override;
-  bool is_coalesced(const Tensor & self) const override;
-  Tensor _indices(const Tensor & self) const override;
-  Tensor _values(const Tensor & self) const override;
-  Tensor & _coalesced_(Tensor & self, bool coalesced) const override;
-  Tensor indices(const Tensor & self) const override;
-  Tensor values(const Tensor & self) const override;
-  Tensor & hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor hspmm(const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor to_sparse(const Tensor & self, int64_t sparse_dim) const override;
   Tensor to_sparse(const Tensor & self) const override;
   Scalar _local_scalar_dense(const Tensor & self) const override;
-  std::tuple<Tensor,Tensor,Tensor> _thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const override;
-  std::tuple<Tensor,Tensor> _thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const override;
   Tensor & tril_(Tensor & self, int64_t diagonal) const override;
   Tensor & triu_(Tensor & self, int64_t diagonal) const override;
   Tensor & lerp_(Tensor & self, const Tensor & end, Scalar weight) const override;
diff --git a/build/aten/src/ATen/CPUDoubleType.cpp b/build/aten/src/ATen/CPUDoubleType.cpp
index 7aa7ddd6d..04e514a3a 100644
--- a/build/aten/src/ATen/CPUDoubleType.cpp
+++ b/build/aten/src/ATen/CPUDoubleType.cpp
@@ -4818,27 +4818,6 @@ Tensor CPUDoubleType::_thnn_im2col_backward(const Tensor & grad_output, IntArray
     grad_input_->maybe_zero_dim(false);
     return grad_input;
 }
-std::tuple<Tensor,Tensor> CPUDoubleType::_cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const {
-    AT_ERROR("_cudnn_ctc_loss not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::_cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const {
-    AT_ERROR("_cudnn_rnn_flatten_weight not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUDoubleType::_cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const {
-    AT_ERROR("_cudnn_rnn not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> CPUDoubleType::_cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const {
-    AT_ERROR("_cudnn_rnn_backward not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::_cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const {
-    AT_ERROR("_cudnn_init_dropout_state not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor> CPUDoubleType::_fused_dropout(const Tensor & self, double p, Generator * generator) const {
-    AT_ERROR("_fused_dropout not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::_masked_scale(const Tensor & self, const Tensor & mask, double scale) const {
-    AT_ERROR("_masked_scale not supported on CPUDoubleType");
-}
 Tensor & CPUDoubleType::abs_(Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_abs__cpu(/* actuals */ self);
@@ -4943,9 +4922,6 @@ Tensor & CPUDoubleType::s_copy_(Tensor & self, const Tensor & src, bool non_bloc
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_copy__cpu(/* actuals */ self, src, non_blocking);
 }
-Tensor CPUDoubleType::_s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const {
-    AT_ERROR("_s_copy_from not supported on CPUDoubleType");
-}
 void CPUDoubleType::_copy_same_type_(Tensor & self, const Tensor & src) const {
     const OptionalDeviceGuard device_guard(device_of(self));
  at::native::_copy_same_type__cpu(/* actuals */ self, src);
@@ -4966,54 +4942,6 @@ Tensor & CPUDoubleType::cosh_out(Tensor & out, const Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_cosh_out_cpu(/* actuals */ out, self);
 }
-Tensor CPUDoubleType::cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const {
-    AT_ERROR("cudnn_affine_grid_generator not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const {
-    AT_ERROR("cudnn_affine_grid_generator_backward not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUDoubleType::cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
-    AT_ERROR("cudnn_batch_norm not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUDoubleType::cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
-    AT_ERROR("cudnn_batch_norm_backward not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_backward_input not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUDoubleType::cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("cudnn_convolution_backward not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::cudnn_convolution_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("cudnn_convolution_backward_bias not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_backward_weight not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUDoubleType::cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("cudnn_convolution_transpose_backward not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_bias not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_input not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_weight not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const {
-    AT_ERROR("cudnn_grid_sampler not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor> CPUDoubleType::cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const {
-    AT_ERROR("cudnn_grid_sampler_backward not supported on CPUDoubleType");
-}
 std::tuple<Tensor,Tensor> CPUDoubleType::_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const {
     const OptionalDeviceGuard device_guard(device_of(log_probs));
     return at::native::ctc_loss_cpu(/* actuals */ log_probs, targets, input_lengths, target_lengths, blank, zero_infinity);
@@ -5182,51 +5110,6 @@ Tensor CPUDoubleType::_log_softmax_backward_data(const Tensor & grad_output, con
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::log_softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
 }
-std::tuple<Tensor,Tensor,Tensor> CPUDoubleType::miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
-    AT_ERROR("miopen_batch_norm not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUDoubleType::miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
-    AT_ERROR("miopen_batch_norm_backward not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_backward_input not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUDoubleType::miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_convolution_backward not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::miopen_convolution_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("miopen_convolution_backward_bias not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_backward_weight not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUDoubleType::miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_convolution_transpose_backward not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose_backward_input not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose_backward_weight not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::miopen_depthwise_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::miopen_depthwise_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution_backward_input not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUDoubleType::miopen_depthwise_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_depthwise_convolution_backward not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::miopen_depthwise_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution_backward_weight not supported on CPUDoubleType");
-}
 Tensor CPUDoubleType::narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::narrow_copy_dense(/* actuals */ self, dim, start, length);
@@ -5235,25 +5118,10 @@ std::tuple<Tensor,Tensor,Tensor> CPUDoubleType::native_batch_norm(const Tensor &
     const OptionalDeviceGuard device_guard(device_of(input));
     return at::native::batch_norm_cpu(/* actuals */ input, weight, bias, running_mean, running_var, training, momentum, eps);
 }
-std::tuple<Tensor,Tensor> CPUDoubleType::batch_norm_stats(const Tensor & input, double eps) const {
-    AT_ERROR("batch_norm_stats not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::batch_norm_elemt(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & mean, const Tensor & invstd, double eps) const {
-    AT_ERROR("batch_norm_elemt not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor> CPUDoubleType::batch_norm_gather_stats(const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & running_mean, const Tensor & running_var, double momentum, double eps, int64_t count) const {
-    AT_ERROR("batch_norm_gather_stats not supported on CPUDoubleType");
-}
 std::tuple<Tensor,Tensor,Tensor> CPUDoubleType::native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const {
     const OptionalDeviceGuard device_guard(device_of(grad_out));
     return at::native::batch_norm_backward_cpu(/* actuals */ grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
 }
-std::tuple<Tensor,Tensor,Tensor,Tensor> CPUDoubleType::batch_norm_backward_reduce(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, bool input_g, bool weight_g, bool bias_g) const {
-    AT_ERROR("batch_norm_backward_reduce not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::batch_norm_backward_elemt(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & weight, const Tensor & mean_dy, const Tensor & mean_dy_xmu) const {
-    AT_ERROR("batch_norm_backward_elemt not supported on CPUDoubleType");
-}
 std::tuple<Tensor,Tensor> CPUDoubleType::batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const {
     const OptionalDeviceGuard device_guard(device_of(input));
     return at::native::batch_norm_update_stats_cpu(/* actuals */ input, running_mean, running_var, momentum);
@@ -5330,28 +5198,10 @@ Tensor CPUDoubleType::_softmax_backward_data(const Tensor & grad_output, const T
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
 }
-Tensor & CPUDoubleType::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
-    AT_ERROR("_sparse_add_out not supported on CPUDoubleType");
-}
 Tensor & CPUDoubleType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::add_out_dense_sparse_cpu(/* actuals */ out, self, other, alpha);
 }
-Tensor & CPUDoubleType::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_div_zerodim_out not supported on CPUDoubleType");
-}
-Tensor & CPUDoubleType::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_div_scalar_out not supported on CPUDoubleType");
-}
-Tensor & CPUDoubleType::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_out not supported on CPUDoubleType");
-}
-Tensor & CPUDoubleType::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_zerodim_out not supported on CPUDoubleType");
-}
-Tensor & CPUDoubleType::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_mul_scalar_out not supported on CPUDoubleType");
-}
 Tensor & CPUDoubleType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_sspaddmm_out_only_sparse(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -5408,12 +5258,6 @@ Tensor CPUDoubleType::_s_where(const Tensor & condition, const Tensor & self, co
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_where_cpu(/* actuals */ condition, self, other);
 }
-std::tuple<Tensor,Tensor> CPUDoubleType::_weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const {
-    AT_ERROR("_weight_norm_cuda_interface not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor> CPUDoubleType::_weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const {
-    AT_ERROR("_weight_norm_cuda_interface_backward not supported on CPUDoubleType");
-}
 Tensor CPUDoubleType::_standard_gamma_grad(const Tensor & self, const Tensor & output) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_standard_gamma_grad_cpu(/* actuals */ self, output);
@@ -5426,27 +5270,6 @@ Tensor CPUDoubleType::poisson(const Tensor & self, Generator * generator) const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_poisson_cpu(/* actuals */ self, generator);
 }
-Tensor CPUDoubleType::native_norm(const Tensor & self, Scalar p) const {
-    AT_ERROR("native_norm not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
-    AT_ERROR("_sparse_sum_backward not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::native_clone(const Tensor & self) const {
-    AT_ERROR("native_clone not supported on CPUDoubleType");
-}
-Tensor & CPUDoubleType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
-    AT_ERROR("native_resize_as_ not supported on CPUDoubleType");
-}
-Tensor & CPUDoubleType::native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow_out not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::native_pow(const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow not supported on CPUDoubleType");
-}
-Tensor & CPUDoubleType::native_zero_(Tensor & self) const {
-    AT_ERROR("native_zero_ not supported on CPUDoubleType");
-}
 Tensor & CPUDoubleType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_out_sparse_dense_cpu(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -5459,64 +5282,10 @@ Tensor & CPUDoubleType::s_native_addmm_(Tensor & self, const Tensor & mat1, cons
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_sparse_dense_cpu_(/* actuals */ self, mat1, mat2, beta, alpha);
 }
-Tensor CPUDoubleType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CPUDoubleType");
-}
-Tensor & CPUDoubleType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_ not supported on CPUDoubleType");
-}
-Tensor & CPUDoubleType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_and_clear_ not supported on CPUDoubleType");
-}
 Tensor CPUDoubleType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::sparse_mask_cpu(/* actuals */ self, mask);
 }
-Tensor CPUDoubleType::to_dense(const Tensor & self) const {
-    AT_ERROR("to_dense not supported on CPUDoubleType");
-}
-int64_t CPUDoubleType::sparse_dim(const Tensor & self) const {
-    AT_ERROR("sparse_dim not supported on CPUDoubleType");
-}
-int64_t CPUDoubleType::dense_dim(const Tensor & self) const {
-    AT_ERROR("dense_dim not supported on CPUDoubleType");
-}
-int64_t CPUDoubleType::_nnz(const Tensor & self) const {
-    AT_ERROR("_nnz not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::coalesce(const Tensor & self) const {
-    AT_ERROR("coalesce not supported on CPUDoubleType");
-}
-bool CPUDoubleType::is_coalesced(const Tensor & self) const {
-    AT_ERROR("is_coalesced not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::_indices(const Tensor & self) const {
-    AT_ERROR("_indices not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::_values(const Tensor & self) const {
-    AT_ERROR("_values not supported on CPUDoubleType");
-}
-Tensor & CPUDoubleType::_coalesced_(Tensor & self, bool coalesced) const {
-    AT_ERROR("_coalesced_ not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::indices(const Tensor & self) const {
-    AT_ERROR("indices not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::values(const Tensor & self) const {
-    AT_ERROR("values not supported on CPUDoubleType");
-}
-Tensor & CPUDoubleType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm_out not supported on CPUDoubleType");
-}
-Tensor CPUDoubleType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm not supported on CPUDoubleType");
-}
-Tensor & CPUDoubleType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
-    AT_ERROR("copy_sparse_to_sparse_ not supported on CPUDoubleType");
-}
 Tensor CPUDoubleType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
@@ -5529,18 +5298,6 @@ Scalar CPUDoubleType::_local_scalar_dense(const Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_local_scalar_dense_cpu(/* actuals */ self);
 }
-std::tuple<Tensor,Tensor,Tensor> CPUDoubleType::_thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const {
-    AT_ERROR("_thnn_fused_lstm_cell not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUDoubleType::_thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const {
-    AT_ERROR("_thnn_fused_lstm_cell_backward not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor> CPUDoubleType::_thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const {
-    AT_ERROR("_thnn_fused_gru_cell not supported on CPUDoubleType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUDoubleType::_thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const {
-    AT_ERROR("_thnn_fused_gru_cell_backward not supported on CPUDoubleType");
-}
 Tensor & CPUDoubleType::tril_(Tensor & self, int64_t diagonal) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::tril_cpu_(/* actuals */ self, diagonal);
diff --git a/build/aten/src/ATen/CPUDoubleType.h b/build/aten/src/ATen/CPUDoubleType.h
index 66dac672b..38401b779 100644
--- a/build/aten/src/ATen/CPUDoubleType.h
+++ b/build/aten/src/ATen/CPUDoubleType.h
@@ -473,13 +473,6 @@ struct CPUDoubleType final : public CPUTypeDefault {
   Tensor _thnn_im2col_forward(const Tensor & self, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const override;
   Tensor & _thnn_im2col_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef input_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const override;
   Tensor _thnn_im2col_backward(const Tensor & grad_output, IntArrayRef input_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const override;
-  std::tuple<Tensor,Tensor> _cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const override;
-  Tensor _cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const override;
-  std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> _cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const override;
-  Tensor _cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const override;
-  std::tuple<Tensor,Tensor> _fused_dropout(const Tensor & self, double p, Generator * generator) const override;
-  Tensor _masked_scale(const Tensor & self, const Tensor & mask, double scale) const override;
   Tensor & abs_(Tensor & self) const override;
   Tensor & abs_out(Tensor & out, const Tensor & self) const override;
   Tensor & acos_(Tensor & self) const override;
@@ -506,28 +499,11 @@ struct CPUDoubleType final : public CPUTypeDefault {
   Tensor & clamp_min_(Tensor & self, Scalar min) const override;
   Tensor & clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const override;
   Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const override;
-  Tensor _s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const override;
   void _copy_same_type_(Tensor & self, const Tensor & src) const override;
   Tensor & cos_(Tensor & self) const override;
   Tensor & cos_out(Tensor & out, const Tensor & self) const override;
   Tensor & cosh_(Tensor & self) const override;
   Tensor & cosh_out(Tensor & out, const Tensor & self) const override;
-  Tensor cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const override;
-  Tensor cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const override;
-  Tensor cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor cudnn_convolution_backward_bias(const Tensor & grad_output) const override;
-  Tensor cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const override;
-  Tensor cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const override;
-  std::tuple<Tensor,Tensor> cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const override;
   std::tuple<Tensor,Tensor> _ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const override;
   Tensor _ctc_loss_backward(const Tensor & grad, const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, const Tensor & neg_log_likelihood, const Tensor & log_alpha, int64_t blank, bool zero_infinity) const override;
   Tensor embedding_dense_backward(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) const override;
@@ -570,29 +546,9 @@ struct CPUDoubleType final : public CPUTypeDefault {
   Tensor & logspace_out(Tensor & out, Scalar start, Scalar end, int64_t steps) const override;
   Tensor _log_softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _log_softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const override;
-  Tensor miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_convolution_backward_bias(const Tensor & grad_output) const override;
-  Tensor miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_depthwise_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_depthwise_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_depthwise_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_depthwise_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
   Tensor narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const override;
   std::tuple<Tensor,Tensor,Tensor> native_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps) const override;
-  std::tuple<Tensor,Tensor> batch_norm_stats(const Tensor & input, double eps) const override;
-  Tensor batch_norm_elemt(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & mean, const Tensor & invstd, double eps) const override;
-  std::tuple<Tensor,Tensor> batch_norm_gather_stats(const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & running_mean, const Tensor & running_var, double momentum, double eps, int64_t count) const override;
   std::tuple<Tensor,Tensor,Tensor> native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor> batch_norm_backward_reduce(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, bool input_g, bool weight_g, bool bias_g) const override;
-  Tensor batch_norm_backward_elemt(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & weight, const Tensor & mean_dy, const Tensor & mean_dy_xmu) const override;
   std::tuple<Tensor,Tensor> batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const override;
   Tensor & randperm_out(Tensor & out, int64_t n, Generator * generator) const override;
   Tensor & range_out(Tensor & out, Scalar start, Scalar end, Scalar step) const override;
@@ -612,13 +568,7 @@ struct CPUDoubleType final : public CPUTypeDefault {
   Tensor & sinh_out(Tensor & out, const Tensor & self) const override;
   Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  Tensor & _sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const override;
   Tensor & _sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const override;
-  Tensor & _sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
-  Tensor & _sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
   Tensor & sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & sqrt_(Tensor & self) const override;
   Tensor & sqrt_out(Tensor & out, const Tensor & self) const override;
@@ -633,47 +583,16 @@ struct CPUDoubleType final : public CPUTypeDefault {
   std::tuple<Tensor,Tensor> _unique(const Tensor & self, bool sorted, bool return_inverse) const override;
   std::tuple<Tensor,Tensor> _unique_dim(const Tensor & self, int64_t dim, bool sorted, bool return_inverse) const override;
   Tensor _s_where(const Tensor & condition, const Tensor & self, const Tensor & other) const override;
-  std::tuple<Tensor,Tensor> _weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const override;
-  std::tuple<Tensor,Tensor> _weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const override;
   Tensor _standard_gamma_grad(const Tensor & self, const Tensor & output) const override;
   Tensor _standard_gamma(const Tensor & self, Generator * generator) const override;
   Tensor poisson(const Tensor & self, Generator * generator) const override;
-  Tensor native_norm(const Tensor & self, Scalar p) const override;
-  Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const override;
-  Tensor native_clone(const Tensor & self) const override;
-  Tensor & native_resize_as_(Tensor & self, const Tensor & the_template) const override;
-  Tensor & native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const override;
-  Tensor native_pow(const Tensor & self, Scalar exponent) const override;
-  Tensor & native_zero_(Tensor & self) const override;
   Tensor & s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
-  Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const override;
-  Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const override;
-  Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
-  Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
   Tensor sparse_mask(const Tensor & self, SparseTensorRef mask) const override;
-  Tensor to_dense(const Tensor & self) const override;
-  int64_t sparse_dim(const Tensor & self) const override;
-  int64_t dense_dim(const Tensor & self) const override;
-  int64_t _nnz(const Tensor & self) const override;
-  Tensor coalesce(const Tensor & self) const override;
-  bool is_coalesced(const Tensor & self) const override;
-  Tensor _indices(const Tensor & self) const override;
-  Tensor _values(const Tensor & self) const override;
-  Tensor & _coalesced_(Tensor & self, bool coalesced) const override;
-  Tensor indices(const Tensor & self) const override;
-  Tensor values(const Tensor & self) const override;
-  Tensor & hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor hspmm(const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor to_sparse(const Tensor & self, int64_t sparse_dim) const override;
   Tensor to_sparse(const Tensor & self) const override;
   Scalar _local_scalar_dense(const Tensor & self) const override;
-  std::tuple<Tensor,Tensor,Tensor> _thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const override;
-  std::tuple<Tensor,Tensor> _thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const override;
   Tensor & tril_(Tensor & self, int64_t diagonal) const override;
   Tensor & triu_(Tensor & self, int64_t diagonal) const override;
   Tensor & lerp_(Tensor & self, const Tensor & end, Scalar weight) const override;
diff --git a/build/aten/src/ATen/CPUFloatType.cpp b/build/aten/src/ATen/CPUFloatType.cpp
index e5dbb6086..d898b01e5 100644
--- a/build/aten/src/ATen/CPUFloatType.cpp
+++ b/build/aten/src/ATen/CPUFloatType.cpp
@@ -4818,27 +4818,6 @@ Tensor CPUFloatType::_thnn_im2col_backward(const Tensor & grad_output, IntArrayR
     grad_input_->maybe_zero_dim(false);
     return grad_input;
 }
-std::tuple<Tensor,Tensor> CPUFloatType::_cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const {
-    AT_ERROR("_cudnn_ctc_loss not supported on CPUFloatType");
-}
-Tensor CPUFloatType::_cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const {
-    AT_ERROR("_cudnn_rnn_flatten_weight not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUFloatType::_cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const {
-    AT_ERROR("_cudnn_rnn not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> CPUFloatType::_cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const {
-    AT_ERROR("_cudnn_rnn_backward not supported on CPUFloatType");
-}
-Tensor CPUFloatType::_cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const {
-    AT_ERROR("_cudnn_init_dropout_state not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor> CPUFloatType::_fused_dropout(const Tensor & self, double p, Generator * generator) const {
-    AT_ERROR("_fused_dropout not supported on CPUFloatType");
-}
-Tensor CPUFloatType::_masked_scale(const Tensor & self, const Tensor & mask, double scale) const {
-    AT_ERROR("_masked_scale not supported on CPUFloatType");
-}
 Tensor & CPUFloatType::abs_(Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_abs__cpu(/* actuals */ self);
@@ -4943,9 +4922,6 @@ Tensor & CPUFloatType::s_copy_(Tensor & self, const Tensor & src, bool non_block
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_copy__cpu(/* actuals */ self, src, non_blocking);
 }
-Tensor CPUFloatType::_s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const {
-    AT_ERROR("_s_copy_from not supported on CPUFloatType");
-}
 void CPUFloatType::_copy_same_type_(Tensor & self, const Tensor & src) const {
     const OptionalDeviceGuard device_guard(device_of(self));
  at::native::_copy_same_type__cpu(/* actuals */ self, src);
@@ -4966,54 +4942,6 @@ Tensor & CPUFloatType::cosh_out(Tensor & out, const Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_cosh_out_cpu(/* actuals */ out, self);
 }
-Tensor CPUFloatType::cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const {
-    AT_ERROR("cudnn_affine_grid_generator not supported on CPUFloatType");
-}
-Tensor CPUFloatType::cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const {
-    AT_ERROR("cudnn_affine_grid_generator_backward not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUFloatType::cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
-    AT_ERROR("cudnn_batch_norm not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUFloatType::cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
-    AT_ERROR("cudnn_batch_norm_backward not supported on CPUFloatType");
-}
-Tensor CPUFloatType::cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution not supported on CPUFloatType");
-}
-Tensor CPUFloatType::cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_backward_input not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUFloatType::cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("cudnn_convolution_backward not supported on CPUFloatType");
-}
-Tensor CPUFloatType::cudnn_convolution_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("cudnn_convolution_backward_bias not supported on CPUFloatType");
-}
-Tensor CPUFloatType::cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_backward_weight not supported on CPUFloatType");
-}
-Tensor CPUFloatType::cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUFloatType::cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("cudnn_convolution_transpose_backward not supported on CPUFloatType");
-}
-Tensor CPUFloatType::cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_bias not supported on CPUFloatType");
-}
-Tensor CPUFloatType::cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_input not supported on CPUFloatType");
-}
-Tensor CPUFloatType::cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_weight not supported on CPUFloatType");
-}
-Tensor CPUFloatType::cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const {
-    AT_ERROR("cudnn_grid_sampler not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor> CPUFloatType::cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const {
-    AT_ERROR("cudnn_grid_sampler_backward not supported on CPUFloatType");
-}
 std::tuple<Tensor,Tensor> CPUFloatType::_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const {
     const OptionalDeviceGuard device_guard(device_of(log_probs));
     return at::native::ctc_loss_cpu(/* actuals */ log_probs, targets, input_lengths, target_lengths, blank, zero_infinity);
@@ -5182,51 +5110,6 @@ Tensor CPUFloatType::_log_softmax_backward_data(const Tensor & grad_output, cons
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::log_softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
 }
-std::tuple<Tensor,Tensor,Tensor> CPUFloatType::miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
-    AT_ERROR("miopen_batch_norm not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUFloatType::miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
-    AT_ERROR("miopen_batch_norm_backward not supported on CPUFloatType");
-}
-Tensor CPUFloatType::miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution not supported on CPUFloatType");
-}
-Tensor CPUFloatType::miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_backward_input not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUFloatType::miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_convolution_backward not supported on CPUFloatType");
-}
-Tensor CPUFloatType::miopen_convolution_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("miopen_convolution_backward_bias not supported on CPUFloatType");
-}
-Tensor CPUFloatType::miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_backward_weight not supported on CPUFloatType");
-}
-Tensor CPUFloatType::miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUFloatType::miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_convolution_transpose_backward not supported on CPUFloatType");
-}
-Tensor CPUFloatType::miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose_backward_input not supported on CPUFloatType");
-}
-Tensor CPUFloatType::miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose_backward_weight not supported on CPUFloatType");
-}
-Tensor CPUFloatType::miopen_depthwise_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution not supported on CPUFloatType");
-}
-Tensor CPUFloatType::miopen_depthwise_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution_backward_input not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUFloatType::miopen_depthwise_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_depthwise_convolution_backward not supported on CPUFloatType");
-}
-Tensor CPUFloatType::miopen_depthwise_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution_backward_weight not supported on CPUFloatType");
-}
 Tensor CPUFloatType::narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::narrow_copy_dense(/* actuals */ self, dim, start, length);
@@ -5235,25 +5118,10 @@ std::tuple<Tensor,Tensor,Tensor> CPUFloatType::native_batch_norm(const Tensor &
     const OptionalDeviceGuard device_guard(device_of(input));
     return at::native::batch_norm_cpu(/* actuals */ input, weight, bias, running_mean, running_var, training, momentum, eps);
 }
-std::tuple<Tensor,Tensor> CPUFloatType::batch_norm_stats(const Tensor & input, double eps) const {
-    AT_ERROR("batch_norm_stats not supported on CPUFloatType");
-}
-Tensor CPUFloatType::batch_norm_elemt(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & mean, const Tensor & invstd, double eps) const {
-    AT_ERROR("batch_norm_elemt not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor> CPUFloatType::batch_norm_gather_stats(const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & running_mean, const Tensor & running_var, double momentum, double eps, int64_t count) const {
-    AT_ERROR("batch_norm_gather_stats not supported on CPUFloatType");
-}
 std::tuple<Tensor,Tensor,Tensor> CPUFloatType::native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const {
     const OptionalDeviceGuard device_guard(device_of(grad_out));
     return at::native::batch_norm_backward_cpu(/* actuals */ grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
 }
-std::tuple<Tensor,Tensor,Tensor,Tensor> CPUFloatType::batch_norm_backward_reduce(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, bool input_g, bool weight_g, bool bias_g) const {
-    AT_ERROR("batch_norm_backward_reduce not supported on CPUFloatType");
-}
-Tensor CPUFloatType::batch_norm_backward_elemt(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & weight, const Tensor & mean_dy, const Tensor & mean_dy_xmu) const {
-    AT_ERROR("batch_norm_backward_elemt not supported on CPUFloatType");
-}
 std::tuple<Tensor,Tensor> CPUFloatType::batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const {
     const OptionalDeviceGuard device_guard(device_of(input));
     return at::native::batch_norm_update_stats_cpu(/* actuals */ input, running_mean, running_var, momentum);
@@ -5330,28 +5198,10 @@ Tensor CPUFloatType::_softmax_backward_data(const Tensor & grad_output, const Te
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
 }
-Tensor & CPUFloatType::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
-    AT_ERROR("_sparse_add_out not supported on CPUFloatType");
-}
 Tensor & CPUFloatType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::add_out_dense_sparse_cpu(/* actuals */ out, self, other, alpha);
 }
-Tensor & CPUFloatType::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_div_zerodim_out not supported on CPUFloatType");
-}
-Tensor & CPUFloatType::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_div_scalar_out not supported on CPUFloatType");
-}
-Tensor & CPUFloatType::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_out not supported on CPUFloatType");
-}
-Tensor & CPUFloatType::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_zerodim_out not supported on CPUFloatType");
-}
-Tensor & CPUFloatType::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_mul_scalar_out not supported on CPUFloatType");
-}
 Tensor & CPUFloatType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_sspaddmm_out_only_sparse(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -5408,12 +5258,6 @@ Tensor CPUFloatType::_s_where(const Tensor & condition, const Tensor & self, con
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_where_cpu(/* actuals */ condition, self, other);
 }
-std::tuple<Tensor,Tensor> CPUFloatType::_weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const {
-    AT_ERROR("_weight_norm_cuda_interface not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor> CPUFloatType::_weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const {
-    AT_ERROR("_weight_norm_cuda_interface_backward not supported on CPUFloatType");
-}
 Tensor CPUFloatType::_standard_gamma_grad(const Tensor & self, const Tensor & output) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_standard_gamma_grad_cpu(/* actuals */ self, output);
@@ -5426,27 +5270,6 @@ Tensor CPUFloatType::poisson(const Tensor & self, Generator * generator) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_poisson_cpu(/* actuals */ self, generator);
 }
-Tensor CPUFloatType::native_norm(const Tensor & self, Scalar p) const {
-    AT_ERROR("native_norm not supported on CPUFloatType");
-}
-Tensor CPUFloatType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
-    AT_ERROR("_sparse_sum_backward not supported on CPUFloatType");
-}
-Tensor CPUFloatType::native_clone(const Tensor & self) const {
-    AT_ERROR("native_clone not supported on CPUFloatType");
-}
-Tensor & CPUFloatType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
-    AT_ERROR("native_resize_as_ not supported on CPUFloatType");
-}
-Tensor & CPUFloatType::native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow_out not supported on CPUFloatType");
-}
-Tensor CPUFloatType::native_pow(const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow not supported on CPUFloatType");
-}
-Tensor & CPUFloatType::native_zero_(Tensor & self) const {
-    AT_ERROR("native_zero_ not supported on CPUFloatType");
-}
 Tensor & CPUFloatType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_out_sparse_dense_cpu(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -5459,64 +5282,10 @@ Tensor & CPUFloatType::s_native_addmm_(Tensor & self, const Tensor & mat1, const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_sparse_dense_cpu_(/* actuals */ self, mat1, mat2, beta, alpha);
 }
-Tensor CPUFloatType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CPUFloatType");
-}
-Tensor CPUFloatType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CPUFloatType");
-}
-Tensor & CPUFloatType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_ not supported on CPUFloatType");
-}
-Tensor & CPUFloatType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_and_clear_ not supported on CPUFloatType");
-}
 Tensor CPUFloatType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::sparse_mask_cpu(/* actuals */ self, mask);
 }
-Tensor CPUFloatType::to_dense(const Tensor & self) const {
-    AT_ERROR("to_dense not supported on CPUFloatType");
-}
-int64_t CPUFloatType::sparse_dim(const Tensor & self) const {
-    AT_ERROR("sparse_dim not supported on CPUFloatType");
-}
-int64_t CPUFloatType::dense_dim(const Tensor & self) const {
-    AT_ERROR("dense_dim not supported on CPUFloatType");
-}
-int64_t CPUFloatType::_nnz(const Tensor & self) const {
-    AT_ERROR("_nnz not supported on CPUFloatType");
-}
-Tensor CPUFloatType::coalesce(const Tensor & self) const {
-    AT_ERROR("coalesce not supported on CPUFloatType");
-}
-bool CPUFloatType::is_coalesced(const Tensor & self) const {
-    AT_ERROR("is_coalesced not supported on CPUFloatType");
-}
-Tensor CPUFloatType::_indices(const Tensor & self) const {
-    AT_ERROR("_indices not supported on CPUFloatType");
-}
-Tensor CPUFloatType::_values(const Tensor & self) const {
-    AT_ERROR("_values not supported on CPUFloatType");
-}
-Tensor & CPUFloatType::_coalesced_(Tensor & self, bool coalesced) const {
-    AT_ERROR("_coalesced_ not supported on CPUFloatType");
-}
-Tensor CPUFloatType::indices(const Tensor & self) const {
-    AT_ERROR("indices not supported on CPUFloatType");
-}
-Tensor CPUFloatType::values(const Tensor & self) const {
-    AT_ERROR("values not supported on CPUFloatType");
-}
-Tensor & CPUFloatType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm_out not supported on CPUFloatType");
-}
-Tensor CPUFloatType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm not supported on CPUFloatType");
-}
-Tensor & CPUFloatType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
-    AT_ERROR("copy_sparse_to_sparse_ not supported on CPUFloatType");
-}
 Tensor CPUFloatType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
@@ -5529,18 +5298,6 @@ Scalar CPUFloatType::_local_scalar_dense(const Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_local_scalar_dense_cpu(/* actuals */ self);
 }
-std::tuple<Tensor,Tensor,Tensor> CPUFloatType::_thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const {
-    AT_ERROR("_thnn_fused_lstm_cell not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUFloatType::_thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const {
-    AT_ERROR("_thnn_fused_lstm_cell_backward not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor> CPUFloatType::_thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const {
-    AT_ERROR("_thnn_fused_gru_cell not supported on CPUFloatType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUFloatType::_thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const {
-    AT_ERROR("_thnn_fused_gru_cell_backward not supported on CPUFloatType");
-}
 Tensor & CPUFloatType::tril_(Tensor & self, int64_t diagonal) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::tril_cpu_(/* actuals */ self, diagonal);
diff --git a/build/aten/src/ATen/CPUFloatType.h b/build/aten/src/ATen/CPUFloatType.h
index 224dce4b3..51480ebee 100644
--- a/build/aten/src/ATen/CPUFloatType.h
+++ b/build/aten/src/ATen/CPUFloatType.h
@@ -473,13 +473,6 @@ struct CPUFloatType final : public CPUTypeDefault {
   Tensor _thnn_im2col_forward(const Tensor & self, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const override;
   Tensor & _thnn_im2col_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef input_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const override;
   Tensor _thnn_im2col_backward(const Tensor & grad_output, IntArrayRef input_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const override;
-  std::tuple<Tensor,Tensor> _cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const override;
-  Tensor _cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const override;
-  std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> _cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const override;
-  Tensor _cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const override;
-  std::tuple<Tensor,Tensor> _fused_dropout(const Tensor & self, double p, Generator * generator) const override;
-  Tensor _masked_scale(const Tensor & self, const Tensor & mask, double scale) const override;
   Tensor & abs_(Tensor & self) const override;
   Tensor & abs_out(Tensor & out, const Tensor & self) const override;
   Tensor & acos_(Tensor & self) const override;
@@ -506,28 +499,11 @@ struct CPUFloatType final : public CPUTypeDefault {
   Tensor & clamp_min_(Tensor & self, Scalar min) const override;
   Tensor & clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const override;
   Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const override;
-  Tensor _s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const override;
   void _copy_same_type_(Tensor & self, const Tensor & src) const override;
   Tensor & cos_(Tensor & self) const override;
   Tensor & cos_out(Tensor & out, const Tensor & self) const override;
   Tensor & cosh_(Tensor & self) const override;
   Tensor & cosh_out(Tensor & out, const Tensor & self) const override;
-  Tensor cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const override;
-  Tensor cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const override;
-  Tensor cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor cudnn_convolution_backward_bias(const Tensor & grad_output) const override;
-  Tensor cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const override;
-  Tensor cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const override;
-  std::tuple<Tensor,Tensor> cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const override;
   std::tuple<Tensor,Tensor> _ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const override;
   Tensor _ctc_loss_backward(const Tensor & grad, const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, const Tensor & neg_log_likelihood, const Tensor & log_alpha, int64_t blank, bool zero_infinity) const override;
   Tensor embedding_dense_backward(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) const override;
@@ -570,29 +546,9 @@ struct CPUFloatType final : public CPUTypeDefault {
   Tensor & logspace_out(Tensor & out, Scalar start, Scalar end, int64_t steps) const override;
   Tensor _log_softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _log_softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const override;
-  Tensor miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_convolution_backward_bias(const Tensor & grad_output) const override;
-  Tensor miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_depthwise_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_depthwise_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_depthwise_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_depthwise_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
   Tensor narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const override;
   std::tuple<Tensor,Tensor,Tensor> native_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps) const override;
-  std::tuple<Tensor,Tensor> batch_norm_stats(const Tensor & input, double eps) const override;
-  Tensor batch_norm_elemt(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & mean, const Tensor & invstd, double eps) const override;
-  std::tuple<Tensor,Tensor> batch_norm_gather_stats(const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & running_mean, const Tensor & running_var, double momentum, double eps, int64_t count) const override;
   std::tuple<Tensor,Tensor,Tensor> native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor> batch_norm_backward_reduce(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, bool input_g, bool weight_g, bool bias_g) const override;
-  Tensor batch_norm_backward_elemt(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & weight, const Tensor & mean_dy, const Tensor & mean_dy_xmu) const override;
   std::tuple<Tensor,Tensor> batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const override;
   Tensor & randperm_out(Tensor & out, int64_t n, Generator * generator) const override;
   Tensor & range_out(Tensor & out, Scalar start, Scalar end, Scalar step) const override;
@@ -612,13 +568,7 @@ struct CPUFloatType final : public CPUTypeDefault {
   Tensor & sinh_out(Tensor & out, const Tensor & self) const override;
   Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  Tensor & _sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const override;
   Tensor & _sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const override;
-  Tensor & _sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
-  Tensor & _sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
   Tensor & sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & sqrt_(Tensor & self) const override;
   Tensor & sqrt_out(Tensor & out, const Tensor & self) const override;
@@ -633,47 +583,16 @@ struct CPUFloatType final : public CPUTypeDefault {
   std::tuple<Tensor,Tensor> _unique(const Tensor & self, bool sorted, bool return_inverse) const override;
   std::tuple<Tensor,Tensor> _unique_dim(const Tensor & self, int64_t dim, bool sorted, bool return_inverse) const override;
   Tensor _s_where(const Tensor & condition, const Tensor & self, const Tensor & other) const override;
-  std::tuple<Tensor,Tensor> _weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const override;
-  std::tuple<Tensor,Tensor> _weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const override;
   Tensor _standard_gamma_grad(const Tensor & self, const Tensor & output) const override;
   Tensor _standard_gamma(const Tensor & self, Generator * generator) const override;
   Tensor poisson(const Tensor & self, Generator * generator) const override;
-  Tensor native_norm(const Tensor & self, Scalar p) const override;
-  Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const override;
-  Tensor native_clone(const Tensor & self) const override;
-  Tensor & native_resize_as_(Tensor & self, const Tensor & the_template) const override;
-  Tensor & native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const override;
-  Tensor native_pow(const Tensor & self, Scalar exponent) const override;
-  Tensor & native_zero_(Tensor & self) const override;
   Tensor & s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
-  Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const override;
-  Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const override;
-  Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
-  Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
   Tensor sparse_mask(const Tensor & self, SparseTensorRef mask) const override;
-  Tensor to_dense(const Tensor & self) const override;
-  int64_t sparse_dim(const Tensor & self) const override;
-  int64_t dense_dim(const Tensor & self) const override;
-  int64_t _nnz(const Tensor & self) const override;
-  Tensor coalesce(const Tensor & self) const override;
-  bool is_coalesced(const Tensor & self) const override;
-  Tensor _indices(const Tensor & self) const override;
-  Tensor _values(const Tensor & self) const override;
-  Tensor & _coalesced_(Tensor & self, bool coalesced) const override;
-  Tensor indices(const Tensor & self) const override;
-  Tensor values(const Tensor & self) const override;
-  Tensor & hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor hspmm(const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor to_sparse(const Tensor & self, int64_t sparse_dim) const override;
   Tensor to_sparse(const Tensor & self) const override;
   Scalar _local_scalar_dense(const Tensor & self) const override;
-  std::tuple<Tensor,Tensor,Tensor> _thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const override;
-  std::tuple<Tensor,Tensor> _thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const override;
   Tensor & tril_(Tensor & self, int64_t diagonal) const override;
   Tensor & triu_(Tensor & self, int64_t diagonal) const override;
   Tensor & lerp_(Tensor & self, const Tensor & end, Scalar weight) const override;
diff --git a/build/aten/src/ATen/CPUHalfType.cpp b/build/aten/src/ATen/CPUHalfType.cpp
index 02473ddc8..5f8436598 100644
--- a/build/aten/src/ATen/CPUHalfType.cpp
+++ b/build/aten/src/ATen/CPUHalfType.cpp
@@ -169,9 +169,6 @@ Tensor & CPUHalfType::s_copy_(Tensor & self, const Tensor & src, bool non_blocki
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_copy__cpu(/* actuals */ self, src, non_blocking);
 }
-Tensor CPUHalfType::_s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const {
-    AT_ERROR("_s_copy_from not supported on CPUHalfType");
-}
 void CPUHalfType::_copy_same_type_(Tensor & self, const Tensor & src) const {
     const OptionalDeviceGuard device_guard(device_of(self));
  at::native::_copy_same_type__cpu(/* actuals */ self, src);
diff --git a/build/aten/src/ATen/CPUHalfType.h b/build/aten/src/ATen/CPUHalfType.h
index beabedc72..e20dc1529 100644
--- a/build/aten/src/ATen/CPUHalfType.h
+++ b/build/aten/src/ATen/CPUHalfType.h
@@ -41,7 +41,6 @@ struct CPUHalfType final : public CPUTypeDefault {
   Tensor & _th_cat_out(Tensor & self, TensorList tensors, int64_t dim) const override;
   Tensor _th_cat(TensorList tensors, int64_t dim) const override;
   Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const override;
-  Tensor _s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const override;
   void _copy_same_type_(Tensor & self, const Tensor & src) const override;
   Tensor empty(IntArrayRef size, const TensorOptions & options) const override;
   Tensor & resize_(Tensor & self, IntArrayRef size) const override;
diff --git a/build/aten/src/ATen/CPUIntType.cpp b/build/aten/src/ATen/CPUIntType.cpp
index 0eae26209..82fffa22c 100644
--- a/build/aten/src/ATen/CPUIntType.cpp
+++ b/build/aten/src/ATen/CPUIntType.cpp
@@ -1841,27 +1841,6 @@ Tensor CPUIntType::_th_cat(TensorList tensors, int64_t dim) const {
     THIntTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
     return self;
 }
-std::tuple<Tensor,Tensor> CPUIntType::_cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const {
-    AT_ERROR("_cudnn_ctc_loss not supported on CPUIntType");
-}
-Tensor CPUIntType::_cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const {
-    AT_ERROR("_cudnn_rnn_flatten_weight not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUIntType::_cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const {
-    AT_ERROR("_cudnn_rnn not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> CPUIntType::_cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const {
-    AT_ERROR("_cudnn_rnn_backward not supported on CPUIntType");
-}
-Tensor CPUIntType::_cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const {
-    AT_ERROR("_cudnn_init_dropout_state not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor> CPUIntType::_fused_dropout(const Tensor & self, double p, Generator * generator) const {
-    AT_ERROR("_fused_dropout not supported on CPUIntType");
-}
-Tensor CPUIntType::_masked_scale(const Tensor & self, const Tensor & mask, double scale) const {
-    AT_ERROR("_masked_scale not supported on CPUIntType");
-}
 Tensor & CPUIntType::abs_(Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_abs__cpu(/* actuals */ self);
@@ -1966,9 +1945,6 @@ Tensor & CPUIntType::s_copy_(Tensor & self, const Tensor & src, bool non_blockin
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_copy__cpu(/* actuals */ self, src, non_blocking);
 }
-Tensor CPUIntType::_s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const {
-    AT_ERROR("_s_copy_from not supported on CPUIntType");
-}
 void CPUIntType::_copy_same_type_(Tensor & self, const Tensor & src) const {
     const OptionalDeviceGuard device_guard(device_of(self));
  at::native::_copy_same_type__cpu(/* actuals */ self, src);
@@ -1989,54 +1965,6 @@ Tensor & CPUIntType::cosh_out(Tensor & out, const Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_cosh_out_cpu(/* actuals */ out, self);
 }
-Tensor CPUIntType::cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const {
-    AT_ERROR("cudnn_affine_grid_generator not supported on CPUIntType");
-}
-Tensor CPUIntType::cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const {
-    AT_ERROR("cudnn_affine_grid_generator_backward not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUIntType::cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
-    AT_ERROR("cudnn_batch_norm not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUIntType::cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
-    AT_ERROR("cudnn_batch_norm_backward not supported on CPUIntType");
-}
-Tensor CPUIntType::cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution not supported on CPUIntType");
-}
-Tensor CPUIntType::cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_backward_input not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUIntType::cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("cudnn_convolution_backward not supported on CPUIntType");
-}
-Tensor CPUIntType::cudnn_convolution_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("cudnn_convolution_backward_bias not supported on CPUIntType");
-}
-Tensor CPUIntType::cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_backward_weight not supported on CPUIntType");
-}
-Tensor CPUIntType::cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUIntType::cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("cudnn_convolution_transpose_backward not supported on CPUIntType");
-}
-Tensor CPUIntType::cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_bias not supported on CPUIntType");
-}
-Tensor CPUIntType::cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_input not supported on CPUIntType");
-}
-Tensor CPUIntType::cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_weight not supported on CPUIntType");
-}
-Tensor CPUIntType::cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const {
-    AT_ERROR("cudnn_grid_sampler not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor> CPUIntType::cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const {
-    AT_ERROR("cudnn_grid_sampler_backward not supported on CPUIntType");
-}
 std::tuple<Tensor,Tensor> CPUIntType::_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const {
     const OptionalDeviceGuard device_guard(device_of(log_probs));
     return at::native::ctc_loss_cpu(/* actuals */ log_probs, targets, input_lengths, target_lengths, blank, zero_infinity);
@@ -2205,51 +2133,6 @@ Tensor CPUIntType::_log_softmax_backward_data(const Tensor & grad_output, const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::log_softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
 }
-std::tuple<Tensor,Tensor,Tensor> CPUIntType::miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
-    AT_ERROR("miopen_batch_norm not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUIntType::miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
-    AT_ERROR("miopen_batch_norm_backward not supported on CPUIntType");
-}
-Tensor CPUIntType::miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution not supported on CPUIntType");
-}
-Tensor CPUIntType::miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_backward_input not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUIntType::miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_convolution_backward not supported on CPUIntType");
-}
-Tensor CPUIntType::miopen_convolution_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("miopen_convolution_backward_bias not supported on CPUIntType");
-}
-Tensor CPUIntType::miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_backward_weight not supported on CPUIntType");
-}
-Tensor CPUIntType::miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUIntType::miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_convolution_transpose_backward not supported on CPUIntType");
-}
-Tensor CPUIntType::miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose_backward_input not supported on CPUIntType");
-}
-Tensor CPUIntType::miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose_backward_weight not supported on CPUIntType");
-}
-Tensor CPUIntType::miopen_depthwise_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution not supported on CPUIntType");
-}
-Tensor CPUIntType::miopen_depthwise_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution_backward_input not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUIntType::miopen_depthwise_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_depthwise_convolution_backward not supported on CPUIntType");
-}
-Tensor CPUIntType::miopen_depthwise_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution_backward_weight not supported on CPUIntType");
-}
 Tensor CPUIntType::narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::narrow_copy_dense(/* actuals */ self, dim, start, length);
@@ -2258,25 +2141,10 @@ std::tuple<Tensor,Tensor,Tensor> CPUIntType::native_batch_norm(const Tensor & in
     const OptionalDeviceGuard device_guard(device_of(input));
     return at::native::batch_norm_cpu(/* actuals */ input, weight, bias, running_mean, running_var, training, momentum, eps);
 }
-std::tuple<Tensor,Tensor> CPUIntType::batch_norm_stats(const Tensor & input, double eps) const {
-    AT_ERROR("batch_norm_stats not supported on CPUIntType");
-}
-Tensor CPUIntType::batch_norm_elemt(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & mean, const Tensor & invstd, double eps) const {
-    AT_ERROR("batch_norm_elemt not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor> CPUIntType::batch_norm_gather_stats(const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & running_mean, const Tensor & running_var, double momentum, double eps, int64_t count) const {
-    AT_ERROR("batch_norm_gather_stats not supported on CPUIntType");
-}
 std::tuple<Tensor,Tensor,Tensor> CPUIntType::native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const {
     const OptionalDeviceGuard device_guard(device_of(grad_out));
     return at::native::batch_norm_backward_cpu(/* actuals */ grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
 }
-std::tuple<Tensor,Tensor,Tensor,Tensor> CPUIntType::batch_norm_backward_reduce(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, bool input_g, bool weight_g, bool bias_g) const {
-    AT_ERROR("batch_norm_backward_reduce not supported on CPUIntType");
-}
-Tensor CPUIntType::batch_norm_backward_elemt(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & weight, const Tensor & mean_dy, const Tensor & mean_dy_xmu) const {
-    AT_ERROR("batch_norm_backward_elemt not supported on CPUIntType");
-}
 std::tuple<Tensor,Tensor> CPUIntType::batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const {
     const OptionalDeviceGuard device_guard(device_of(input));
     return at::native::batch_norm_update_stats_cpu(/* actuals */ input, running_mean, running_var, momentum);
@@ -2353,28 +2221,10 @@ Tensor CPUIntType::_softmax_backward_data(const Tensor & grad_output, const Tens
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
 }
-Tensor & CPUIntType::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
-    AT_ERROR("_sparse_add_out not supported on CPUIntType");
-}
 Tensor & CPUIntType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::add_out_dense_sparse_cpu(/* actuals */ out, self, other, alpha);
 }
-Tensor & CPUIntType::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_div_zerodim_out not supported on CPUIntType");
-}
-Tensor & CPUIntType::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_div_scalar_out not supported on CPUIntType");
-}
-Tensor & CPUIntType::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_out not supported on CPUIntType");
-}
-Tensor & CPUIntType::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_zerodim_out not supported on CPUIntType");
-}
-Tensor & CPUIntType::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_mul_scalar_out not supported on CPUIntType");
-}
 Tensor & CPUIntType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_sspaddmm_out_only_sparse(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2431,12 +2281,6 @@ Tensor CPUIntType::_s_where(const Tensor & condition, const Tensor & self, const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_where_cpu(/* actuals */ condition, self, other);
 }
-std::tuple<Tensor,Tensor> CPUIntType::_weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const {
-    AT_ERROR("_weight_norm_cuda_interface not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor> CPUIntType::_weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const {
-    AT_ERROR("_weight_norm_cuda_interface_backward not supported on CPUIntType");
-}
 Tensor CPUIntType::_standard_gamma_grad(const Tensor & self, const Tensor & output) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_standard_gamma_grad_cpu(/* actuals */ self, output);
@@ -2449,27 +2293,6 @@ Tensor CPUIntType::poisson(const Tensor & self, Generator * generator) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_poisson_cpu(/* actuals */ self, generator);
 }
-Tensor CPUIntType::native_norm(const Tensor & self, Scalar p) const {
-    AT_ERROR("native_norm not supported on CPUIntType");
-}
-Tensor CPUIntType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
-    AT_ERROR("_sparse_sum_backward not supported on CPUIntType");
-}
-Tensor CPUIntType::native_clone(const Tensor & self) const {
-    AT_ERROR("native_clone not supported on CPUIntType");
-}
-Tensor & CPUIntType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
-    AT_ERROR("native_resize_as_ not supported on CPUIntType");
-}
-Tensor & CPUIntType::native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow_out not supported on CPUIntType");
-}
-Tensor CPUIntType::native_pow(const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow not supported on CPUIntType");
-}
-Tensor & CPUIntType::native_zero_(Tensor & self) const {
-    AT_ERROR("native_zero_ not supported on CPUIntType");
-}
 Tensor & CPUIntType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_out_sparse_dense_cpu(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2482,64 +2305,10 @@ Tensor & CPUIntType::s_native_addmm_(Tensor & self, const Tensor & mat1, const T
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_sparse_dense_cpu_(/* actuals */ self, mat1, mat2, beta, alpha);
 }
-Tensor CPUIntType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CPUIntType");
-}
-Tensor CPUIntType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CPUIntType");
-}
-Tensor & CPUIntType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_ not supported on CPUIntType");
-}
-Tensor & CPUIntType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_and_clear_ not supported on CPUIntType");
-}
 Tensor CPUIntType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::sparse_mask_cpu(/* actuals */ self, mask);
 }
-Tensor CPUIntType::to_dense(const Tensor & self) const {
-    AT_ERROR("to_dense not supported on CPUIntType");
-}
-int64_t CPUIntType::sparse_dim(const Tensor & self) const {
-    AT_ERROR("sparse_dim not supported on CPUIntType");
-}
-int64_t CPUIntType::dense_dim(const Tensor & self) const {
-    AT_ERROR("dense_dim not supported on CPUIntType");
-}
-int64_t CPUIntType::_nnz(const Tensor & self) const {
-    AT_ERROR("_nnz not supported on CPUIntType");
-}
-Tensor CPUIntType::coalesce(const Tensor & self) const {
-    AT_ERROR("coalesce not supported on CPUIntType");
-}
-bool CPUIntType::is_coalesced(const Tensor & self) const {
-    AT_ERROR("is_coalesced not supported on CPUIntType");
-}
-Tensor CPUIntType::_indices(const Tensor & self) const {
-    AT_ERROR("_indices not supported on CPUIntType");
-}
-Tensor CPUIntType::_values(const Tensor & self) const {
-    AT_ERROR("_values not supported on CPUIntType");
-}
-Tensor & CPUIntType::_coalesced_(Tensor & self, bool coalesced) const {
-    AT_ERROR("_coalesced_ not supported on CPUIntType");
-}
-Tensor CPUIntType::indices(const Tensor & self) const {
-    AT_ERROR("indices not supported on CPUIntType");
-}
-Tensor CPUIntType::values(const Tensor & self) const {
-    AT_ERROR("values not supported on CPUIntType");
-}
-Tensor & CPUIntType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm_out not supported on CPUIntType");
-}
-Tensor CPUIntType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm not supported on CPUIntType");
-}
-Tensor & CPUIntType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
-    AT_ERROR("copy_sparse_to_sparse_ not supported on CPUIntType");
-}
 Tensor CPUIntType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
@@ -2552,18 +2321,6 @@ Scalar CPUIntType::_local_scalar_dense(const Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_local_scalar_dense_cpu(/* actuals */ self);
 }
-std::tuple<Tensor,Tensor,Tensor> CPUIntType::_thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const {
-    AT_ERROR("_thnn_fused_lstm_cell not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUIntType::_thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const {
-    AT_ERROR("_thnn_fused_lstm_cell_backward not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor> CPUIntType::_thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const {
-    AT_ERROR("_thnn_fused_gru_cell not supported on CPUIntType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUIntType::_thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const {
-    AT_ERROR("_thnn_fused_gru_cell_backward not supported on CPUIntType");
-}
 Tensor & CPUIntType::tril_(Tensor & self, int64_t diagonal) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::tril_cpu_(/* actuals */ self, diagonal);
diff --git a/build/aten/src/ATen/CPUIntType.h b/build/aten/src/ATen/CPUIntType.h
index 64c120389..70e88db4f 100644
--- a/build/aten/src/ATen/CPUIntType.h
+++ b/build/aten/src/ATen/CPUIntType.h
@@ -220,13 +220,6 @@ struct CPUIntType final : public CPUTypeDefault {
   Tensor _th_alias(const Tensor & self) const override;
   Tensor & _th_cat_out(Tensor & self, TensorList tensors, int64_t dim) const override;
   Tensor _th_cat(TensorList tensors, int64_t dim) const override;
-  std::tuple<Tensor,Tensor> _cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const override;
-  Tensor _cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const override;
-  std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> _cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const override;
-  Tensor _cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const override;
-  std::tuple<Tensor,Tensor> _fused_dropout(const Tensor & self, double p, Generator * generator) const override;
-  Tensor _masked_scale(const Tensor & self, const Tensor & mask, double scale) const override;
   Tensor & abs_(Tensor & self) const override;
   Tensor & abs_out(Tensor & out, const Tensor & self) const override;
   Tensor & acos_(Tensor & self) const override;
@@ -253,28 +246,11 @@ struct CPUIntType final : public CPUTypeDefault {
   Tensor & clamp_min_(Tensor & self, Scalar min) const override;
   Tensor & clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const override;
   Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const override;
-  Tensor _s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const override;
   void _copy_same_type_(Tensor & self, const Tensor & src) const override;
   Tensor & cos_(Tensor & self) const override;
   Tensor & cos_out(Tensor & out, const Tensor & self) const override;
   Tensor & cosh_(Tensor & self) const override;
   Tensor & cosh_out(Tensor & out, const Tensor & self) const override;
-  Tensor cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const override;
-  Tensor cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const override;
-  Tensor cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor cudnn_convolution_backward_bias(const Tensor & grad_output) const override;
-  Tensor cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const override;
-  Tensor cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const override;
-  std::tuple<Tensor,Tensor> cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const override;
   std::tuple<Tensor,Tensor> _ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const override;
   Tensor _ctc_loss_backward(const Tensor & grad, const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, const Tensor & neg_log_likelihood, const Tensor & log_alpha, int64_t blank, bool zero_infinity) const override;
   Tensor embedding_dense_backward(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) const override;
@@ -317,29 +293,9 @@ struct CPUIntType final : public CPUTypeDefault {
   Tensor & logspace_out(Tensor & out, Scalar start, Scalar end, int64_t steps) const override;
   Tensor _log_softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _log_softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const override;
-  Tensor miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_convolution_backward_bias(const Tensor & grad_output) const override;
-  Tensor miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_depthwise_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_depthwise_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_depthwise_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_depthwise_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
   Tensor narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const override;
   std::tuple<Tensor,Tensor,Tensor> native_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps) const override;
-  std::tuple<Tensor,Tensor> batch_norm_stats(const Tensor & input, double eps) const override;
-  Tensor batch_norm_elemt(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & mean, const Tensor & invstd, double eps) const override;
-  std::tuple<Tensor,Tensor> batch_norm_gather_stats(const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & running_mean, const Tensor & running_var, double momentum, double eps, int64_t count) const override;
   std::tuple<Tensor,Tensor,Tensor> native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor> batch_norm_backward_reduce(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, bool input_g, bool weight_g, bool bias_g) const override;
-  Tensor batch_norm_backward_elemt(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & weight, const Tensor & mean_dy, const Tensor & mean_dy_xmu) const override;
   std::tuple<Tensor,Tensor> batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const override;
   Tensor & randperm_out(Tensor & out, int64_t n, Generator * generator) const override;
   Tensor & range_out(Tensor & out, Scalar start, Scalar end, Scalar step) const override;
@@ -359,13 +315,7 @@ struct CPUIntType final : public CPUTypeDefault {
   Tensor & sinh_out(Tensor & out, const Tensor & self) const override;
   Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  Tensor & _sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const override;
   Tensor & _sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const override;
-  Tensor & _sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
-  Tensor & _sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
   Tensor & sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & sqrt_(Tensor & self) const override;
   Tensor & sqrt_out(Tensor & out, const Tensor & self) const override;
@@ -380,47 +330,16 @@ struct CPUIntType final : public CPUTypeDefault {
   std::tuple<Tensor,Tensor> _unique(const Tensor & self, bool sorted, bool return_inverse) const override;
   std::tuple<Tensor,Tensor> _unique_dim(const Tensor & self, int64_t dim, bool sorted, bool return_inverse) const override;
   Tensor _s_where(const Tensor & condition, const Tensor & self, const Tensor & other) const override;
-  std::tuple<Tensor,Tensor> _weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const override;
-  std::tuple<Tensor,Tensor> _weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const override;
   Tensor _standard_gamma_grad(const Tensor & self, const Tensor & output) const override;
   Tensor _standard_gamma(const Tensor & self, Generator * generator) const override;
   Tensor poisson(const Tensor & self, Generator * generator) const override;
-  Tensor native_norm(const Tensor & self, Scalar p) const override;
-  Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const override;
-  Tensor native_clone(const Tensor & self) const override;
-  Tensor & native_resize_as_(Tensor & self, const Tensor & the_template) const override;
-  Tensor & native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const override;
-  Tensor native_pow(const Tensor & self, Scalar exponent) const override;
-  Tensor & native_zero_(Tensor & self) const override;
   Tensor & s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
-  Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const override;
-  Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const override;
-  Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
-  Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
   Tensor sparse_mask(const Tensor & self, SparseTensorRef mask) const override;
-  Tensor to_dense(const Tensor & self) const override;
-  int64_t sparse_dim(const Tensor & self) const override;
-  int64_t dense_dim(const Tensor & self) const override;
-  int64_t _nnz(const Tensor & self) const override;
-  Tensor coalesce(const Tensor & self) const override;
-  bool is_coalesced(const Tensor & self) const override;
-  Tensor _indices(const Tensor & self) const override;
-  Tensor _values(const Tensor & self) const override;
-  Tensor & _coalesced_(Tensor & self, bool coalesced) const override;
-  Tensor indices(const Tensor & self) const override;
-  Tensor values(const Tensor & self) const override;
-  Tensor & hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor hspmm(const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor to_sparse(const Tensor & self, int64_t sparse_dim) const override;
   Tensor to_sparse(const Tensor & self) const override;
   Scalar _local_scalar_dense(const Tensor & self) const override;
-  std::tuple<Tensor,Tensor,Tensor> _thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const override;
-  std::tuple<Tensor,Tensor> _thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const override;
   Tensor & tril_(Tensor & self, int64_t diagonal) const override;
   Tensor & triu_(Tensor & self, int64_t diagonal) const override;
   Tensor & lerp_(Tensor & self, const Tensor & end, Scalar weight) const override;
diff --git a/build/aten/src/ATen/CPULongType.cpp b/build/aten/src/ATen/CPULongType.cpp
index 529d5ed72..894cecea8 100644
--- a/build/aten/src/ATen/CPULongType.cpp
+++ b/build/aten/src/ATen/CPULongType.cpp
@@ -1841,27 +1841,6 @@ Tensor CPULongType::_th_cat(TensorList tensors, int64_t dim) const {
     THLongTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
     return self;
 }
-std::tuple<Tensor,Tensor> CPULongType::_cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const {
-    AT_ERROR("_cudnn_ctc_loss not supported on CPULongType");
-}
-Tensor CPULongType::_cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const {
-    AT_ERROR("_cudnn_rnn_flatten_weight not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPULongType::_cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const {
-    AT_ERROR("_cudnn_rnn not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> CPULongType::_cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const {
-    AT_ERROR("_cudnn_rnn_backward not supported on CPULongType");
-}
-Tensor CPULongType::_cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const {
-    AT_ERROR("_cudnn_init_dropout_state not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor> CPULongType::_fused_dropout(const Tensor & self, double p, Generator * generator) const {
-    AT_ERROR("_fused_dropout not supported on CPULongType");
-}
-Tensor CPULongType::_masked_scale(const Tensor & self, const Tensor & mask, double scale) const {
-    AT_ERROR("_masked_scale not supported on CPULongType");
-}
 Tensor & CPULongType::abs_(Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_abs__cpu(/* actuals */ self);
@@ -1966,9 +1945,6 @@ Tensor & CPULongType::s_copy_(Tensor & self, const Tensor & src, bool non_blocki
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_copy__cpu(/* actuals */ self, src, non_blocking);
 }
-Tensor CPULongType::_s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const {
-    AT_ERROR("_s_copy_from not supported on CPULongType");
-}
 void CPULongType::_copy_same_type_(Tensor & self, const Tensor & src) const {
     const OptionalDeviceGuard device_guard(device_of(self));
  at::native::_copy_same_type__cpu(/* actuals */ self, src);
@@ -1989,54 +1965,6 @@ Tensor & CPULongType::cosh_out(Tensor & out, const Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_cosh_out_cpu(/* actuals */ out, self);
 }
-Tensor CPULongType::cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const {
-    AT_ERROR("cudnn_affine_grid_generator not supported on CPULongType");
-}
-Tensor CPULongType::cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const {
-    AT_ERROR("cudnn_affine_grid_generator_backward not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPULongType::cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
-    AT_ERROR("cudnn_batch_norm not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPULongType::cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
-    AT_ERROR("cudnn_batch_norm_backward not supported on CPULongType");
-}
-Tensor CPULongType::cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution not supported on CPULongType");
-}
-Tensor CPULongType::cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_backward_input not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPULongType::cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("cudnn_convolution_backward not supported on CPULongType");
-}
-Tensor CPULongType::cudnn_convolution_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("cudnn_convolution_backward_bias not supported on CPULongType");
-}
-Tensor CPULongType::cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_backward_weight not supported on CPULongType");
-}
-Tensor CPULongType::cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPULongType::cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("cudnn_convolution_transpose_backward not supported on CPULongType");
-}
-Tensor CPULongType::cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_bias not supported on CPULongType");
-}
-Tensor CPULongType::cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_input not supported on CPULongType");
-}
-Tensor CPULongType::cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_weight not supported on CPULongType");
-}
-Tensor CPULongType::cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const {
-    AT_ERROR("cudnn_grid_sampler not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor> CPULongType::cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const {
-    AT_ERROR("cudnn_grid_sampler_backward not supported on CPULongType");
-}
 std::tuple<Tensor,Tensor> CPULongType::_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const {
     const OptionalDeviceGuard device_guard(device_of(log_probs));
     return at::native::ctc_loss_cpu(/* actuals */ log_probs, targets, input_lengths, target_lengths, blank, zero_infinity);
@@ -2205,51 +2133,6 @@ Tensor CPULongType::_log_softmax_backward_data(const Tensor & grad_output, const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::log_softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
 }
-std::tuple<Tensor,Tensor,Tensor> CPULongType::miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
-    AT_ERROR("miopen_batch_norm not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPULongType::miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
-    AT_ERROR("miopen_batch_norm_backward not supported on CPULongType");
-}
-Tensor CPULongType::miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution not supported on CPULongType");
-}
-Tensor CPULongType::miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_backward_input not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPULongType::miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_convolution_backward not supported on CPULongType");
-}
-Tensor CPULongType::miopen_convolution_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("miopen_convolution_backward_bias not supported on CPULongType");
-}
-Tensor CPULongType::miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_backward_weight not supported on CPULongType");
-}
-Tensor CPULongType::miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPULongType::miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_convolution_transpose_backward not supported on CPULongType");
-}
-Tensor CPULongType::miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose_backward_input not supported on CPULongType");
-}
-Tensor CPULongType::miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose_backward_weight not supported on CPULongType");
-}
-Tensor CPULongType::miopen_depthwise_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution not supported on CPULongType");
-}
-Tensor CPULongType::miopen_depthwise_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution_backward_input not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPULongType::miopen_depthwise_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_depthwise_convolution_backward not supported on CPULongType");
-}
-Tensor CPULongType::miopen_depthwise_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution_backward_weight not supported on CPULongType");
-}
 Tensor CPULongType::narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::narrow_copy_dense(/* actuals */ self, dim, start, length);
@@ -2258,25 +2141,10 @@ std::tuple<Tensor,Tensor,Tensor> CPULongType::native_batch_norm(const Tensor & i
     const OptionalDeviceGuard device_guard(device_of(input));
     return at::native::batch_norm_cpu(/* actuals */ input, weight, bias, running_mean, running_var, training, momentum, eps);
 }
-std::tuple<Tensor,Tensor> CPULongType::batch_norm_stats(const Tensor & input, double eps) const {
-    AT_ERROR("batch_norm_stats not supported on CPULongType");
-}
-Tensor CPULongType::batch_norm_elemt(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & mean, const Tensor & invstd, double eps) const {
-    AT_ERROR("batch_norm_elemt not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor> CPULongType::batch_norm_gather_stats(const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & running_mean, const Tensor & running_var, double momentum, double eps, int64_t count) const {
-    AT_ERROR("batch_norm_gather_stats not supported on CPULongType");
-}
 std::tuple<Tensor,Tensor,Tensor> CPULongType::native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const {
     const OptionalDeviceGuard device_guard(device_of(grad_out));
     return at::native::batch_norm_backward_cpu(/* actuals */ grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
 }
-std::tuple<Tensor,Tensor,Tensor,Tensor> CPULongType::batch_norm_backward_reduce(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, bool input_g, bool weight_g, bool bias_g) const {
-    AT_ERROR("batch_norm_backward_reduce not supported on CPULongType");
-}
-Tensor CPULongType::batch_norm_backward_elemt(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & weight, const Tensor & mean_dy, const Tensor & mean_dy_xmu) const {
-    AT_ERROR("batch_norm_backward_elemt not supported on CPULongType");
-}
 std::tuple<Tensor,Tensor> CPULongType::batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const {
     const OptionalDeviceGuard device_guard(device_of(input));
     return at::native::batch_norm_update_stats_cpu(/* actuals */ input, running_mean, running_var, momentum);
@@ -2353,28 +2221,10 @@ Tensor CPULongType::_softmax_backward_data(const Tensor & grad_output, const Ten
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
 }
-Tensor & CPULongType::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
-    AT_ERROR("_sparse_add_out not supported on CPULongType");
-}
 Tensor & CPULongType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::add_out_dense_sparse_cpu(/* actuals */ out, self, other, alpha);
 }
-Tensor & CPULongType::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_div_zerodim_out not supported on CPULongType");
-}
-Tensor & CPULongType::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_div_scalar_out not supported on CPULongType");
-}
-Tensor & CPULongType::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_out not supported on CPULongType");
-}
-Tensor & CPULongType::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_zerodim_out not supported on CPULongType");
-}
-Tensor & CPULongType::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_mul_scalar_out not supported on CPULongType");
-}
 Tensor & CPULongType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_sspaddmm_out_only_sparse(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2431,12 +2281,6 @@ Tensor CPULongType::_s_where(const Tensor & condition, const Tensor & self, cons
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_where_cpu(/* actuals */ condition, self, other);
 }
-std::tuple<Tensor,Tensor> CPULongType::_weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const {
-    AT_ERROR("_weight_norm_cuda_interface not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor> CPULongType::_weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const {
-    AT_ERROR("_weight_norm_cuda_interface_backward not supported on CPULongType");
-}
 Tensor CPULongType::_standard_gamma_grad(const Tensor & self, const Tensor & output) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_standard_gamma_grad_cpu(/* actuals */ self, output);
@@ -2449,27 +2293,6 @@ Tensor CPULongType::poisson(const Tensor & self, Generator * generator) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_poisson_cpu(/* actuals */ self, generator);
 }
-Tensor CPULongType::native_norm(const Tensor & self, Scalar p) const {
-    AT_ERROR("native_norm not supported on CPULongType");
-}
-Tensor CPULongType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
-    AT_ERROR("_sparse_sum_backward not supported on CPULongType");
-}
-Tensor CPULongType::native_clone(const Tensor & self) const {
-    AT_ERROR("native_clone not supported on CPULongType");
-}
-Tensor & CPULongType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
-    AT_ERROR("native_resize_as_ not supported on CPULongType");
-}
-Tensor & CPULongType::native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow_out not supported on CPULongType");
-}
-Tensor CPULongType::native_pow(const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow not supported on CPULongType");
-}
-Tensor & CPULongType::native_zero_(Tensor & self) const {
-    AT_ERROR("native_zero_ not supported on CPULongType");
-}
 Tensor & CPULongType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_out_sparse_dense_cpu(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2482,64 +2305,10 @@ Tensor & CPULongType::s_native_addmm_(Tensor & self, const Tensor & mat1, const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_sparse_dense_cpu_(/* actuals */ self, mat1, mat2, beta, alpha);
 }
-Tensor CPULongType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CPULongType");
-}
-Tensor CPULongType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CPULongType");
-}
-Tensor & CPULongType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_ not supported on CPULongType");
-}
-Tensor & CPULongType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_and_clear_ not supported on CPULongType");
-}
 Tensor CPULongType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::sparse_mask_cpu(/* actuals */ self, mask);
 }
-Tensor CPULongType::to_dense(const Tensor & self) const {
-    AT_ERROR("to_dense not supported on CPULongType");
-}
-int64_t CPULongType::sparse_dim(const Tensor & self) const {
-    AT_ERROR("sparse_dim not supported on CPULongType");
-}
-int64_t CPULongType::dense_dim(const Tensor & self) const {
-    AT_ERROR("dense_dim not supported on CPULongType");
-}
-int64_t CPULongType::_nnz(const Tensor & self) const {
-    AT_ERROR("_nnz not supported on CPULongType");
-}
-Tensor CPULongType::coalesce(const Tensor & self) const {
-    AT_ERROR("coalesce not supported on CPULongType");
-}
-bool CPULongType::is_coalesced(const Tensor & self) const {
-    AT_ERROR("is_coalesced not supported on CPULongType");
-}
-Tensor CPULongType::_indices(const Tensor & self) const {
-    AT_ERROR("_indices not supported on CPULongType");
-}
-Tensor CPULongType::_values(const Tensor & self) const {
-    AT_ERROR("_values not supported on CPULongType");
-}
-Tensor & CPULongType::_coalesced_(Tensor & self, bool coalesced) const {
-    AT_ERROR("_coalesced_ not supported on CPULongType");
-}
-Tensor CPULongType::indices(const Tensor & self) const {
-    AT_ERROR("indices not supported on CPULongType");
-}
-Tensor CPULongType::values(const Tensor & self) const {
-    AT_ERROR("values not supported on CPULongType");
-}
-Tensor & CPULongType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm_out not supported on CPULongType");
-}
-Tensor CPULongType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm not supported on CPULongType");
-}
-Tensor & CPULongType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
-    AT_ERROR("copy_sparse_to_sparse_ not supported on CPULongType");
-}
 Tensor CPULongType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
@@ -2552,18 +2321,6 @@ Scalar CPULongType::_local_scalar_dense(const Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_local_scalar_dense_cpu(/* actuals */ self);
 }
-std::tuple<Tensor,Tensor,Tensor> CPULongType::_thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const {
-    AT_ERROR("_thnn_fused_lstm_cell not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPULongType::_thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const {
-    AT_ERROR("_thnn_fused_lstm_cell_backward not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor> CPULongType::_thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const {
-    AT_ERROR("_thnn_fused_gru_cell not supported on CPULongType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPULongType::_thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const {
-    AT_ERROR("_thnn_fused_gru_cell_backward not supported on CPULongType");
-}
 Tensor & CPULongType::tril_(Tensor & self, int64_t diagonal) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::tril_cpu_(/* actuals */ self, diagonal);
diff --git a/build/aten/src/ATen/CPULongType.h b/build/aten/src/ATen/CPULongType.h
index ea00245f9..f6c238735 100644
--- a/build/aten/src/ATen/CPULongType.h
+++ b/build/aten/src/ATen/CPULongType.h
@@ -220,13 +220,6 @@ struct CPULongType final : public CPUTypeDefault {
   Tensor _th_alias(const Tensor & self) const override;
   Tensor & _th_cat_out(Tensor & self, TensorList tensors, int64_t dim) const override;
   Tensor _th_cat(TensorList tensors, int64_t dim) const override;
-  std::tuple<Tensor,Tensor> _cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const override;
-  Tensor _cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const override;
-  std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> _cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const override;
-  Tensor _cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const override;
-  std::tuple<Tensor,Tensor> _fused_dropout(const Tensor & self, double p, Generator * generator) const override;
-  Tensor _masked_scale(const Tensor & self, const Tensor & mask, double scale) const override;
   Tensor & abs_(Tensor & self) const override;
   Tensor & abs_out(Tensor & out, const Tensor & self) const override;
   Tensor & acos_(Tensor & self) const override;
@@ -253,28 +246,11 @@ struct CPULongType final : public CPUTypeDefault {
   Tensor & clamp_min_(Tensor & self, Scalar min) const override;
   Tensor & clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const override;
   Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const override;
-  Tensor _s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const override;
   void _copy_same_type_(Tensor & self, const Tensor & src) const override;
   Tensor & cos_(Tensor & self) const override;
   Tensor & cos_out(Tensor & out, const Tensor & self) const override;
   Tensor & cosh_(Tensor & self) const override;
   Tensor & cosh_out(Tensor & out, const Tensor & self) const override;
-  Tensor cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const override;
-  Tensor cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const override;
-  Tensor cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor cudnn_convolution_backward_bias(const Tensor & grad_output) const override;
-  Tensor cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const override;
-  Tensor cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const override;
-  std::tuple<Tensor,Tensor> cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const override;
   std::tuple<Tensor,Tensor> _ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const override;
   Tensor _ctc_loss_backward(const Tensor & grad, const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, const Tensor & neg_log_likelihood, const Tensor & log_alpha, int64_t blank, bool zero_infinity) const override;
   Tensor embedding_dense_backward(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) const override;
@@ -317,29 +293,9 @@ struct CPULongType final : public CPUTypeDefault {
   Tensor & logspace_out(Tensor & out, Scalar start, Scalar end, int64_t steps) const override;
   Tensor _log_softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _log_softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const override;
-  Tensor miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_convolution_backward_bias(const Tensor & grad_output) const override;
-  Tensor miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_depthwise_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_depthwise_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_depthwise_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_depthwise_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
   Tensor narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const override;
   std::tuple<Tensor,Tensor,Tensor> native_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps) const override;
-  std::tuple<Tensor,Tensor> batch_norm_stats(const Tensor & input, double eps) const override;
-  Tensor batch_norm_elemt(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & mean, const Tensor & invstd, double eps) const override;
-  std::tuple<Tensor,Tensor> batch_norm_gather_stats(const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & running_mean, const Tensor & running_var, double momentum, double eps, int64_t count) const override;
   std::tuple<Tensor,Tensor,Tensor> native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor> batch_norm_backward_reduce(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, bool input_g, bool weight_g, bool bias_g) const override;
-  Tensor batch_norm_backward_elemt(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & weight, const Tensor & mean_dy, const Tensor & mean_dy_xmu) const override;
   std::tuple<Tensor,Tensor> batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const override;
   Tensor & randperm_out(Tensor & out, int64_t n, Generator * generator) const override;
   Tensor & range_out(Tensor & out, Scalar start, Scalar end, Scalar step) const override;
@@ -359,13 +315,7 @@ struct CPULongType final : public CPUTypeDefault {
   Tensor & sinh_out(Tensor & out, const Tensor & self) const override;
   Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  Tensor & _sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const override;
   Tensor & _sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const override;
-  Tensor & _sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
-  Tensor & _sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
   Tensor & sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & sqrt_(Tensor & self) const override;
   Tensor & sqrt_out(Tensor & out, const Tensor & self) const override;
@@ -380,47 +330,16 @@ struct CPULongType final : public CPUTypeDefault {
   std::tuple<Tensor,Tensor> _unique(const Tensor & self, bool sorted, bool return_inverse) const override;
   std::tuple<Tensor,Tensor> _unique_dim(const Tensor & self, int64_t dim, bool sorted, bool return_inverse) const override;
   Tensor _s_where(const Tensor & condition, const Tensor & self, const Tensor & other) const override;
-  std::tuple<Tensor,Tensor> _weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const override;
-  std::tuple<Tensor,Tensor> _weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const override;
   Tensor _standard_gamma_grad(const Tensor & self, const Tensor & output) const override;
   Tensor _standard_gamma(const Tensor & self, Generator * generator) const override;
   Tensor poisson(const Tensor & self, Generator * generator) const override;
-  Tensor native_norm(const Tensor & self, Scalar p) const override;
-  Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const override;
-  Tensor native_clone(const Tensor & self) const override;
-  Tensor & native_resize_as_(Tensor & self, const Tensor & the_template) const override;
-  Tensor & native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const override;
-  Tensor native_pow(const Tensor & self, Scalar exponent) const override;
-  Tensor & native_zero_(Tensor & self) const override;
   Tensor & s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
-  Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const override;
-  Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const override;
-  Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
-  Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
   Tensor sparse_mask(const Tensor & self, SparseTensorRef mask) const override;
-  Tensor to_dense(const Tensor & self) const override;
-  int64_t sparse_dim(const Tensor & self) const override;
-  int64_t dense_dim(const Tensor & self) const override;
-  int64_t _nnz(const Tensor & self) const override;
-  Tensor coalesce(const Tensor & self) const override;
-  bool is_coalesced(const Tensor & self) const override;
-  Tensor _indices(const Tensor & self) const override;
-  Tensor _values(const Tensor & self) const override;
-  Tensor & _coalesced_(Tensor & self, bool coalesced) const override;
-  Tensor indices(const Tensor & self) const override;
-  Tensor values(const Tensor & self) const override;
-  Tensor & hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor hspmm(const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor to_sparse(const Tensor & self, int64_t sparse_dim) const override;
   Tensor to_sparse(const Tensor & self) const override;
   Scalar _local_scalar_dense(const Tensor & self) const override;
-  std::tuple<Tensor,Tensor,Tensor> _thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const override;
-  std::tuple<Tensor,Tensor> _thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const override;
   Tensor & tril_(Tensor & self, int64_t diagonal) const override;
   Tensor & triu_(Tensor & self, int64_t diagonal) const override;
   Tensor & lerp_(Tensor & self, const Tensor & end, Scalar weight) const override;
diff --git a/build/aten/src/ATen/CPUShortType.cpp b/build/aten/src/ATen/CPUShortType.cpp
index 5517267d3..119954b25 100644
--- a/build/aten/src/ATen/CPUShortType.cpp
+++ b/build/aten/src/ATen/CPUShortType.cpp
@@ -1841,27 +1841,6 @@ Tensor CPUShortType::_th_cat(TensorList tensors, int64_t dim) const {
     THShortTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
     return self;
 }
-std::tuple<Tensor,Tensor> CPUShortType::_cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const {
-    AT_ERROR("_cudnn_ctc_loss not supported on CPUShortType");
-}
-Tensor CPUShortType::_cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const {
-    AT_ERROR("_cudnn_rnn_flatten_weight not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUShortType::_cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const {
-    AT_ERROR("_cudnn_rnn not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> CPUShortType::_cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const {
-    AT_ERROR("_cudnn_rnn_backward not supported on CPUShortType");
-}
-Tensor CPUShortType::_cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const {
-    AT_ERROR("_cudnn_init_dropout_state not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor> CPUShortType::_fused_dropout(const Tensor & self, double p, Generator * generator) const {
-    AT_ERROR("_fused_dropout not supported on CPUShortType");
-}
-Tensor CPUShortType::_masked_scale(const Tensor & self, const Tensor & mask, double scale) const {
-    AT_ERROR("_masked_scale not supported on CPUShortType");
-}
 Tensor & CPUShortType::abs_(Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_abs__cpu(/* actuals */ self);
@@ -1966,9 +1945,6 @@ Tensor & CPUShortType::s_copy_(Tensor & self, const Tensor & src, bool non_block
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_copy__cpu(/* actuals */ self, src, non_blocking);
 }
-Tensor CPUShortType::_s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const {
-    AT_ERROR("_s_copy_from not supported on CPUShortType");
-}
 void CPUShortType::_copy_same_type_(Tensor & self, const Tensor & src) const {
     const OptionalDeviceGuard device_guard(device_of(self));
  at::native::_copy_same_type__cpu(/* actuals */ self, src);
@@ -1989,54 +1965,6 @@ Tensor & CPUShortType::cosh_out(Tensor & out, const Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_cosh_out_cpu(/* actuals */ out, self);
 }
-Tensor CPUShortType::cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const {
-    AT_ERROR("cudnn_affine_grid_generator not supported on CPUShortType");
-}
-Tensor CPUShortType::cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const {
-    AT_ERROR("cudnn_affine_grid_generator_backward not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUShortType::cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
-    AT_ERROR("cudnn_batch_norm not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUShortType::cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
-    AT_ERROR("cudnn_batch_norm_backward not supported on CPUShortType");
-}
-Tensor CPUShortType::cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution not supported on CPUShortType");
-}
-Tensor CPUShortType::cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_backward_input not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUShortType::cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("cudnn_convolution_backward not supported on CPUShortType");
-}
-Tensor CPUShortType::cudnn_convolution_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("cudnn_convolution_backward_bias not supported on CPUShortType");
-}
-Tensor CPUShortType::cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_backward_weight not supported on CPUShortType");
-}
-Tensor CPUShortType::cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUShortType::cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("cudnn_convolution_transpose_backward not supported on CPUShortType");
-}
-Tensor CPUShortType::cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_bias not supported on CPUShortType");
-}
-Tensor CPUShortType::cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_input not supported on CPUShortType");
-}
-Tensor CPUShortType::cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("cudnn_convolution_transpose_backward_weight not supported on CPUShortType");
-}
-Tensor CPUShortType::cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const {
-    AT_ERROR("cudnn_grid_sampler not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor> CPUShortType::cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const {
-    AT_ERROR("cudnn_grid_sampler_backward not supported on CPUShortType");
-}
 std::tuple<Tensor,Tensor> CPUShortType::_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const {
     const OptionalDeviceGuard device_guard(device_of(log_probs));
     return at::native::ctc_loss_cpu(/* actuals */ log_probs, targets, input_lengths, target_lengths, blank, zero_infinity);
@@ -2205,51 +2133,6 @@ Tensor CPUShortType::_log_softmax_backward_data(const Tensor & grad_output, cons
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::log_softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
 }
-std::tuple<Tensor,Tensor,Tensor> CPUShortType::miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
-    AT_ERROR("miopen_batch_norm not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUShortType::miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
-    AT_ERROR("miopen_batch_norm_backward not supported on CPUShortType");
-}
-Tensor CPUShortType::miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution not supported on CPUShortType");
-}
-Tensor CPUShortType::miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_backward_input not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUShortType::miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_convolution_backward not supported on CPUShortType");
-}
-Tensor CPUShortType::miopen_convolution_backward_bias(const Tensor & grad_output) const {
-    AT_ERROR("miopen_convolution_backward_bias not supported on CPUShortType");
-}
-Tensor CPUShortType::miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_backward_weight not supported on CPUShortType");
-}
-Tensor CPUShortType::miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUShortType::miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_convolution_transpose_backward not supported on CPUShortType");
-}
-Tensor CPUShortType::miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose_backward_input not supported on CPUShortType");
-}
-Tensor CPUShortType::miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_convolution_transpose_backward_weight not supported on CPUShortType");
-}
-Tensor CPUShortType::miopen_depthwise_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution not supported on CPUShortType");
-}
-Tensor CPUShortType::miopen_depthwise_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution_backward_input not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor,Tensor> CPUShortType::miopen_depthwise_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
-    AT_ERROR("miopen_depthwise_convolution_backward not supported on CPUShortType");
-}
-Tensor CPUShortType::miopen_depthwise_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
-    AT_ERROR("miopen_depthwise_convolution_backward_weight not supported on CPUShortType");
-}
 Tensor CPUShortType::narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::narrow_copy_dense(/* actuals */ self, dim, start, length);
@@ -2258,25 +2141,10 @@ std::tuple<Tensor,Tensor,Tensor> CPUShortType::native_batch_norm(const Tensor &
     const OptionalDeviceGuard device_guard(device_of(input));
     return at::native::batch_norm_cpu(/* actuals */ input, weight, bias, running_mean, running_var, training, momentum, eps);
 }
-std::tuple<Tensor,Tensor> CPUShortType::batch_norm_stats(const Tensor & input, double eps) const {
-    AT_ERROR("batch_norm_stats not supported on CPUShortType");
-}
-Tensor CPUShortType::batch_norm_elemt(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & mean, const Tensor & invstd, double eps) const {
-    AT_ERROR("batch_norm_elemt not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor> CPUShortType::batch_norm_gather_stats(const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & running_mean, const Tensor & running_var, double momentum, double eps, int64_t count) const {
-    AT_ERROR("batch_norm_gather_stats not supported on CPUShortType");
-}
 std::tuple<Tensor,Tensor,Tensor> CPUShortType::native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const {
     const OptionalDeviceGuard device_guard(device_of(grad_out));
     return at::native::batch_norm_backward_cpu(/* actuals */ grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
 }
-std::tuple<Tensor,Tensor,Tensor,Tensor> CPUShortType::batch_norm_backward_reduce(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, bool input_g, bool weight_g, bool bias_g) const {
-    AT_ERROR("batch_norm_backward_reduce not supported on CPUShortType");
-}
-Tensor CPUShortType::batch_norm_backward_elemt(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & weight, const Tensor & mean_dy, const Tensor & mean_dy_xmu) const {
-    AT_ERROR("batch_norm_backward_elemt not supported on CPUShortType");
-}
 std::tuple<Tensor,Tensor> CPUShortType::batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const {
     const OptionalDeviceGuard device_guard(device_of(input));
     return at::native::batch_norm_update_stats_cpu(/* actuals */ input, running_mean, running_var, momentum);
@@ -2353,28 +2221,10 @@ Tensor CPUShortType::_softmax_backward_data(const Tensor & grad_output, const Te
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
 }
-Tensor & CPUShortType::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
-    AT_ERROR("_sparse_add_out not supported on CPUShortType");
-}
 Tensor & CPUShortType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::add_out_dense_sparse_cpu(/* actuals */ out, self, other, alpha);
 }
-Tensor & CPUShortType::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_div_zerodim_out not supported on CPUShortType");
-}
-Tensor & CPUShortType::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_div_scalar_out not supported on CPUShortType");
-}
-Tensor & CPUShortType::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_out not supported on CPUShortType");
-}
-Tensor & CPUShortType::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_zerodim_out not supported on CPUShortType");
-}
-Tensor & CPUShortType::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_mul_scalar_out not supported on CPUShortType");
-}
 Tensor & CPUShortType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_sspaddmm_out_only_sparse(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2431,12 +2281,6 @@ Tensor CPUShortType::_s_where(const Tensor & condition, const Tensor & self, con
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_where_cpu(/* actuals */ condition, self, other);
 }
-std::tuple<Tensor,Tensor> CPUShortType::_weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const {
-    AT_ERROR("_weight_norm_cuda_interface not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor> CPUShortType::_weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const {
-    AT_ERROR("_weight_norm_cuda_interface_backward not supported on CPUShortType");
-}
 Tensor CPUShortType::_standard_gamma_grad(const Tensor & self, const Tensor & output) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_standard_gamma_grad_cpu(/* actuals */ self, output);
@@ -2449,27 +2293,6 @@ Tensor CPUShortType::poisson(const Tensor & self, Generator * generator) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_poisson_cpu(/* actuals */ self, generator);
 }
-Tensor CPUShortType::native_norm(const Tensor & self, Scalar p) const {
-    AT_ERROR("native_norm not supported on CPUShortType");
-}
-Tensor CPUShortType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
-    AT_ERROR("_sparse_sum_backward not supported on CPUShortType");
-}
-Tensor CPUShortType::native_clone(const Tensor & self) const {
-    AT_ERROR("native_clone not supported on CPUShortType");
-}
-Tensor & CPUShortType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
-    AT_ERROR("native_resize_as_ not supported on CPUShortType");
-}
-Tensor & CPUShortType::native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow_out not supported on CPUShortType");
-}
-Tensor CPUShortType::native_pow(const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow not supported on CPUShortType");
-}
-Tensor & CPUShortType::native_zero_(Tensor & self) const {
-    AT_ERROR("native_zero_ not supported on CPUShortType");
-}
 Tensor & CPUShortType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_out_sparse_dense_cpu(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2482,64 +2305,10 @@ Tensor & CPUShortType::s_native_addmm_(Tensor & self, const Tensor & mat1, const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_sparse_dense_cpu_(/* actuals */ self, mat1, mat2, beta, alpha);
 }
-Tensor CPUShortType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CPUShortType");
-}
-Tensor CPUShortType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CPUShortType");
-}
-Tensor & CPUShortType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_ not supported on CPUShortType");
-}
-Tensor & CPUShortType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_and_clear_ not supported on CPUShortType");
-}
 Tensor CPUShortType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::sparse_mask_cpu(/* actuals */ self, mask);
 }
-Tensor CPUShortType::to_dense(const Tensor & self) const {
-    AT_ERROR("to_dense not supported on CPUShortType");
-}
-int64_t CPUShortType::sparse_dim(const Tensor & self) const {
-    AT_ERROR("sparse_dim not supported on CPUShortType");
-}
-int64_t CPUShortType::dense_dim(const Tensor & self) const {
-    AT_ERROR("dense_dim not supported on CPUShortType");
-}
-int64_t CPUShortType::_nnz(const Tensor & self) const {
-    AT_ERROR("_nnz not supported on CPUShortType");
-}
-Tensor CPUShortType::coalesce(const Tensor & self) const {
-    AT_ERROR("coalesce not supported on CPUShortType");
-}
-bool CPUShortType::is_coalesced(const Tensor & self) const {
-    AT_ERROR("is_coalesced not supported on CPUShortType");
-}
-Tensor CPUShortType::_indices(const Tensor & self) const {
-    AT_ERROR("_indices not supported on CPUShortType");
-}
-Tensor CPUShortType::_values(const Tensor & self) const {
-    AT_ERROR("_values not supported on CPUShortType");
-}
-Tensor & CPUShortType::_coalesced_(Tensor & self, bool coalesced) const {
-    AT_ERROR("_coalesced_ not supported on CPUShortType");
-}
-Tensor CPUShortType::indices(const Tensor & self) const {
-    AT_ERROR("indices not supported on CPUShortType");
-}
-Tensor CPUShortType::values(const Tensor & self) const {
-    AT_ERROR("values not supported on CPUShortType");
-}
-Tensor & CPUShortType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm_out not supported on CPUShortType");
-}
-Tensor CPUShortType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm not supported on CPUShortType");
-}
-Tensor & CPUShortType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
-    AT_ERROR("copy_sparse_to_sparse_ not supported on CPUShortType");
-}
 Tensor CPUShortType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
@@ -2552,18 +2321,6 @@ Scalar CPUShortType::_local_scalar_dense(const Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_local_scalar_dense_cpu(/* actuals */ self);
 }
-std::tuple<Tensor,Tensor,Tensor> CPUShortType::_thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const {
-    AT_ERROR("_thnn_fused_lstm_cell not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUShortType::_thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const {
-    AT_ERROR("_thnn_fused_lstm_cell_backward not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor> CPUShortType::_thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const {
-    AT_ERROR("_thnn_fused_gru_cell not supported on CPUShortType");
-}
-std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CPUShortType::_thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const {
-    AT_ERROR("_thnn_fused_gru_cell_backward not supported on CPUShortType");
-}
 Tensor & CPUShortType::tril_(Tensor & self, int64_t diagonal) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::tril_cpu_(/* actuals */ self, diagonal);
diff --git a/build/aten/src/ATen/CPUShortType.h b/build/aten/src/ATen/CPUShortType.h
index ac407a27c..96f5359e5 100644
--- a/build/aten/src/ATen/CPUShortType.h
+++ b/build/aten/src/ATen/CPUShortType.h
@@ -220,13 +220,6 @@ struct CPUShortType final : public CPUTypeDefault {
   Tensor _th_alias(const Tensor & self) const override;
   Tensor & _th_cat_out(Tensor & self, TensorList tensors, int64_t dim) const override;
   Tensor _th_cat(TensorList tensors, int64_t dim) const override;
-  std::tuple<Tensor,Tensor> _cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const override;
-  Tensor _cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const override;
-  std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> _cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const override;
-  Tensor _cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const override;
-  std::tuple<Tensor,Tensor> _fused_dropout(const Tensor & self, double p, Generator * generator) const override;
-  Tensor _masked_scale(const Tensor & self, const Tensor & mask, double scale) const override;
   Tensor & abs_(Tensor & self) const override;
   Tensor & abs_out(Tensor & out, const Tensor & self) const override;
   Tensor & acos_(Tensor & self) const override;
@@ -253,28 +246,11 @@ struct CPUShortType final : public CPUTypeDefault {
   Tensor & clamp_min_(Tensor & self, Scalar min) const override;
   Tensor & clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const override;
   Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const override;
-  Tensor _s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const override;
   void _copy_same_type_(Tensor & self, const Tensor & src) const override;
   Tensor & cos_(Tensor & self) const override;
   Tensor & cos_out(Tensor & out, const Tensor & self) const override;
   Tensor & cosh_(Tensor & self) const override;
   Tensor & cosh_out(Tensor & out, const Tensor & self) const override;
-  Tensor cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const override;
-  Tensor cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const override;
-  Tensor cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor cudnn_convolution_backward_bias(const Tensor & grad_output) const override;
-  Tensor cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const override;
-  Tensor cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const override;
-  std::tuple<Tensor,Tensor> cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const override;
   std::tuple<Tensor,Tensor> _ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const override;
   Tensor _ctc_loss_backward(const Tensor & grad, const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, const Tensor & neg_log_likelihood, const Tensor & log_alpha, int64_t blank, bool zero_infinity) const override;
   Tensor embedding_dense_backward(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) const override;
@@ -317,29 +293,9 @@ struct CPUShortType final : public CPUTypeDefault {
   Tensor & logspace_out(Tensor & out, Scalar start, Scalar end, int64_t steps) const override;
   Tensor _log_softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _log_softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const override;
-  Tensor miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_convolution_backward_bias(const Tensor & grad_output) const override;
-  Tensor miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_depthwise_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  Tensor miopen_depthwise_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
-  std::tuple<Tensor,Tensor,Tensor> miopen_depthwise_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const override;
-  Tensor miopen_depthwise_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const override;
   Tensor narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const override;
   std::tuple<Tensor,Tensor,Tensor> native_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps) const override;
-  std::tuple<Tensor,Tensor> batch_norm_stats(const Tensor & input, double eps) const override;
-  Tensor batch_norm_elemt(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & mean, const Tensor & invstd, double eps) const override;
-  std::tuple<Tensor,Tensor> batch_norm_gather_stats(const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & running_mean, const Tensor & running_var, double momentum, double eps, int64_t count) const override;
   std::tuple<Tensor,Tensor,Tensor> native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor> batch_norm_backward_reduce(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, bool input_g, bool weight_g, bool bias_g) const override;
-  Tensor batch_norm_backward_elemt(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & weight, const Tensor & mean_dy, const Tensor & mean_dy_xmu) const override;
   std::tuple<Tensor,Tensor> batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const override;
   Tensor & randperm_out(Tensor & out, int64_t n, Generator * generator) const override;
   Tensor & range_out(Tensor & out, Scalar start, Scalar end, Scalar step) const override;
@@ -359,13 +315,7 @@ struct CPUShortType final : public CPUTypeDefault {
   Tensor & sinh_out(Tensor & out, const Tensor & self) const override;
   Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  Tensor & _sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const override;
   Tensor & _sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const override;
-  Tensor & _sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
-  Tensor & _sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
   Tensor & sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & sqrt_(Tensor & self) const override;
   Tensor & sqrt_out(Tensor & out, const Tensor & self) const override;
@@ -380,47 +330,16 @@ struct CPUShortType final : public CPUTypeDefault {
   std::tuple<Tensor,Tensor> _unique(const Tensor & self, bool sorted, bool return_inverse) const override;
   std::tuple<Tensor,Tensor> _unique_dim(const Tensor & self, int64_t dim, bool sorted, bool return_inverse) const override;
   Tensor _s_where(const Tensor & condition, const Tensor & self, const Tensor & other) const override;
-  std::tuple<Tensor,Tensor> _weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const override;
-  std::tuple<Tensor,Tensor> _weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const override;
   Tensor _standard_gamma_grad(const Tensor & self, const Tensor & output) const override;
   Tensor _standard_gamma(const Tensor & self, Generator * generator) const override;
   Tensor poisson(const Tensor & self, Generator * generator) const override;
-  Tensor native_norm(const Tensor & self, Scalar p) const override;
-  Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const override;
-  Tensor native_clone(const Tensor & self) const override;
-  Tensor & native_resize_as_(Tensor & self, const Tensor & the_template) const override;
-  Tensor & native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const override;
-  Tensor native_pow(const Tensor & self, Scalar exponent) const override;
-  Tensor & native_zero_(Tensor & self) const override;
   Tensor & s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
-  Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const override;
-  Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const override;
-  Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
-  Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
   Tensor sparse_mask(const Tensor & self, SparseTensorRef mask) const override;
-  Tensor to_dense(const Tensor & self) const override;
-  int64_t sparse_dim(const Tensor & self) const override;
-  int64_t dense_dim(const Tensor & self) const override;
-  int64_t _nnz(const Tensor & self) const override;
-  Tensor coalesce(const Tensor & self) const override;
-  bool is_coalesced(const Tensor & self) const override;
-  Tensor _indices(const Tensor & self) const override;
-  Tensor _values(const Tensor & self) const override;
-  Tensor & _coalesced_(Tensor & self, bool coalesced) const override;
-  Tensor indices(const Tensor & self) const override;
-  Tensor values(const Tensor & self) const override;
-  Tensor & hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor hspmm(const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor to_sparse(const Tensor & self, int64_t sparse_dim) const override;
   Tensor to_sparse(const Tensor & self) const override;
   Scalar _local_scalar_dense(const Tensor & self) const override;
-  std::tuple<Tensor,Tensor,Tensor> _thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const override;
-  std::tuple<Tensor,Tensor> _thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const override;
-  std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> _thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const override;
   Tensor & tril_(Tensor & self, int64_t diagonal) const override;
   Tensor & triu_(Tensor & self, int64_t diagonal) const override;
   Tensor & lerp_(Tensor & self, const Tensor & end, Scalar weight) const override;
diff --git a/build/aten/src/ATen/CUDAByteType.cpp b/build/aten/src/ATen/CUDAByteType.cpp
index 53448da7e..9ca52d0d0 100644
--- a/build/aten/src/ATen/CUDAByteType.cpp
+++ b/build/aten/src/ATen/CUDAByteType.cpp
@@ -2035,9 +2035,6 @@ Tensor CUDAByteType::_s_copy_from(const Tensor & self, const Tensor & dst, bool
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_copy_from_cuda(/* actuals */ self, dst, non_blocking);
 }
-void CUDAByteType::_copy_same_type_(Tensor & self, const Tensor & src) const {
-    AT_ERROR("_copy_same_type_ not supported on CUDAByteType");
-}
 Tensor & CUDAByteType::cos_(Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_cos__cuda(/* actuals */ self);
@@ -2454,28 +2451,10 @@ Tensor CUDAByteType::_softmax_backward_data(const Tensor & grad_output, const Te
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::softmax_backward_cuda(/* actuals */ grad_output, output, dim, self);
 }
-Tensor & CUDAByteType::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
-    AT_ERROR("_sparse_add_out not supported on CUDAByteType");
-}
 Tensor & CUDAByteType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::add_out_dense_sparse_cuda(/* actuals */ out, self, other, alpha);
 }
-Tensor & CUDAByteType::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_div_zerodim_out not supported on CUDAByteType");
-}
-Tensor & CUDAByteType::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_div_scalar_out not supported on CUDAByteType");
-}
-Tensor & CUDAByteType::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_out not supported on CUDAByteType");
-}
-Tensor & CUDAByteType::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_zerodim_out not supported on CUDAByteType");
-}
-Tensor & CUDAByteType::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_mul_scalar_out not supported on CUDAByteType");
-}
 Tensor & CUDAByteType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_sspaddmm_out_only_sparse_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2552,27 +2531,6 @@ Tensor CUDAByteType::poisson(const Tensor & self, Generator * generator) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_poisson_cuda(/* actuals */ self, generator);
 }
-Tensor CUDAByteType::native_norm(const Tensor & self, Scalar p) const {
-    AT_ERROR("native_norm not supported on CUDAByteType");
-}
-Tensor CUDAByteType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
-    AT_ERROR("_sparse_sum_backward not supported on CUDAByteType");
-}
-Tensor CUDAByteType::native_clone(const Tensor & self) const {
-    AT_ERROR("native_clone not supported on CUDAByteType");
-}
-Tensor & CUDAByteType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
-    AT_ERROR("native_resize_as_ not supported on CUDAByteType");
-}
-Tensor & CUDAByteType::native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow_out not supported on CUDAByteType");
-}
-Tensor CUDAByteType::native_pow(const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow not supported on CUDAByteType");
-}
-Tensor & CUDAByteType::native_zero_(Tensor & self) const {
-    AT_ERROR("native_zero_ not supported on CUDAByteType");
-}
 Tensor & CUDAByteType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_out_sparse_dense_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2585,64 +2543,10 @@ Tensor & CUDAByteType::s_native_addmm_(Tensor & self, const Tensor & mat1, const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_sparse_dense_cuda_(/* actuals */ self, mat1, mat2, beta, alpha);
 }
-Tensor CUDAByteType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CUDAByteType");
-}
-Tensor CUDAByteType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CUDAByteType");
-}
-Tensor & CUDAByteType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_ not supported on CUDAByteType");
-}
-Tensor & CUDAByteType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_and_clear_ not supported on CUDAByteType");
-}
 Tensor CUDAByteType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::sparse_mask_cuda(/* actuals */ self, mask);
 }
-Tensor CUDAByteType::to_dense(const Tensor & self) const {
-    AT_ERROR("to_dense not supported on CUDAByteType");
-}
-int64_t CUDAByteType::sparse_dim(const Tensor & self) const {
-    AT_ERROR("sparse_dim not supported on CUDAByteType");
-}
-int64_t CUDAByteType::dense_dim(const Tensor & self) const {
-    AT_ERROR("dense_dim not supported on CUDAByteType");
-}
-int64_t CUDAByteType::_nnz(const Tensor & self) const {
-    AT_ERROR("_nnz not supported on CUDAByteType");
-}
-Tensor CUDAByteType::coalesce(const Tensor & self) const {
-    AT_ERROR("coalesce not supported on CUDAByteType");
-}
-bool CUDAByteType::is_coalesced(const Tensor & self) const {
-    AT_ERROR("is_coalesced not supported on CUDAByteType");
-}
-Tensor CUDAByteType::_indices(const Tensor & self) const {
-    AT_ERROR("_indices not supported on CUDAByteType");
-}
-Tensor CUDAByteType::_values(const Tensor & self) const {
-    AT_ERROR("_values not supported on CUDAByteType");
-}
-Tensor & CUDAByteType::_coalesced_(Tensor & self, bool coalesced) const {
-    AT_ERROR("_coalesced_ not supported on CUDAByteType");
-}
-Tensor CUDAByteType::indices(const Tensor & self) const {
-    AT_ERROR("indices not supported on CUDAByteType");
-}
-Tensor CUDAByteType::values(const Tensor & self) const {
-    AT_ERROR("values not supported on CUDAByteType");
-}
-Tensor & CUDAByteType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm_out not supported on CUDAByteType");
-}
-Tensor CUDAByteType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm not supported on CUDAByteType");
-}
-Tensor & CUDAByteType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
-    AT_ERROR("copy_sparse_to_sparse_ not supported on CUDAByteType");
-}
 Tensor CUDAByteType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
diff --git a/build/aten/src/ATen/CUDAByteType.h b/build/aten/src/ATen/CUDAByteType.h
index ef203a356..393bb925d 100644
--- a/build/aten/src/ATen/CUDAByteType.h
+++ b/build/aten/src/ATen/CUDAByteType.h
@@ -262,7 +262,6 @@ struct CUDAByteType final : public CUDATypeDefault {
   Tensor & clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const override;
   Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor _s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const override;
-  void _copy_same_type_(Tensor & self, const Tensor & src) const override;
   Tensor & cos_(Tensor & self) const override;
   Tensor & cos_out(Tensor & out, const Tensor & self) const override;
   Tensor & cosh_(Tensor & self) const override;
@@ -367,13 +366,7 @@ struct CUDAByteType final : public CUDATypeDefault {
   Tensor & sinh_out(Tensor & out, const Tensor & self) const override;
   Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  Tensor & _sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const override;
   Tensor & _sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const override;
-  Tensor & _sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
-  Tensor & _sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
   Tensor & sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & sqrt_(Tensor & self) const override;
   Tensor & sqrt_out(Tensor & out, const Tensor & self) const override;
@@ -393,35 +386,10 @@ struct CUDAByteType final : public CUDATypeDefault {
   Tensor _standard_gamma_grad(const Tensor & self, const Tensor & output) const override;
   Tensor _standard_gamma(const Tensor & self, Generator * generator) const override;
   Tensor poisson(const Tensor & self, Generator * generator) const override;
-  Tensor native_norm(const Tensor & self, Scalar p) const override;
-  Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const override;
-  Tensor native_clone(const Tensor & self) const override;
-  Tensor & native_resize_as_(Tensor & self, const Tensor & the_template) const override;
-  Tensor & native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const override;
-  Tensor native_pow(const Tensor & self, Scalar exponent) const override;
-  Tensor & native_zero_(Tensor & self) const override;
   Tensor & s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
-  Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const override;
-  Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const override;
-  Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
-  Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
   Tensor sparse_mask(const Tensor & self, SparseTensorRef mask) const override;
-  Tensor to_dense(const Tensor & self) const override;
-  int64_t sparse_dim(const Tensor & self) const override;
-  int64_t dense_dim(const Tensor & self) const override;
-  int64_t _nnz(const Tensor & self) const override;
-  Tensor coalesce(const Tensor & self) const override;
-  bool is_coalesced(const Tensor & self) const override;
-  Tensor _indices(const Tensor & self) const override;
-  Tensor _values(const Tensor & self) const override;
-  Tensor & _coalesced_(Tensor & self, bool coalesced) const override;
-  Tensor indices(const Tensor & self) const override;
-  Tensor values(const Tensor & self) const override;
-  Tensor & hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor hspmm(const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor to_sparse(const Tensor & self, int64_t sparse_dim) const override;
   Tensor to_sparse(const Tensor & self) const override;
   Scalar _local_scalar_dense(const Tensor & self) const override;
diff --git a/build/aten/src/ATen/CUDACharType.cpp b/build/aten/src/ATen/CUDACharType.cpp
index 4f52ee798..d5845412e 100644
--- a/build/aten/src/ATen/CUDACharType.cpp
+++ b/build/aten/src/ATen/CUDACharType.cpp
@@ -2035,9 +2035,6 @@ Tensor CUDACharType::_s_copy_from(const Tensor & self, const Tensor & dst, bool
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_copy_from_cuda(/* actuals */ self, dst, non_blocking);
 }
-void CUDACharType::_copy_same_type_(Tensor & self, const Tensor & src) const {
-    AT_ERROR("_copy_same_type_ not supported on CUDACharType");
-}
 Tensor & CUDACharType::cos_(Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_cos__cuda(/* actuals */ self);
@@ -2454,28 +2451,10 @@ Tensor CUDACharType::_softmax_backward_data(const Tensor & grad_output, const Te
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::softmax_backward_cuda(/* actuals */ grad_output, output, dim, self);
 }
-Tensor & CUDACharType::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
-    AT_ERROR("_sparse_add_out not supported on CUDACharType");
-}
 Tensor & CUDACharType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::add_out_dense_sparse_cuda(/* actuals */ out, self, other, alpha);
 }
-Tensor & CUDACharType::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_div_zerodim_out not supported on CUDACharType");
-}
-Tensor & CUDACharType::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_div_scalar_out not supported on CUDACharType");
-}
-Tensor & CUDACharType::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_out not supported on CUDACharType");
-}
-Tensor & CUDACharType::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_zerodim_out not supported on CUDACharType");
-}
-Tensor & CUDACharType::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_mul_scalar_out not supported on CUDACharType");
-}
 Tensor & CUDACharType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_sspaddmm_out_only_sparse_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2552,27 +2531,6 @@ Tensor CUDACharType::poisson(const Tensor & self, Generator * generator) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_poisson_cuda(/* actuals */ self, generator);
 }
-Tensor CUDACharType::native_norm(const Tensor & self, Scalar p) const {
-    AT_ERROR("native_norm not supported on CUDACharType");
-}
-Tensor CUDACharType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
-    AT_ERROR("_sparse_sum_backward not supported on CUDACharType");
-}
-Tensor CUDACharType::native_clone(const Tensor & self) const {
-    AT_ERROR("native_clone not supported on CUDACharType");
-}
-Tensor & CUDACharType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
-    AT_ERROR("native_resize_as_ not supported on CUDACharType");
-}
-Tensor & CUDACharType::native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow_out not supported on CUDACharType");
-}
-Tensor CUDACharType::native_pow(const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow not supported on CUDACharType");
-}
-Tensor & CUDACharType::native_zero_(Tensor & self) const {
-    AT_ERROR("native_zero_ not supported on CUDACharType");
-}
 Tensor & CUDACharType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_out_sparse_dense_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2585,64 +2543,10 @@ Tensor & CUDACharType::s_native_addmm_(Tensor & self, const Tensor & mat1, const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_sparse_dense_cuda_(/* actuals */ self, mat1, mat2, beta, alpha);
 }
-Tensor CUDACharType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CUDACharType");
-}
-Tensor CUDACharType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CUDACharType");
-}
-Tensor & CUDACharType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_ not supported on CUDACharType");
-}
-Tensor & CUDACharType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_and_clear_ not supported on CUDACharType");
-}
 Tensor CUDACharType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::sparse_mask_cuda(/* actuals */ self, mask);
 }
-Tensor CUDACharType::to_dense(const Tensor & self) const {
-    AT_ERROR("to_dense not supported on CUDACharType");
-}
-int64_t CUDACharType::sparse_dim(const Tensor & self) const {
-    AT_ERROR("sparse_dim not supported on CUDACharType");
-}
-int64_t CUDACharType::dense_dim(const Tensor & self) const {
-    AT_ERROR("dense_dim not supported on CUDACharType");
-}
-int64_t CUDACharType::_nnz(const Tensor & self) const {
-    AT_ERROR("_nnz not supported on CUDACharType");
-}
-Tensor CUDACharType::coalesce(const Tensor & self) const {
-    AT_ERROR("coalesce not supported on CUDACharType");
-}
-bool CUDACharType::is_coalesced(const Tensor & self) const {
-    AT_ERROR("is_coalesced not supported on CUDACharType");
-}
-Tensor CUDACharType::_indices(const Tensor & self) const {
-    AT_ERROR("_indices not supported on CUDACharType");
-}
-Tensor CUDACharType::_values(const Tensor & self) const {
-    AT_ERROR("_values not supported on CUDACharType");
-}
-Tensor & CUDACharType::_coalesced_(Tensor & self, bool coalesced) const {
-    AT_ERROR("_coalesced_ not supported on CUDACharType");
-}
-Tensor CUDACharType::indices(const Tensor & self) const {
-    AT_ERROR("indices not supported on CUDACharType");
-}
-Tensor CUDACharType::values(const Tensor & self) const {
-    AT_ERROR("values not supported on CUDACharType");
-}
-Tensor & CUDACharType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm_out not supported on CUDACharType");
-}
-Tensor CUDACharType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm not supported on CUDACharType");
-}
-Tensor & CUDACharType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
-    AT_ERROR("copy_sparse_to_sparse_ not supported on CUDACharType");
-}
 Tensor CUDACharType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
diff --git a/build/aten/src/ATen/CUDACharType.h b/build/aten/src/ATen/CUDACharType.h
index 12832e7c1..23552bc10 100644
--- a/build/aten/src/ATen/CUDACharType.h
+++ b/build/aten/src/ATen/CUDACharType.h
@@ -262,7 +262,6 @@ struct CUDACharType final : public CUDATypeDefault {
   Tensor & clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const override;
   Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor _s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const override;
-  void _copy_same_type_(Tensor & self, const Tensor & src) const override;
   Tensor & cos_(Tensor & self) const override;
   Tensor & cos_out(Tensor & out, const Tensor & self) const override;
   Tensor & cosh_(Tensor & self) const override;
@@ -367,13 +366,7 @@ struct CUDACharType final : public CUDATypeDefault {
   Tensor & sinh_out(Tensor & out, const Tensor & self) const override;
   Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  Tensor & _sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const override;
   Tensor & _sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const override;
-  Tensor & _sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
-  Tensor & _sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
   Tensor & sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & sqrt_(Tensor & self) const override;
   Tensor & sqrt_out(Tensor & out, const Tensor & self) const override;
@@ -393,35 +386,10 @@ struct CUDACharType final : public CUDATypeDefault {
   Tensor _standard_gamma_grad(const Tensor & self, const Tensor & output) const override;
   Tensor _standard_gamma(const Tensor & self, Generator * generator) const override;
   Tensor poisson(const Tensor & self, Generator * generator) const override;
-  Tensor native_norm(const Tensor & self, Scalar p) const override;
-  Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const override;
-  Tensor native_clone(const Tensor & self) const override;
-  Tensor & native_resize_as_(Tensor & self, const Tensor & the_template) const override;
-  Tensor & native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const override;
-  Tensor native_pow(const Tensor & self, Scalar exponent) const override;
-  Tensor & native_zero_(Tensor & self) const override;
   Tensor & s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
-  Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const override;
-  Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const override;
-  Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
-  Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
   Tensor sparse_mask(const Tensor & self, SparseTensorRef mask) const override;
-  Tensor to_dense(const Tensor & self) const override;
-  int64_t sparse_dim(const Tensor & self) const override;
-  int64_t dense_dim(const Tensor & self) const override;
-  int64_t _nnz(const Tensor & self) const override;
-  Tensor coalesce(const Tensor & self) const override;
-  bool is_coalesced(const Tensor & self) const override;
-  Tensor _indices(const Tensor & self) const override;
-  Tensor _values(const Tensor & self) const override;
-  Tensor & _coalesced_(Tensor & self, bool coalesced) const override;
-  Tensor indices(const Tensor & self) const override;
-  Tensor values(const Tensor & self) const override;
-  Tensor & hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor hspmm(const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor to_sparse(const Tensor & self, int64_t sparse_dim) const override;
   Tensor to_sparse(const Tensor & self) const override;
   Scalar _local_scalar_dense(const Tensor & self) const override;
diff --git a/build/aten/src/ATen/CUDADoubleType.cpp b/build/aten/src/ATen/CUDADoubleType.cpp
index 5a855e994..bee3d07c4 100644
--- a/build/aten/src/ATen/CUDADoubleType.cpp
+++ b/build/aten/src/ATen/CUDADoubleType.cpp
@@ -5288,9 +5288,6 @@ Tensor CUDADoubleType::_s_copy_from(const Tensor & self, const Tensor & dst, boo
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_copy_from_cuda(/* actuals */ self, dst, non_blocking);
 }
-void CUDADoubleType::_copy_same_type_(Tensor & self, const Tensor & src) const {
-    AT_ERROR("_copy_same_type_ not supported on CUDADoubleType");
-}
 Tensor & CUDADoubleType::cos_(Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_cos__cuda(/* actuals */ self);
@@ -5707,28 +5704,10 @@ Tensor CUDADoubleType::_softmax_backward_data(const Tensor & grad_output, const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::softmax_backward_cuda(/* actuals */ grad_output, output, dim, self);
 }
-Tensor & CUDADoubleType::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
-    AT_ERROR("_sparse_add_out not supported on CUDADoubleType");
-}
 Tensor & CUDADoubleType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::add_out_dense_sparse_cuda(/* actuals */ out, self, other, alpha);
 }
-Tensor & CUDADoubleType::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_div_zerodim_out not supported on CUDADoubleType");
-}
-Tensor & CUDADoubleType::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_div_scalar_out not supported on CUDADoubleType");
-}
-Tensor & CUDADoubleType::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_out not supported on CUDADoubleType");
-}
-Tensor & CUDADoubleType::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_zerodim_out not supported on CUDADoubleType");
-}
-Tensor & CUDADoubleType::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_mul_scalar_out not supported on CUDADoubleType");
-}
 Tensor & CUDADoubleType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_sspaddmm_out_only_sparse_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -5805,27 +5784,6 @@ Tensor CUDADoubleType::poisson(const Tensor & self, Generator * generator) const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_poisson_cuda(/* actuals */ self, generator);
 }
-Tensor CUDADoubleType::native_norm(const Tensor & self, Scalar p) const {
-    AT_ERROR("native_norm not supported on CUDADoubleType");
-}
-Tensor CUDADoubleType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
-    AT_ERROR("_sparse_sum_backward not supported on CUDADoubleType");
-}
-Tensor CUDADoubleType::native_clone(const Tensor & self) const {
-    AT_ERROR("native_clone not supported on CUDADoubleType");
-}
-Tensor & CUDADoubleType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
-    AT_ERROR("native_resize_as_ not supported on CUDADoubleType");
-}
-Tensor & CUDADoubleType::native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow_out not supported on CUDADoubleType");
-}
-Tensor CUDADoubleType::native_pow(const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow not supported on CUDADoubleType");
-}
-Tensor & CUDADoubleType::native_zero_(Tensor & self) const {
-    AT_ERROR("native_zero_ not supported on CUDADoubleType");
-}
 Tensor & CUDADoubleType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_out_sparse_dense_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -5838,64 +5796,10 @@ Tensor & CUDADoubleType::s_native_addmm_(Tensor & self, const Tensor & mat1, con
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_sparse_dense_cuda_(/* actuals */ self, mat1, mat2, beta, alpha);
 }
-Tensor CUDADoubleType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CUDADoubleType");
-}
-Tensor CUDADoubleType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CUDADoubleType");
-}
-Tensor & CUDADoubleType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_ not supported on CUDADoubleType");
-}
-Tensor & CUDADoubleType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_and_clear_ not supported on CUDADoubleType");
-}
 Tensor CUDADoubleType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::sparse_mask_cuda(/* actuals */ self, mask);
 }
-Tensor CUDADoubleType::to_dense(const Tensor & self) const {
-    AT_ERROR("to_dense not supported on CUDADoubleType");
-}
-int64_t CUDADoubleType::sparse_dim(const Tensor & self) const {
-    AT_ERROR("sparse_dim not supported on CUDADoubleType");
-}
-int64_t CUDADoubleType::dense_dim(const Tensor & self) const {
-    AT_ERROR("dense_dim not supported on CUDADoubleType");
-}
-int64_t CUDADoubleType::_nnz(const Tensor & self) const {
-    AT_ERROR("_nnz not supported on CUDADoubleType");
-}
-Tensor CUDADoubleType::coalesce(const Tensor & self) const {
-    AT_ERROR("coalesce not supported on CUDADoubleType");
-}
-bool CUDADoubleType::is_coalesced(const Tensor & self) const {
-    AT_ERROR("is_coalesced not supported on CUDADoubleType");
-}
-Tensor CUDADoubleType::_indices(const Tensor & self) const {
-    AT_ERROR("_indices not supported on CUDADoubleType");
-}
-Tensor CUDADoubleType::_values(const Tensor & self) const {
-    AT_ERROR("_values not supported on CUDADoubleType");
-}
-Tensor & CUDADoubleType::_coalesced_(Tensor & self, bool coalesced) const {
-    AT_ERROR("_coalesced_ not supported on CUDADoubleType");
-}
-Tensor CUDADoubleType::indices(const Tensor & self) const {
-    AT_ERROR("indices not supported on CUDADoubleType");
-}
-Tensor CUDADoubleType::values(const Tensor & self) const {
-    AT_ERROR("values not supported on CUDADoubleType");
-}
-Tensor & CUDADoubleType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm_out not supported on CUDADoubleType");
-}
-Tensor CUDADoubleType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm not supported on CUDADoubleType");
-}
-Tensor & CUDADoubleType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
-    AT_ERROR("copy_sparse_to_sparse_ not supported on CUDADoubleType");
-}
 Tensor CUDADoubleType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
diff --git a/build/aten/src/ATen/CUDADoubleType.h b/build/aten/src/ATen/CUDADoubleType.h
index 1dfe34673..6dd5ef76f 100644
--- a/build/aten/src/ATen/CUDADoubleType.h
+++ b/build/aten/src/ATen/CUDADoubleType.h
@@ -551,7 +551,6 @@ struct CUDADoubleType final : public CUDATypeDefault {
   Tensor & clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const override;
   Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor _s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const override;
-  void _copy_same_type_(Tensor & self, const Tensor & src) const override;
   Tensor & cos_(Tensor & self) const override;
   Tensor & cos_out(Tensor & out, const Tensor & self) const override;
   Tensor & cosh_(Tensor & self) const override;
@@ -656,13 +655,7 @@ struct CUDADoubleType final : public CUDATypeDefault {
   Tensor & sinh_out(Tensor & out, const Tensor & self) const override;
   Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  Tensor & _sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const override;
   Tensor & _sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const override;
-  Tensor & _sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
-  Tensor & _sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
   Tensor & sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & sqrt_(Tensor & self) const override;
   Tensor & sqrt_out(Tensor & out, const Tensor & self) const override;
@@ -682,35 +675,10 @@ struct CUDADoubleType final : public CUDATypeDefault {
   Tensor _standard_gamma_grad(const Tensor & self, const Tensor & output) const override;
   Tensor _standard_gamma(const Tensor & self, Generator * generator) const override;
   Tensor poisson(const Tensor & self, Generator * generator) const override;
-  Tensor native_norm(const Tensor & self, Scalar p) const override;
-  Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const override;
-  Tensor native_clone(const Tensor & self) const override;
-  Tensor & native_resize_as_(Tensor & self, const Tensor & the_template) const override;
-  Tensor & native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const override;
-  Tensor native_pow(const Tensor & self, Scalar exponent) const override;
-  Tensor & native_zero_(Tensor & self) const override;
   Tensor & s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
-  Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const override;
-  Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const override;
-  Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
-  Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
   Tensor sparse_mask(const Tensor & self, SparseTensorRef mask) const override;
-  Tensor to_dense(const Tensor & self) const override;
-  int64_t sparse_dim(const Tensor & self) const override;
-  int64_t dense_dim(const Tensor & self) const override;
-  int64_t _nnz(const Tensor & self) const override;
-  Tensor coalesce(const Tensor & self) const override;
-  bool is_coalesced(const Tensor & self) const override;
-  Tensor _indices(const Tensor & self) const override;
-  Tensor _values(const Tensor & self) const override;
-  Tensor & _coalesced_(Tensor & self, bool coalesced) const override;
-  Tensor indices(const Tensor & self) const override;
-  Tensor values(const Tensor & self) const override;
-  Tensor & hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor hspmm(const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor to_sparse(const Tensor & self, int64_t sparse_dim) const override;
   Tensor to_sparse(const Tensor & self) const override;
   Scalar _local_scalar_dense(const Tensor & self) const override;
diff --git a/build/aten/src/ATen/CUDAFloatType.cpp b/build/aten/src/ATen/CUDAFloatType.cpp
index 0bca2137f..e97d1cd77 100644
--- a/build/aten/src/ATen/CUDAFloatType.cpp
+++ b/build/aten/src/ATen/CUDAFloatType.cpp
@@ -5288,9 +5288,6 @@ Tensor CUDAFloatType::_s_copy_from(const Tensor & self, const Tensor & dst, bool
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_copy_from_cuda(/* actuals */ self, dst, non_blocking);
 }
-void CUDAFloatType::_copy_same_type_(Tensor & self, const Tensor & src) const {
-    AT_ERROR("_copy_same_type_ not supported on CUDAFloatType");
-}
 Tensor & CUDAFloatType::cos_(Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_cos__cuda(/* actuals */ self);
@@ -5707,28 +5704,10 @@ Tensor CUDAFloatType::_softmax_backward_data(const Tensor & grad_output, const T
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::softmax_backward_cuda(/* actuals */ grad_output, output, dim, self);
 }
-Tensor & CUDAFloatType::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
-    AT_ERROR("_sparse_add_out not supported on CUDAFloatType");
-}
 Tensor & CUDAFloatType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::add_out_dense_sparse_cuda(/* actuals */ out, self, other, alpha);
 }
-Tensor & CUDAFloatType::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_div_zerodim_out not supported on CUDAFloatType");
-}
-Tensor & CUDAFloatType::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_div_scalar_out not supported on CUDAFloatType");
-}
-Tensor & CUDAFloatType::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_out not supported on CUDAFloatType");
-}
-Tensor & CUDAFloatType::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_zerodim_out not supported on CUDAFloatType");
-}
-Tensor & CUDAFloatType::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_mul_scalar_out not supported on CUDAFloatType");
-}
 Tensor & CUDAFloatType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_sspaddmm_out_only_sparse_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -5805,27 +5784,6 @@ Tensor CUDAFloatType::poisson(const Tensor & self, Generator * generator) const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_poisson_cuda(/* actuals */ self, generator);
 }
-Tensor CUDAFloatType::native_norm(const Tensor & self, Scalar p) const {
-    AT_ERROR("native_norm not supported on CUDAFloatType");
-}
-Tensor CUDAFloatType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
-    AT_ERROR("_sparse_sum_backward not supported on CUDAFloatType");
-}
-Tensor CUDAFloatType::native_clone(const Tensor & self) const {
-    AT_ERROR("native_clone not supported on CUDAFloatType");
-}
-Tensor & CUDAFloatType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
-    AT_ERROR("native_resize_as_ not supported on CUDAFloatType");
-}
-Tensor & CUDAFloatType::native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow_out not supported on CUDAFloatType");
-}
-Tensor CUDAFloatType::native_pow(const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow not supported on CUDAFloatType");
-}
-Tensor & CUDAFloatType::native_zero_(Tensor & self) const {
-    AT_ERROR("native_zero_ not supported on CUDAFloatType");
-}
 Tensor & CUDAFloatType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_out_sparse_dense_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -5838,64 +5796,10 @@ Tensor & CUDAFloatType::s_native_addmm_(Tensor & self, const Tensor & mat1, cons
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_sparse_dense_cuda_(/* actuals */ self, mat1, mat2, beta, alpha);
 }
-Tensor CUDAFloatType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CUDAFloatType");
-}
-Tensor CUDAFloatType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CUDAFloatType");
-}
-Tensor & CUDAFloatType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_ not supported on CUDAFloatType");
-}
-Tensor & CUDAFloatType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_and_clear_ not supported on CUDAFloatType");
-}
 Tensor CUDAFloatType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::sparse_mask_cuda(/* actuals */ self, mask);
 }
-Tensor CUDAFloatType::to_dense(const Tensor & self) const {
-    AT_ERROR("to_dense not supported on CUDAFloatType");
-}
-int64_t CUDAFloatType::sparse_dim(const Tensor & self) const {
-    AT_ERROR("sparse_dim not supported on CUDAFloatType");
-}
-int64_t CUDAFloatType::dense_dim(const Tensor & self) const {
-    AT_ERROR("dense_dim not supported on CUDAFloatType");
-}
-int64_t CUDAFloatType::_nnz(const Tensor & self) const {
-    AT_ERROR("_nnz not supported on CUDAFloatType");
-}
-Tensor CUDAFloatType::coalesce(const Tensor & self) const {
-    AT_ERROR("coalesce not supported on CUDAFloatType");
-}
-bool CUDAFloatType::is_coalesced(const Tensor & self) const {
-    AT_ERROR("is_coalesced not supported on CUDAFloatType");
-}
-Tensor CUDAFloatType::_indices(const Tensor & self) const {
-    AT_ERROR("_indices not supported on CUDAFloatType");
-}
-Tensor CUDAFloatType::_values(const Tensor & self) const {
-    AT_ERROR("_values not supported on CUDAFloatType");
-}
-Tensor & CUDAFloatType::_coalesced_(Tensor & self, bool coalesced) const {
-    AT_ERROR("_coalesced_ not supported on CUDAFloatType");
-}
-Tensor CUDAFloatType::indices(const Tensor & self) const {
-    AT_ERROR("indices not supported on CUDAFloatType");
-}
-Tensor CUDAFloatType::values(const Tensor & self) const {
-    AT_ERROR("values not supported on CUDAFloatType");
-}
-Tensor & CUDAFloatType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm_out not supported on CUDAFloatType");
-}
-Tensor CUDAFloatType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm not supported on CUDAFloatType");
-}
-Tensor & CUDAFloatType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
-    AT_ERROR("copy_sparse_to_sparse_ not supported on CUDAFloatType");
-}
 Tensor CUDAFloatType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
diff --git a/build/aten/src/ATen/CUDAFloatType.h b/build/aten/src/ATen/CUDAFloatType.h
index f472fc18d..9ccf7abfe 100644
--- a/build/aten/src/ATen/CUDAFloatType.h
+++ b/build/aten/src/ATen/CUDAFloatType.h
@@ -551,7 +551,6 @@ struct CUDAFloatType final : public CUDATypeDefault {
   Tensor & clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const override;
   Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor _s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const override;
-  void _copy_same_type_(Tensor & self, const Tensor & src) const override;
   Tensor & cos_(Tensor & self) const override;
   Tensor & cos_out(Tensor & out, const Tensor & self) const override;
   Tensor & cosh_(Tensor & self) const override;
@@ -656,13 +655,7 @@ struct CUDAFloatType final : public CUDATypeDefault {
   Tensor & sinh_out(Tensor & out, const Tensor & self) const override;
   Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  Tensor & _sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const override;
   Tensor & _sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const override;
-  Tensor & _sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
-  Tensor & _sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
   Tensor & sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & sqrt_(Tensor & self) const override;
   Tensor & sqrt_out(Tensor & out, const Tensor & self) const override;
@@ -682,35 +675,10 @@ struct CUDAFloatType final : public CUDATypeDefault {
   Tensor _standard_gamma_grad(const Tensor & self, const Tensor & output) const override;
   Tensor _standard_gamma(const Tensor & self, Generator * generator) const override;
   Tensor poisson(const Tensor & self, Generator * generator) const override;
-  Tensor native_norm(const Tensor & self, Scalar p) const override;
-  Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const override;
-  Tensor native_clone(const Tensor & self) const override;
-  Tensor & native_resize_as_(Tensor & self, const Tensor & the_template) const override;
-  Tensor & native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const override;
-  Tensor native_pow(const Tensor & self, Scalar exponent) const override;
-  Tensor & native_zero_(Tensor & self) const override;
   Tensor & s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
-  Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const override;
-  Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const override;
-  Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
-  Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
   Tensor sparse_mask(const Tensor & self, SparseTensorRef mask) const override;
-  Tensor to_dense(const Tensor & self) const override;
-  int64_t sparse_dim(const Tensor & self) const override;
-  int64_t dense_dim(const Tensor & self) const override;
-  int64_t _nnz(const Tensor & self) const override;
-  Tensor coalesce(const Tensor & self) const override;
-  bool is_coalesced(const Tensor & self) const override;
-  Tensor _indices(const Tensor & self) const override;
-  Tensor _values(const Tensor & self) const override;
-  Tensor & _coalesced_(Tensor & self, bool coalesced) const override;
-  Tensor indices(const Tensor & self) const override;
-  Tensor values(const Tensor & self) const override;
-  Tensor & hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor hspmm(const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor to_sparse(const Tensor & self, int64_t sparse_dim) const override;
   Tensor to_sparse(const Tensor & self) const override;
   Scalar _local_scalar_dense(const Tensor & self) const override;
diff --git a/build/aten/src/ATen/CUDAHalfType.cpp b/build/aten/src/ATen/CUDAHalfType.cpp
index 8310dc529..49e7e0df2 100644
--- a/build/aten/src/ATen/CUDAHalfType.cpp
+++ b/build/aten/src/ATen/CUDAHalfType.cpp
@@ -5077,9 +5077,6 @@ Tensor CUDAHalfType::_s_copy_from(const Tensor & self, const Tensor & dst, bool
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_copy_from_cuda(/* actuals */ self, dst, non_blocking);
 }
-void CUDAHalfType::_copy_same_type_(Tensor & self, const Tensor & src) const {
-    AT_ERROR("_copy_same_type_ not supported on CUDAHalfType");
-}
 Tensor & CUDAHalfType::cos_(Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_cos__cuda(/* actuals */ self);
@@ -5496,28 +5493,10 @@ Tensor CUDAHalfType::_softmax_backward_data(const Tensor & grad_output, const Te
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::softmax_backward_cuda(/* actuals */ grad_output, output, dim, self);
 }
-Tensor & CUDAHalfType::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
-    AT_ERROR("_sparse_add_out not supported on CUDAHalfType");
-}
 Tensor & CUDAHalfType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::add_out_dense_sparse_cuda(/* actuals */ out, self, other, alpha);
 }
-Tensor & CUDAHalfType::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_div_zerodim_out not supported on CUDAHalfType");
-}
-Tensor & CUDAHalfType::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_div_scalar_out not supported on CUDAHalfType");
-}
-Tensor & CUDAHalfType::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_out not supported on CUDAHalfType");
-}
-Tensor & CUDAHalfType::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_zerodim_out not supported on CUDAHalfType");
-}
-Tensor & CUDAHalfType::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_mul_scalar_out not supported on CUDAHalfType");
-}
 Tensor & CUDAHalfType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_sspaddmm_out_only_sparse_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -5594,27 +5573,6 @@ Tensor CUDAHalfType::poisson(const Tensor & self, Generator * generator) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_poisson_cuda(/* actuals */ self, generator);
 }
-Tensor CUDAHalfType::native_norm(const Tensor & self, Scalar p) const {
-    AT_ERROR("native_norm not supported on CUDAHalfType");
-}
-Tensor CUDAHalfType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
-    AT_ERROR("_sparse_sum_backward not supported on CUDAHalfType");
-}
-Tensor CUDAHalfType::native_clone(const Tensor & self) const {
-    AT_ERROR("native_clone not supported on CUDAHalfType");
-}
-Tensor & CUDAHalfType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
-    AT_ERROR("native_resize_as_ not supported on CUDAHalfType");
-}
-Tensor & CUDAHalfType::native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow_out not supported on CUDAHalfType");
-}
-Tensor CUDAHalfType::native_pow(const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow not supported on CUDAHalfType");
-}
-Tensor & CUDAHalfType::native_zero_(Tensor & self) const {
-    AT_ERROR("native_zero_ not supported on CUDAHalfType");
-}
 Tensor & CUDAHalfType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_out_sparse_dense_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -5627,64 +5585,10 @@ Tensor & CUDAHalfType::s_native_addmm_(Tensor & self, const Tensor & mat1, const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_sparse_dense_cuda_(/* actuals */ self, mat1, mat2, beta, alpha);
 }
-Tensor CUDAHalfType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CUDAHalfType");
-}
-Tensor CUDAHalfType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CUDAHalfType");
-}
-Tensor & CUDAHalfType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_ not supported on CUDAHalfType");
-}
-Tensor & CUDAHalfType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_and_clear_ not supported on CUDAHalfType");
-}
 Tensor CUDAHalfType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::sparse_mask_cuda(/* actuals */ self, mask);
 }
-Tensor CUDAHalfType::to_dense(const Tensor & self) const {
-    AT_ERROR("to_dense not supported on CUDAHalfType");
-}
-int64_t CUDAHalfType::sparse_dim(const Tensor & self) const {
-    AT_ERROR("sparse_dim not supported on CUDAHalfType");
-}
-int64_t CUDAHalfType::dense_dim(const Tensor & self) const {
-    AT_ERROR("dense_dim not supported on CUDAHalfType");
-}
-int64_t CUDAHalfType::_nnz(const Tensor & self) const {
-    AT_ERROR("_nnz not supported on CUDAHalfType");
-}
-Tensor CUDAHalfType::coalesce(const Tensor & self) const {
-    AT_ERROR("coalesce not supported on CUDAHalfType");
-}
-bool CUDAHalfType::is_coalesced(const Tensor & self) const {
-    AT_ERROR("is_coalesced not supported on CUDAHalfType");
-}
-Tensor CUDAHalfType::_indices(const Tensor & self) const {
-    AT_ERROR("_indices not supported on CUDAHalfType");
-}
-Tensor CUDAHalfType::_values(const Tensor & self) const {
-    AT_ERROR("_values not supported on CUDAHalfType");
-}
-Tensor & CUDAHalfType::_coalesced_(Tensor & self, bool coalesced) const {
-    AT_ERROR("_coalesced_ not supported on CUDAHalfType");
-}
-Tensor CUDAHalfType::indices(const Tensor & self) const {
-    AT_ERROR("indices not supported on CUDAHalfType");
-}
-Tensor CUDAHalfType::values(const Tensor & self) const {
-    AT_ERROR("values not supported on CUDAHalfType");
-}
-Tensor & CUDAHalfType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm_out not supported on CUDAHalfType");
-}
-Tensor CUDAHalfType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm not supported on CUDAHalfType");
-}
-Tensor & CUDAHalfType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
-    AT_ERROR("copy_sparse_to_sparse_ not supported on CUDAHalfType");
-}
 Tensor CUDAHalfType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
diff --git a/build/aten/src/ATen/CUDAHalfType.h b/build/aten/src/ATen/CUDAHalfType.h
index 689b9a5e4..f58b60255 100644
--- a/build/aten/src/ATen/CUDAHalfType.h
+++ b/build/aten/src/ATen/CUDAHalfType.h
@@ -533,7 +533,6 @@ struct CUDAHalfType final : public CUDATypeDefault {
   Tensor & clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const override;
   Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor _s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const override;
-  void _copy_same_type_(Tensor & self, const Tensor & src) const override;
   Tensor & cos_(Tensor & self) const override;
   Tensor & cos_out(Tensor & out, const Tensor & self) const override;
   Tensor & cosh_(Tensor & self) const override;
@@ -638,13 +637,7 @@ struct CUDAHalfType final : public CUDATypeDefault {
   Tensor & sinh_out(Tensor & out, const Tensor & self) const override;
   Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  Tensor & _sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const override;
   Tensor & _sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const override;
-  Tensor & _sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
-  Tensor & _sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
   Tensor & sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & sqrt_(Tensor & self) const override;
   Tensor & sqrt_out(Tensor & out, const Tensor & self) const override;
@@ -664,35 +657,10 @@ struct CUDAHalfType final : public CUDATypeDefault {
   Tensor _standard_gamma_grad(const Tensor & self, const Tensor & output) const override;
   Tensor _standard_gamma(const Tensor & self, Generator * generator) const override;
   Tensor poisson(const Tensor & self, Generator * generator) const override;
-  Tensor native_norm(const Tensor & self, Scalar p) const override;
-  Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const override;
-  Tensor native_clone(const Tensor & self) const override;
-  Tensor & native_resize_as_(Tensor & self, const Tensor & the_template) const override;
-  Tensor & native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const override;
-  Tensor native_pow(const Tensor & self, Scalar exponent) const override;
-  Tensor & native_zero_(Tensor & self) const override;
   Tensor & s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
-  Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const override;
-  Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const override;
-  Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
-  Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
   Tensor sparse_mask(const Tensor & self, SparseTensorRef mask) const override;
-  Tensor to_dense(const Tensor & self) const override;
-  int64_t sparse_dim(const Tensor & self) const override;
-  int64_t dense_dim(const Tensor & self) const override;
-  int64_t _nnz(const Tensor & self) const override;
-  Tensor coalesce(const Tensor & self) const override;
-  bool is_coalesced(const Tensor & self) const override;
-  Tensor _indices(const Tensor & self) const override;
-  Tensor _values(const Tensor & self) const override;
-  Tensor & _coalesced_(Tensor & self, bool coalesced) const override;
-  Tensor indices(const Tensor & self) const override;
-  Tensor values(const Tensor & self) const override;
-  Tensor & hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor hspmm(const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor to_sparse(const Tensor & self, int64_t sparse_dim) const override;
   Tensor to_sparse(const Tensor & self) const override;
   Scalar _local_scalar_dense(const Tensor & self) const override;
diff --git a/build/aten/src/ATen/CUDAIntType.cpp b/build/aten/src/ATen/CUDAIntType.cpp
index 80a046b44..cc0aec2ad 100644
--- a/build/aten/src/ATen/CUDAIntType.cpp
+++ b/build/aten/src/ATen/CUDAIntType.cpp
@@ -2035,9 +2035,6 @@ Tensor CUDAIntType::_s_copy_from(const Tensor & self, const Tensor & dst, bool n
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_copy_from_cuda(/* actuals */ self, dst, non_blocking);
 }
-void CUDAIntType::_copy_same_type_(Tensor & self, const Tensor & src) const {
-    AT_ERROR("_copy_same_type_ not supported on CUDAIntType");
-}
 Tensor & CUDAIntType::cos_(Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_cos__cuda(/* actuals */ self);
@@ -2454,28 +2451,10 @@ Tensor CUDAIntType::_softmax_backward_data(const Tensor & grad_output, const Ten
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::softmax_backward_cuda(/* actuals */ grad_output, output, dim, self);
 }
-Tensor & CUDAIntType::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
-    AT_ERROR("_sparse_add_out not supported on CUDAIntType");
-}
 Tensor & CUDAIntType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::add_out_dense_sparse_cuda(/* actuals */ out, self, other, alpha);
 }
-Tensor & CUDAIntType::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_div_zerodim_out not supported on CUDAIntType");
-}
-Tensor & CUDAIntType::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_div_scalar_out not supported on CUDAIntType");
-}
-Tensor & CUDAIntType::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_out not supported on CUDAIntType");
-}
-Tensor & CUDAIntType::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_zerodim_out not supported on CUDAIntType");
-}
-Tensor & CUDAIntType::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_mul_scalar_out not supported on CUDAIntType");
-}
 Tensor & CUDAIntType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_sspaddmm_out_only_sparse_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2552,27 +2531,6 @@ Tensor CUDAIntType::poisson(const Tensor & self, Generator * generator) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_poisson_cuda(/* actuals */ self, generator);
 }
-Tensor CUDAIntType::native_norm(const Tensor & self, Scalar p) const {
-    AT_ERROR("native_norm not supported on CUDAIntType");
-}
-Tensor CUDAIntType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
-    AT_ERROR("_sparse_sum_backward not supported on CUDAIntType");
-}
-Tensor CUDAIntType::native_clone(const Tensor & self) const {
-    AT_ERROR("native_clone not supported on CUDAIntType");
-}
-Tensor & CUDAIntType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
-    AT_ERROR("native_resize_as_ not supported on CUDAIntType");
-}
-Tensor & CUDAIntType::native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow_out not supported on CUDAIntType");
-}
-Tensor CUDAIntType::native_pow(const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow not supported on CUDAIntType");
-}
-Tensor & CUDAIntType::native_zero_(Tensor & self) const {
-    AT_ERROR("native_zero_ not supported on CUDAIntType");
-}
 Tensor & CUDAIntType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_out_sparse_dense_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2585,64 +2543,10 @@ Tensor & CUDAIntType::s_native_addmm_(Tensor & self, const Tensor & mat1, const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_sparse_dense_cuda_(/* actuals */ self, mat1, mat2, beta, alpha);
 }
-Tensor CUDAIntType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CUDAIntType");
-}
-Tensor CUDAIntType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CUDAIntType");
-}
-Tensor & CUDAIntType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_ not supported on CUDAIntType");
-}
-Tensor & CUDAIntType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_and_clear_ not supported on CUDAIntType");
-}
 Tensor CUDAIntType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::sparse_mask_cuda(/* actuals */ self, mask);
 }
-Tensor CUDAIntType::to_dense(const Tensor & self) const {
-    AT_ERROR("to_dense not supported on CUDAIntType");
-}
-int64_t CUDAIntType::sparse_dim(const Tensor & self) const {
-    AT_ERROR("sparse_dim not supported on CUDAIntType");
-}
-int64_t CUDAIntType::dense_dim(const Tensor & self) const {
-    AT_ERROR("dense_dim not supported on CUDAIntType");
-}
-int64_t CUDAIntType::_nnz(const Tensor & self) const {
-    AT_ERROR("_nnz not supported on CUDAIntType");
-}
-Tensor CUDAIntType::coalesce(const Tensor & self) const {
-    AT_ERROR("coalesce not supported on CUDAIntType");
-}
-bool CUDAIntType::is_coalesced(const Tensor & self) const {
-    AT_ERROR("is_coalesced not supported on CUDAIntType");
-}
-Tensor CUDAIntType::_indices(const Tensor & self) const {
-    AT_ERROR("_indices not supported on CUDAIntType");
-}
-Tensor CUDAIntType::_values(const Tensor & self) const {
-    AT_ERROR("_values not supported on CUDAIntType");
-}
-Tensor & CUDAIntType::_coalesced_(Tensor & self, bool coalesced) const {
-    AT_ERROR("_coalesced_ not supported on CUDAIntType");
-}
-Tensor CUDAIntType::indices(const Tensor & self) const {
-    AT_ERROR("indices not supported on CUDAIntType");
-}
-Tensor CUDAIntType::values(const Tensor & self) const {
-    AT_ERROR("values not supported on CUDAIntType");
-}
-Tensor & CUDAIntType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm_out not supported on CUDAIntType");
-}
-Tensor CUDAIntType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm not supported on CUDAIntType");
-}
-Tensor & CUDAIntType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
-    AT_ERROR("copy_sparse_to_sparse_ not supported on CUDAIntType");
-}
 Tensor CUDAIntType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
diff --git a/build/aten/src/ATen/CUDAIntType.h b/build/aten/src/ATen/CUDAIntType.h
index 2a3ce28a9..94daaef8a 100644
--- a/build/aten/src/ATen/CUDAIntType.h
+++ b/build/aten/src/ATen/CUDAIntType.h
@@ -262,7 +262,6 @@ struct CUDAIntType final : public CUDATypeDefault {
   Tensor & clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const override;
   Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor _s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const override;
-  void _copy_same_type_(Tensor & self, const Tensor & src) const override;
   Tensor & cos_(Tensor & self) const override;
   Tensor & cos_out(Tensor & out, const Tensor & self) const override;
   Tensor & cosh_(Tensor & self) const override;
@@ -367,13 +366,7 @@ struct CUDAIntType final : public CUDATypeDefault {
   Tensor & sinh_out(Tensor & out, const Tensor & self) const override;
   Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  Tensor & _sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const override;
   Tensor & _sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const override;
-  Tensor & _sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
-  Tensor & _sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
   Tensor & sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & sqrt_(Tensor & self) const override;
   Tensor & sqrt_out(Tensor & out, const Tensor & self) const override;
@@ -393,35 +386,10 @@ struct CUDAIntType final : public CUDATypeDefault {
   Tensor _standard_gamma_grad(const Tensor & self, const Tensor & output) const override;
   Tensor _standard_gamma(const Tensor & self, Generator * generator) const override;
   Tensor poisson(const Tensor & self, Generator * generator) const override;
-  Tensor native_norm(const Tensor & self, Scalar p) const override;
-  Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const override;
-  Tensor native_clone(const Tensor & self) const override;
-  Tensor & native_resize_as_(Tensor & self, const Tensor & the_template) const override;
-  Tensor & native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const override;
-  Tensor native_pow(const Tensor & self, Scalar exponent) const override;
-  Tensor & native_zero_(Tensor & self) const override;
   Tensor & s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
-  Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const override;
-  Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const override;
-  Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
-  Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
   Tensor sparse_mask(const Tensor & self, SparseTensorRef mask) const override;
-  Tensor to_dense(const Tensor & self) const override;
-  int64_t sparse_dim(const Tensor & self) const override;
-  int64_t dense_dim(const Tensor & self) const override;
-  int64_t _nnz(const Tensor & self) const override;
-  Tensor coalesce(const Tensor & self) const override;
-  bool is_coalesced(const Tensor & self) const override;
-  Tensor _indices(const Tensor & self) const override;
-  Tensor _values(const Tensor & self) const override;
-  Tensor & _coalesced_(Tensor & self, bool coalesced) const override;
-  Tensor indices(const Tensor & self) const override;
-  Tensor values(const Tensor & self) const override;
-  Tensor & hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor hspmm(const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor to_sparse(const Tensor & self, int64_t sparse_dim) const override;
   Tensor to_sparse(const Tensor & self) const override;
   Scalar _local_scalar_dense(const Tensor & self) const override;
diff --git a/build/aten/src/ATen/CUDALongType.cpp b/build/aten/src/ATen/CUDALongType.cpp
index e00aa3a6e..b80b3116f 100644
--- a/build/aten/src/ATen/CUDALongType.cpp
+++ b/build/aten/src/ATen/CUDALongType.cpp
@@ -2035,9 +2035,6 @@ Tensor CUDALongType::_s_copy_from(const Tensor & self, const Tensor & dst, bool
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_copy_from_cuda(/* actuals */ self, dst, non_blocking);
 }
-void CUDALongType::_copy_same_type_(Tensor & self, const Tensor & src) const {
-    AT_ERROR("_copy_same_type_ not supported on CUDALongType");
-}
 Tensor & CUDALongType::cos_(Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_cos__cuda(/* actuals */ self);
@@ -2454,28 +2451,10 @@ Tensor CUDALongType::_softmax_backward_data(const Tensor & grad_output, const Te
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::softmax_backward_cuda(/* actuals */ grad_output, output, dim, self);
 }
-Tensor & CUDALongType::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
-    AT_ERROR("_sparse_add_out not supported on CUDALongType");
-}
 Tensor & CUDALongType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::add_out_dense_sparse_cuda(/* actuals */ out, self, other, alpha);
 }
-Tensor & CUDALongType::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_div_zerodim_out not supported on CUDALongType");
-}
-Tensor & CUDALongType::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_div_scalar_out not supported on CUDALongType");
-}
-Tensor & CUDALongType::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_out not supported on CUDALongType");
-}
-Tensor & CUDALongType::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_zerodim_out not supported on CUDALongType");
-}
-Tensor & CUDALongType::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_mul_scalar_out not supported on CUDALongType");
-}
 Tensor & CUDALongType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_sspaddmm_out_only_sparse_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2552,27 +2531,6 @@ Tensor CUDALongType::poisson(const Tensor & self, Generator * generator) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_poisson_cuda(/* actuals */ self, generator);
 }
-Tensor CUDALongType::native_norm(const Tensor & self, Scalar p) const {
-    AT_ERROR("native_norm not supported on CUDALongType");
-}
-Tensor CUDALongType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
-    AT_ERROR("_sparse_sum_backward not supported on CUDALongType");
-}
-Tensor CUDALongType::native_clone(const Tensor & self) const {
-    AT_ERROR("native_clone not supported on CUDALongType");
-}
-Tensor & CUDALongType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
-    AT_ERROR("native_resize_as_ not supported on CUDALongType");
-}
-Tensor & CUDALongType::native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow_out not supported on CUDALongType");
-}
-Tensor CUDALongType::native_pow(const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow not supported on CUDALongType");
-}
-Tensor & CUDALongType::native_zero_(Tensor & self) const {
-    AT_ERROR("native_zero_ not supported on CUDALongType");
-}
 Tensor & CUDALongType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_out_sparse_dense_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2585,64 +2543,10 @@ Tensor & CUDALongType::s_native_addmm_(Tensor & self, const Tensor & mat1, const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_sparse_dense_cuda_(/* actuals */ self, mat1, mat2, beta, alpha);
 }
-Tensor CUDALongType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CUDALongType");
-}
-Tensor CUDALongType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CUDALongType");
-}
-Tensor & CUDALongType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_ not supported on CUDALongType");
-}
-Tensor & CUDALongType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_and_clear_ not supported on CUDALongType");
-}
 Tensor CUDALongType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::sparse_mask_cuda(/* actuals */ self, mask);
 }
-Tensor CUDALongType::to_dense(const Tensor & self) const {
-    AT_ERROR("to_dense not supported on CUDALongType");
-}
-int64_t CUDALongType::sparse_dim(const Tensor & self) const {
-    AT_ERROR("sparse_dim not supported on CUDALongType");
-}
-int64_t CUDALongType::dense_dim(const Tensor & self) const {
-    AT_ERROR("dense_dim not supported on CUDALongType");
-}
-int64_t CUDALongType::_nnz(const Tensor & self) const {
-    AT_ERROR("_nnz not supported on CUDALongType");
-}
-Tensor CUDALongType::coalesce(const Tensor & self) const {
-    AT_ERROR("coalesce not supported on CUDALongType");
-}
-bool CUDALongType::is_coalesced(const Tensor & self) const {
-    AT_ERROR("is_coalesced not supported on CUDALongType");
-}
-Tensor CUDALongType::_indices(const Tensor & self) const {
-    AT_ERROR("_indices not supported on CUDALongType");
-}
-Tensor CUDALongType::_values(const Tensor & self) const {
-    AT_ERROR("_values not supported on CUDALongType");
-}
-Tensor & CUDALongType::_coalesced_(Tensor & self, bool coalesced) const {
-    AT_ERROR("_coalesced_ not supported on CUDALongType");
-}
-Tensor CUDALongType::indices(const Tensor & self) const {
-    AT_ERROR("indices not supported on CUDALongType");
-}
-Tensor CUDALongType::values(const Tensor & self) const {
-    AT_ERROR("values not supported on CUDALongType");
-}
-Tensor & CUDALongType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm_out not supported on CUDALongType");
-}
-Tensor CUDALongType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm not supported on CUDALongType");
-}
-Tensor & CUDALongType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
-    AT_ERROR("copy_sparse_to_sparse_ not supported on CUDALongType");
-}
 Tensor CUDALongType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
diff --git a/build/aten/src/ATen/CUDALongType.h b/build/aten/src/ATen/CUDALongType.h
index 21900b539..c3da73611 100644
--- a/build/aten/src/ATen/CUDALongType.h
+++ b/build/aten/src/ATen/CUDALongType.h
@@ -262,7 +262,6 @@ struct CUDALongType final : public CUDATypeDefault {
   Tensor & clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const override;
   Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor _s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const override;
-  void _copy_same_type_(Tensor & self, const Tensor & src) const override;
   Tensor & cos_(Tensor & self) const override;
   Tensor & cos_out(Tensor & out, const Tensor & self) const override;
   Tensor & cosh_(Tensor & self) const override;
@@ -367,13 +366,7 @@ struct CUDALongType final : public CUDATypeDefault {
   Tensor & sinh_out(Tensor & out, const Tensor & self) const override;
   Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  Tensor & _sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const override;
   Tensor & _sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const override;
-  Tensor & _sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
-  Tensor & _sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
   Tensor & sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & sqrt_(Tensor & self) const override;
   Tensor & sqrt_out(Tensor & out, const Tensor & self) const override;
@@ -393,35 +386,10 @@ struct CUDALongType final : public CUDATypeDefault {
   Tensor _standard_gamma_grad(const Tensor & self, const Tensor & output) const override;
   Tensor _standard_gamma(const Tensor & self, Generator * generator) const override;
   Tensor poisson(const Tensor & self, Generator * generator) const override;
-  Tensor native_norm(const Tensor & self, Scalar p) const override;
-  Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const override;
-  Tensor native_clone(const Tensor & self) const override;
-  Tensor & native_resize_as_(Tensor & self, const Tensor & the_template) const override;
-  Tensor & native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const override;
-  Tensor native_pow(const Tensor & self, Scalar exponent) const override;
-  Tensor & native_zero_(Tensor & self) const override;
   Tensor & s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
-  Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const override;
-  Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const override;
-  Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
-  Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
   Tensor sparse_mask(const Tensor & self, SparseTensorRef mask) const override;
-  Tensor to_dense(const Tensor & self) const override;
-  int64_t sparse_dim(const Tensor & self) const override;
-  int64_t dense_dim(const Tensor & self) const override;
-  int64_t _nnz(const Tensor & self) const override;
-  Tensor coalesce(const Tensor & self) const override;
-  bool is_coalesced(const Tensor & self) const override;
-  Tensor _indices(const Tensor & self) const override;
-  Tensor _values(const Tensor & self) const override;
-  Tensor & _coalesced_(Tensor & self, bool coalesced) const override;
-  Tensor indices(const Tensor & self) const override;
-  Tensor values(const Tensor & self) const override;
-  Tensor & hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor hspmm(const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor to_sparse(const Tensor & self, int64_t sparse_dim) const override;
   Tensor to_sparse(const Tensor & self) const override;
   Scalar _local_scalar_dense(const Tensor & self) const override;
diff --git a/build/aten/src/ATen/CUDAShortType.cpp b/build/aten/src/ATen/CUDAShortType.cpp
index 4227e7001..c3540e99a 100644
--- a/build/aten/src/ATen/CUDAShortType.cpp
+++ b/build/aten/src/ATen/CUDAShortType.cpp
@@ -2035,9 +2035,6 @@ Tensor CUDAShortType::_s_copy_from(const Tensor & self, const Tensor & dst, bool
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_copy_from_cuda(/* actuals */ self, dst, non_blocking);
 }
-void CUDAShortType::_copy_same_type_(Tensor & self, const Tensor & src) const {
-    AT_ERROR("_copy_same_type_ not supported on CUDAShortType");
-}
 Tensor & CUDAShortType::cos_(Tensor & self) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_cos__cuda(/* actuals */ self);
@@ -2454,28 +2451,10 @@ Tensor CUDAShortType::_softmax_backward_data(const Tensor & grad_output, const T
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::softmax_backward_cuda(/* actuals */ grad_output, output, dim, self);
 }
-Tensor & CUDAShortType::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
-    AT_ERROR("_sparse_add_out not supported on CUDAShortType");
-}
 Tensor & CUDAShortType::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::add_out_dense_sparse_cuda(/* actuals */ out, self, other, alpha);
 }
-Tensor & CUDAShortType::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_div_zerodim_out not supported on CUDAShortType");
-}
-Tensor & CUDAShortType::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_div_scalar_out not supported on CUDAShortType");
-}
-Tensor & CUDAShortType::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_out not supported on CUDAShortType");
-}
-Tensor & CUDAShortType::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
-    AT_ERROR("_sparse_mul_zerodim_out not supported on CUDAShortType");
-}
-Tensor & CUDAShortType::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
-    AT_ERROR("_sparse_mul_scalar_out not supported on CUDAShortType");
-}
 Tensor & CUDAShortType::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_sspaddmm_out_only_sparse_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2552,27 +2531,6 @@ Tensor CUDAShortType::poisson(const Tensor & self, Generator * generator) const
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::_s_poisson_cuda(/* actuals */ self, generator);
 }
-Tensor CUDAShortType::native_norm(const Tensor & self, Scalar p) const {
-    AT_ERROR("native_norm not supported on CUDAShortType");
-}
-Tensor CUDAShortType::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
-    AT_ERROR("_sparse_sum_backward not supported on CUDAShortType");
-}
-Tensor CUDAShortType::native_clone(const Tensor & self) const {
-    AT_ERROR("native_clone not supported on CUDAShortType");
-}
-Tensor & CUDAShortType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
-    AT_ERROR("native_resize_as_ not supported on CUDAShortType");
-}
-Tensor & CUDAShortType::native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow_out not supported on CUDAShortType");
-}
-Tensor CUDAShortType::native_pow(const Tensor & self, Scalar exponent) const {
-    AT_ERROR("native_pow not supported on CUDAShortType");
-}
-Tensor & CUDAShortType::native_zero_(Tensor & self) const {
-    AT_ERROR("native_zero_ not supported on CUDAShortType");
-}
 Tensor & CUDAShortType::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_out_sparse_dense_cuda(/* actuals */ out, self, mat1, mat2, beta, alpha);
@@ -2585,64 +2543,10 @@ Tensor & CUDAShortType::s_native_addmm_(Tensor & self, const Tensor & mat1, cons
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::s_addmm_sparse_dense_cuda_(/* actuals */ self, mat1, mat2, beta, alpha);
 }
-Tensor CUDAShortType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CUDAShortType");
-}
-Tensor CUDAShortType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
-    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CUDAShortType");
-}
-Tensor & CUDAShortType::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_ not supported on CUDAShortType");
-}
-Tensor & CUDAShortType::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
-    AT_ERROR("sparse_resize_and_clear_ not supported on CUDAShortType");
-}
 Tensor CUDAShortType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::sparse_mask_cuda(/* actuals */ self, mask);
 }
-Tensor CUDAShortType::to_dense(const Tensor & self) const {
-    AT_ERROR("to_dense not supported on CUDAShortType");
-}
-int64_t CUDAShortType::sparse_dim(const Tensor & self) const {
-    AT_ERROR("sparse_dim not supported on CUDAShortType");
-}
-int64_t CUDAShortType::dense_dim(const Tensor & self) const {
-    AT_ERROR("dense_dim not supported on CUDAShortType");
-}
-int64_t CUDAShortType::_nnz(const Tensor & self) const {
-    AT_ERROR("_nnz not supported on CUDAShortType");
-}
-Tensor CUDAShortType::coalesce(const Tensor & self) const {
-    AT_ERROR("coalesce not supported on CUDAShortType");
-}
-bool CUDAShortType::is_coalesced(const Tensor & self) const {
-    AT_ERROR("is_coalesced not supported on CUDAShortType");
-}
-Tensor CUDAShortType::_indices(const Tensor & self) const {
-    AT_ERROR("_indices not supported on CUDAShortType");
-}
-Tensor CUDAShortType::_values(const Tensor & self) const {
-    AT_ERROR("_values not supported on CUDAShortType");
-}
-Tensor & CUDAShortType::_coalesced_(Tensor & self, bool coalesced) const {
-    AT_ERROR("_coalesced_ not supported on CUDAShortType");
-}
-Tensor CUDAShortType::indices(const Tensor & self) const {
-    AT_ERROR("indices not supported on CUDAShortType");
-}
-Tensor CUDAShortType::values(const Tensor & self) const {
-    AT_ERROR("values not supported on CUDAShortType");
-}
-Tensor & CUDAShortType::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm_out not supported on CUDAShortType");
-}
-Tensor CUDAShortType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
-    AT_ERROR("hspmm not supported on CUDAShortType");
-}
-Tensor & CUDAShortType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
-    AT_ERROR("copy_sparse_to_sparse_ not supported on CUDAShortType");
-}
 Tensor CUDAShortType::to_sparse(const Tensor & self, int64_t sparse_dim) const {
     const OptionalDeviceGuard device_guard(device_of(self));
     return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
diff --git a/build/aten/src/ATen/CUDAShortType.h b/build/aten/src/ATen/CUDAShortType.h
index 555c2343c..d6146b3dd 100644
--- a/build/aten/src/ATen/CUDAShortType.h
+++ b/build/aten/src/ATen/CUDAShortType.h
@@ -262,7 +262,6 @@ struct CUDAShortType final : public CUDATypeDefault {
   Tensor & clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const override;
   Tensor & s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor _s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const override;
-  void _copy_same_type_(Tensor & self, const Tensor & src) const override;
   Tensor & cos_(Tensor & self) const override;
   Tensor & cos_out(Tensor & out, const Tensor & self) const override;
   Tensor & cosh_(Tensor & self) const override;
@@ -367,13 +366,7 @@ struct CUDAShortType final : public CUDATypeDefault {
   Tensor & sinh_out(Tensor & out, const Tensor & self) const override;
   Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) const override;
   Tensor _softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const override;
-  Tensor & _sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const override;
   Tensor & _sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const override;
-  Tensor & _sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
-  Tensor & _sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const override;
-  Tensor & _sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const override;
   Tensor & sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & sqrt_(Tensor & self) const override;
   Tensor & sqrt_out(Tensor & out, const Tensor & self) const override;
@@ -393,35 +386,10 @@ struct CUDAShortType final : public CUDATypeDefault {
   Tensor _standard_gamma_grad(const Tensor & self, const Tensor & output) const override;
   Tensor _standard_gamma(const Tensor & self, Generator * generator) const override;
   Tensor poisson(const Tensor & self, Generator * generator) const override;
-  Tensor native_norm(const Tensor & self, Scalar p) const override;
-  Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const override;
-  Tensor native_clone(const Tensor & self) const override;
-  Tensor & native_resize_as_(Tensor & self, const Tensor & the_template) const override;
-  Tensor & native_pow_out(Tensor & out, const Tensor & self, Scalar exponent) const override;
-  Tensor native_pow(const Tensor & self, Scalar exponent) const override;
-  Tensor & native_zero_(Tensor & self) const override;
   Tensor & s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
   Tensor & s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const override;
-  Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const override;
-  Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const override;
-  Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
-  Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const override;
   Tensor sparse_mask(const Tensor & self, SparseTensorRef mask) const override;
-  Tensor to_dense(const Tensor & self) const override;
-  int64_t sparse_dim(const Tensor & self) const override;
-  int64_t dense_dim(const Tensor & self) const override;
-  int64_t _nnz(const Tensor & self) const override;
-  Tensor coalesce(const Tensor & self) const override;
-  bool is_coalesced(const Tensor & self) const override;
-  Tensor _indices(const Tensor & self) const override;
-  Tensor _values(const Tensor & self) const override;
-  Tensor & _coalesced_(Tensor & self, bool coalesced) const override;
-  Tensor indices(const Tensor & self) const override;
-  Tensor values(const Tensor & self) const override;
-  Tensor & hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor hspmm(const Tensor & mat1, const Tensor & mat2) const override;
-  Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const override;
   Tensor to_sparse(const Tensor & self, int64_t sparse_dim) const override;
   Tensor to_sparse(const Tensor & self) const override;
   Scalar _local_scalar_dense(const Tensor & self) const override;
