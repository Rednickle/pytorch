

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>PyTorch 0.1.11 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  
    <link rel="stylesheet" href="_static/css/pytorch_theme.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="None" href="index.html#document-index"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html#document-index">
          

          
            
            <img src="_static/pytorch-logo-dark.svg" class="logo" />
          
          </a>

          

          

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
            
              <!-- Local TOC -->
              <div class="local-toc"><p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-notes/autograd">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-notes/cuda">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-notes/extending">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-notes/multiprocessing">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-notes/serialization">Serialization semantics</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-torch">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-tensors">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-storage">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-nn">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#torch-nn-functional">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-optim">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-autograd">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-multiprocessing">torch.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-legacy">torch.legacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-cuda">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-ffi">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-data">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-model_zoo">torch.utils.model_zoo</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-torchvision/torchvision">torchvision</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-torchvision/datasets">torchvision.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-torchvision/models">torchvision.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-torchvision/transforms">torchvision.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-torchvision/utils">torchvision.utils</a></li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html#document-index">PyTorch</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html#document-index">Docs</a> &raquo;</li>
        
      <li>PyTorch 0.1.11 documentation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="pytorch-documentation">
<h1>PyTorch documentation<a class="headerlink" href="#pytorch-documentation" title="Permalink to this headline">¶</a></h1>
<p>PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.</p>
<div class="toctree-wrapper compound">
<span id="document-notes/autograd"></span><div class="section" id="autograd-mechanics">
<h2>Autograd mechanics<a class="headerlink" href="#autograd-mechanics" title="Permalink to this headline">¶</a></h2>
<p>This note will present an overview of how autograd works and records the
operations. It&#8217;s not strictly necessary to understand all this, but we recommend
getting familiar with it, as it will help you write more efficient, cleaner
programs, and can aid you in debugging.</p>
<div class="section" id="excluding-subgraphs-from-backward">
<span id="excluding-subgraphs"></span><h3>Excluding subgraphs from backward<a class="headerlink" href="#excluding-subgraphs-from-backward" title="Permalink to this headline">¶</a></h3>
<p>Every Variable has two flags: <code class="xref py py-attr docutils literal"><span class="pre">requires_grad</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">volatile</span></code>.
They both allow for fine grained exclusion of subgraphs from gradient
computation and can increase efficiency.</p>
<div class="section" id="requires-grad">
<span id="excluding-requires-grad"></span><h4><code class="docutils literal"><span class="pre">requires_grad</span></code><a class="headerlink" href="#requires-grad" title="Permalink to this headline">¶</a></h4>
<p>If there&#8217;s a single input to an operation that requires gradient, its output
will also require gradient. Conversely, only if all inputs don&#8217;t require
gradient, the output also won&#8217;t require it. Backward computation is never
performed in the subgraphs, where all Variables didn&#8217;t require gradients.</p>
<div class="code highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">True</span>
</pre></div>
</div>
<p>This is especially useful when you want to freeze part of your model, or you
know in advance that you&#8217;re not going to use gradients w.r.t. some parameters.
For example if you want to finetune a pretrained CNN, it&#8217;s enough to switch the
<code class="xref py py-attr docutils literal"><span class="pre">requires_grad</span></code> flags in the frozen base, and no intermediate buffers will
be saved, until the computation gets to the last layer, where the affine
transform will use weights that require gradient, and the output of the network
will also require them.</p>
<div class="code highlight-default"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
<span class="c1"># Replace the last fully-connected layer</span>
<span class="c1"># Parameters of newly constructed modules have requires_grad=True by default</span>
<span class="n">model</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Optimize only the classifier</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="volatile">
<h4><code class="docutils literal"><span class="pre">volatile</span></code><a class="headerlink" href="#volatile" title="Permalink to this headline">¶</a></h4>
<p>Volatile is recommended for purely inference mode, when you&#8217;re sure you won&#8217;t
be even calling <cite>.backward()</cite>. It&#8217;s more efficient than any other autograd
setting - it will use the absolute minimal amount of memory to evaluate the
model. <code class="docutils literal"><span class="pre">volatile</span></code> also determines that <code class="docutils literal"><span class="pre">requires_grad</span> <span class="pre">is</span> <span class="pre">False</span></code>.</p>
<p>Volatile differs from <a class="reference internal" href="#excluding-requires-grad"><span class="std std-ref">requires_grad</span></a> in how the flag propagates.
If there&#8217;s even a single volatile input to an operation, its output is also
going to be volatile. Volatility spreads accross the graph much easier than
non-requiring gradient - you only need a <strong>single</strong> volatile leaf to have a
volatile output, while you need <strong>all</strong> leaves to not require gradient to
have an output the doesn&#8217;t require gradient. Using volatile flag you don&#8217;t
need to change any settings of your model parameters to use it for
inference. It&#8217;s enough to create a volatile input, and this will ensure that
no intermediate states are saved.</p>
<div class="code highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">regular_input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">volatile_input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">volatile</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="p">(</span><span class="n">regular_input</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="p">(</span><span class="n">volatile_input</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="p">(</span><span class="n">volatile_input</span><span class="p">)</span><span class="o">.</span><span class="n">volatile</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="p">(</span><span class="n">volatile_input</span><span class="p">)</span><span class="o">.</span><span class="n">creator</span> <span class="ow">is</span> <span class="kc">None</span>
<span class="go">True</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="how-autograd-encodes-the-history">
<h3>How autograd encodes the history<a class="headerlink" href="#how-autograd-encodes-the-history" title="Permalink to this headline">¶</a></h3>
<p>Each Variable has a <code class="docutils literal"><span class="pre">.creator</span></code> attribute, that points to the function, of
which it is an output. This is an entry point to a directed acyclic graph (DAG)
consisting of <code class="xref py py-class docutils literal"><span class="pre">Function</span></code> objects as nodes, and references between them
being the edges. Every time an operation is performed, a new <code class="xref py py-class docutils literal"><span class="pre">Function</span></code>
representing it is instantiated, its <a class="reference internal" href="index.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal"><span class="pre">forward()</span></code></a>
method is called, and its output <code class="xref py py-class docutils literal"><span class="pre">Variable</span></code> s creators are set to it.
Then, by following the path from any <code class="xref py py-class docutils literal"><span class="pre">Variable</span></code> to the leaves, it is
possible to reconstruct the sequence of operations that has created the data,
and automatically compute the gradients.</p>
<p>An important thing to note is that the graph is recreated from scratch at every
iteration, and this is exactly what allows for using arbitrary Python control
flow statements, that can change the overall shape and size of the graph at
every iteration. You don&#8217;t have to encode all possible paths before you
launch the training - what you run is what you differentiate.</p>
</div>
<div class="section" id="in-place-operations-on-variables">
<h3>In-place operations on Variables<a class="headerlink" href="#in-place-operations-on-variables" title="Permalink to this headline">¶</a></h3>
<p>Supporting in-place operations in autograd is a hard matter, and we discourage
their use in most cases. Autograd&#8217;s aggressive buffer freeing and reuse makes
it very efficient and there are very few occasions when in-place operations
actually lower memory usage by any significant amount. Unless you&#8217;re operating
under heavy memory pressure, you might never need to use them.</p>
<p>There are two main reasons that limit the applicability of in-place operations:</p>
<ol class="arabic simple">
<li>Overwriting values required to compute gradients. This is why variables don&#8217;t
support <code class="docutils literal"><span class="pre">log_</span></code>. Its gradient formula requires the original input, and while
it is possible to recreate it by computing the inverse operation, it is
numerically unstable, and requires additional work that often defeats the
purpose of using these functions.</li>
<li>Every in-place operation actually requires the implementation to rewrite the
computational graph. Out-of-place versions simply allocate new objects and
keep references to the old graph, while in-place operations, require
changing the creator of all inputs to the <code class="xref py py-class docutils literal"><span class="pre">Function</span></code> representing
this operation. This can be tricky, especially if there are many Variables
that reference the same storage (e.g. created by indexing or transposing),
and in-place functions will actually raise an error if the storage of
modified inputs is referenced by any other <code class="xref py py-class docutils literal"><span class="pre">Variable</span></code>.</li>
</ol>
</div>
<div class="section" id="in-place-correctness-checks">
<h3>In-place correctness checks<a class="headerlink" href="#in-place-correctness-checks" title="Permalink to this headline">¶</a></h3>
<p>Every variable keeps a version counter, that is incremented every time it&#8217;s
marked dirty in any operation. When a Function saves any tensors for backward,
a version counter of their containing Variable is saved as well. Once you access
<code class="docutils literal"><span class="pre">self.saved_tensors</span></code> it is checked, and if it&#8217;s greater than the saved value
an error is raised.</p>
</div>
</div>
<span id="document-notes/cuda"></span><div class="section" id="cuda-semantics">
<span id="id1"></span><h2>CUDA semantics<a class="headerlink" href="#cuda-semantics" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="index.html#module-torch.cuda" title="torch.cuda"><code class="xref py py-mod docutils literal"><span class="pre">torch.cuda</span></code></a> keeps track of currently selected GPU, and all CUDA tensors
you allocate will be created on it. The selected device can be changed with a
<a class="reference internal" href="index.html#torch.cuda.device" title="torch.cuda.device"><code class="xref any py py-class docutils literal"><span class="pre">torch.cuda.device</span></code></a> context manager.</p>
<p>However, once a tensor is allocated, you can do operations on it irrespectively
of your selected device, and the results will be always placed in on the same
device as the tensor.</p>
<p>Cross-GPU operations are not allowed by default, with the only exception of
<a class="reference internal" href="index.html#torch.Tensor.copy_" title="torch.Tensor.copy_"><code class="xref py py-meth docutils literal"><span class="pre">copy_()</span></code></a>. Unless you enable peer-to-peer memory accesses
any attempts to launch ops on tensors spread across different devices will
raise an error.</p>
<p>Below you can find a small example showcasing this:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># x.get_device() == 0</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="c1"># y.get_device() == 0</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># allocates a tensor on GPU 1</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># transfers a tensor from CPU to GPU 1</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="c1"># a.get_device() == b.get_device() == 1</span>

    <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="c1"># c.get_device() == 1</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="c1"># z.get_device() == 0</span>

    <span class="c1"># even within a context, you can give a GPU id to the .cuda call</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># d.get_device() == 2</span>
</pre></div>
</div>
<div class="section" id="best-practices">
<h3>Best practices<a class="headerlink" href="#best-practices" title="Permalink to this headline">¶</a></h3>
<div class="section" id="use-pinned-memory-buffers">
<h4>Use pinned memory buffers<a class="headerlink" href="#use-pinned-memory-buffers" title="Permalink to this headline">¶</a></h4>
<p>Host to GPU copies are much faster when they originate from pinned (page-locked)
memory. CPU tensors and storages expose a <a class="reference internal" href="index.html#torch.Tensor.pin_memory" title="torch.Tensor.pin_memory"><code class="xref py py-meth docutils literal"><span class="pre">pin_memory()</span></code></a>
method, that returns a copy of the object, with data put in a pinned region.</p>
<p>Also, once you pin a tensor or storage, you can use asynchronous GPU copies.
Just pass an additional <code class="docutils literal"><span class="pre">async=True</span></code> argument to a <a class="reference internal" href="index.html#torch.Tensor.cuda" title="torch.Tensor.cuda"><code class="xref py py-meth docutils literal"><span class="pre">cuda()</span></code></a>
call. This can be used to overlap data transfers with computation.</p>
<p>You can make the <a class="reference internal" href="index.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code class="xref py py-class docutils literal"><span class="pre">DataLoader</span></code></a> return batches placed in
pinned memory by passing <code class="docutils literal"><span class="pre">pin_memory=True</span></code> to its constructor.</p>
</div>
<div class="section" id="use-nn-dataparallel-instead-of-multiprocessing">
<span id="cuda-nn-dataparallel-instead"></span><h4>Use nn.DataParallel instead of multiprocessing<a class="headerlink" href="#use-nn-dataparallel-instead-of-multiprocessing" title="Permalink to this headline">¶</a></h4>
<p>Most use cases involving batched input and multiple GPUs should default to using
<a class="reference internal" href="index.html#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal"><span class="pre">DataParallel</span></code></a> to utilize more than one GPU. Even with the GIL,
a single python process can saturate multiple GPUs.</p>
<p>As of version 0.1.9, large numbers of GPUs (8+) might not be fully utilized.
However, this is a known issue that is under active development. As always,
test your use case.</p>
<p>There are significant caveats to using CUDA models with
<a class="reference internal" href="index.html#module-torch.multiprocessing" title="torch.multiprocessing"><code class="xref py py-mod docutils literal"><span class="pre">multiprocessing</span></code></a>; unless care is taken to meet the data handling
requirements exactly, it is likely that your program will have incorrect or
undefined behavior.</p>
</div>
</div>
</div>
<span id="document-notes/extending"></span><div class="section" id="extending-pytorch">
<h2>Extending PyTorch<a class="headerlink" href="#extending-pytorch" title="Permalink to this headline">¶</a></h2>
<p>In this note we&#8217;ll cover ways of extending <a class="reference internal" href="index.html#module-torch.nn" title="torch.nn"><code class="xref py py-mod docutils literal"><span class="pre">torch.nn</span></code></a>,
<a class="reference internal" href="index.html#module-torch.autograd" title="torch.autograd"><code class="xref py py-mod docutils literal"><span class="pre">torch.autograd</span></code></a>, and writing custom C extensions utilizing our C
libraries.</p>
<div class="section" id="extending-torch-autograd">
<h3>Extending <a class="reference internal" href="index.html#module-torch.autograd" title="torch.autograd"><code class="xref py py-mod docutils literal"><span class="pre">torch.autograd</span></code></a><a class="headerlink" href="#extending-torch-autograd" title="Permalink to this headline">¶</a></h3>
<p>Adding operations to <a class="reference internal" href="index.html#module-torch.autograd" title="torch.autograd"><code class="xref py py-mod docutils literal"><span class="pre">autograd</span></code></a> requires implementing a new
<a class="reference internal" href="index.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal"><span class="pre">Function</span></code></a> subclass for each operation. Recall that <a class="reference internal" href="index.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal"><span class="pre">Function</span></code></a> s
are what <a class="reference internal" href="index.html#module-torch.autograd" title="torch.autograd"><code class="xref py py-mod docutils literal"><span class="pre">autograd</span></code></a> uses to compute the results and gradients, and
encode the operation history. Every new function requires you to implement 3
methods:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">__init__</span></code> (<em>optional</em>) - if your operation is parametrized by/uses
objects different than <a class="reference internal" href="index.html#torch.autograd.Variable" title="torch.autograd.Variable"><code class="xref py py-class docutils literal"><span class="pre">Variable</span></code></a> s, you should pass them as arguments
to <code class="docutils literal"><span class="pre">__init__</span></code>. For example, <code class="docutils literal"><span class="pre">AddConstant</span></code> function takes a scalar to add,
while <code class="docutils literal"><span class="pre">Transpose</span></code> requires specifying which two dimensions to swap. If your
function doesn&#8217;t require any additional parameters, you can skip it.</li>
<li><a class="reference internal" href="index.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal"><span class="pre">forward()</span></code></a> - the code that performs the operation. It can take
as many arguments as you want, with some of them being
optional, if you specify the default values. Keep in mind that only
<a class="reference internal" href="index.html#torch.autograd.Variable" title="torch.autograd.Variable"><code class="xref py py-class docutils literal"><span class="pre">Variable</span></code></a> s will be passed in here. You can return either a single
<a class="reference internal" href="index.html#torch.autograd.Variable" title="torch.autograd.Variable"><code class="xref py py-class docutils literal"><span class="pre">Variable</span></code></a> output, or a <code class="xref py py-class docutils literal"><span class="pre">tuple</span></code> of <a class="reference internal" href="index.html#torch.autograd.Variable" title="torch.autograd.Variable"><code class="xref py py-class docutils literal"><span class="pre">Variable</span></code></a> s if there
are multiple. Also, please refer to the docs of <a class="reference internal" href="index.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal"><span class="pre">Function</span></code></a> to find
descriptions of useful methods that can be called only from
<a class="reference internal" href="index.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal"><span class="pre">forward()</span></code></a>.</li>
<li><a class="reference internal" href="index.html#torch.autograd.Function.backward" title="torch.autograd.Function.backward"><code class="xref py py-meth docutils literal"><span class="pre">backward()</span></code></a> - gradient formula. It will be given
as many arguments as there were outputs, with each of them representing
gradient w.r.t. that output. It should return as many <code class="xref py py-class docutils literal"><span class="pre">Tensor</span></code> s as
there were inputs, with each of them containing the gradient w.r.t.
corresponding input. If you inputs didn&#8217;t require gradient (see
<code class="xref py py-attr docutils literal"><span class="pre">needs_input_grad</span></code>), or it was non-differentiable, you
can return <code class="xref py py-class docutils literal"><span class="pre">None</span></code>. Also, if you have optional arguments to
<code class="xref py py-meth docutils literal"><span class="pre">forward()</span></code> you can return more gradients than there were
inputs, as long as they&#8217;re all <a class="reference external" href="https://docs.python.org/2/library/constants.html#None" title="(in Python v2.7)"><code class="docutils literal"><span class="pre">None</span></code></a>.</li>
</ul>
<p>Below you can find code for a <code class="docutils literal"><span class="pre">Linear</span></code> function from <a class="reference internal" href="index.html#module-torch.nn" title="torch.nn"><code class="xref py py-mod docutils literal"><span class="pre">torch.nn</span></code></a>, with
additional comments:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># Inherit from Function</span>
<span class="k">class</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>

    <span class="c1"># bias is an optional argument</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">+=</span> <span class="n">bias</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="c1"># This function has only a single output, so it gets only one gradient</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="c1"># This is a pattern that is very convenient - at the top of backward</span>
        <span class="c1"># unpack saved_tensors and initialize all gradients w.r.t. inputs to</span>
        <span class="c1"># None. Thanks to the fact that additional trailing Nones are</span>
        <span class="c1"># ignored, the return statement is simple even when the function has</span>
        <span class="c1"># optional inputs.</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">grad_weight</span> <span class="o">=</span> <span class="n">grad_bias</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># These needs_input_grad checks are optional and there only to</span>
        <span class="c1"># improve efficiency. If you want to make your code simpler, you can</span>
        <span class="c1"># skip them. Returning gradients for inputs that don&#39;t require it is</span>
        <span class="c1"># not an error.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">grad_input</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">grad_weight</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
            <span class="n">grad_bias</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_weight</span><span class="p">,</span> <span class="n">grad_bias</span>
</pre></div>
</div>
<p>Now, to make it easier to use these custom ops, we recommend wrapping them in
small helper functions:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># First braces create a Function object. Any arguments given here</span>
    <span class="c1"># will be passed to __init__. Second braces will invoke the __call__</span>
    <span class="c1"># operator, that will then use forward() to compute the result and</span>
    <span class="c1"># return it.</span>
    <span class="k">return</span> <span class="n">Linear</span><span class="p">()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
<p>You probably want to check if the backward method you implemented actually
computes the derivatives of your function. It is possible by comparing with
numerical approximations using small finite differences:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="k">import</span> <span class="n">gradcheck</span>

<span class="c1"># gradchek takes a tuple of tensor as input, check if your gradient</span>
<span class="c1"># evaluated with these tensors are close enough to numerical</span>
<span class="c1"># approximations and returns True if they all verify this condition.</span>
<span class="nb">input</span> <span class="o">=</span> <span class="p">(</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),)</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">gradcheck</span><span class="p">(</span><span class="n">Linear</span><span class="p">(),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="extending-torch-nn">
<h3>Extending <a class="reference internal" href="index.html#module-torch.nn" title="torch.nn"><code class="xref py py-mod docutils literal"><span class="pre">torch.nn</span></code></a><a class="headerlink" href="#extending-torch-nn" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="index.html#module-torch.nn" title="torch.nn"><code class="xref py py-mod docutils literal"><span class="pre">nn</span></code></a> exports two kinds of interfaces - modules and their functional
versions. You can extend it in both ways, but we recommend using modules for
all kinds of layers, that hold any parameters or buffers, and recommend using
a functional form parameter-less operations like activation functions, pooling,
etc.</p>
<p>Adding a functional version of an operation is already fully covered in the
section above.</p>
<div class="section" id="adding-a-module">
<h4>Adding a <a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal"><span class="pre">Module</span></code></a><a class="headerlink" href="#adding-a-module" title="Permalink to this headline">¶</a></h4>
<p>Since <a class="reference internal" href="index.html#module-torch.nn" title="torch.nn"><code class="xref py py-mod docutils literal"><span class="pre">nn</span></code></a> heavily utilizes <a class="reference internal" href="index.html#module-torch.autograd" title="torch.autograd"><code class="xref py py-mod docutils literal"><span class="pre">autograd</span></code></a>, adding a new
<a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal"><span class="pre">Module</span></code></a> requires implementing a <a class="reference internal" href="index.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal"><span class="pre">Function</span></code></a>
that performs the operation and can compute the gradient. From now on let&#8217;s
assume that we want to implement a <code class="docutils literal"><span class="pre">Linear</span></code> module and we have the function
implementated as in the listing above. There&#8217;s very little code required to
add this. Now, there are two functions that need to be implemented:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">__init__</span></code> (<em>optional</em>) - takes in arguments such as kernel sizes, numbers
of features, etc. and initializes parameters and buffers.</li>
<li><a class="reference internal" href="index.html#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-meth docutils literal"><span class="pre">forward()</span></code></a> - instantiates a <a class="reference internal" href="index.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal"><span class="pre">Function</span></code></a> and
uses it to perform the operation. It&#8217;s very similar to a functional wrapper
shown above.</li>
</ul>
<p>This is how a <code class="docutils literal"><span class="pre">Linear</span></code> module can be implemented:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">,</span> <span class="n">output_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_features</span> <span class="o">=</span> <span class="n">input_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_features</span> <span class="o">=</span> <span class="n">output_features</span>

        <span class="c1"># nn.Parameter is a special kind of Variable, that will get</span>
        <span class="c1"># automatically registered as Module&#39;s parameter once it&#39;s assigned</span>
        <span class="c1"># as an attribute. Parameters and buffers need to be registered, or</span>
        <span class="c1"># they won&#39;t appear in .parameters() (doesn&#39;t apply to buffers), and</span>
        <span class="c1"># won&#39;t be converted when e.g. .cuda() is called. You can use</span>
        <span class="c1"># .register_buffer() to register buffers.</span>
        <span class="c1"># nn.Parameters can never be volatile and, different than Variables,</span>
        <span class="c1"># they require gradients by default.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span> <span class="n">output_features</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">output_features</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># You should always register all possible parameters, but the</span>
            <span class="c1"># optional ones can be None if you want.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Not a very smart way to initialize weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="c1"># See the autograd section for explanation of what happens here.</span>
        <span class="k">return</span> <span class="n">Linear</span><span class="p">()(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="writing-custom-c-extensions">
<h3>Writing custom C extensions<a class="headerlink" href="#writing-custom-c-extensions" title="Permalink to this headline">¶</a></h3>
<p>Coming soon. For now you can find an example at
<a class="reference external" href="https://github.com/pytorch/extension-ffi">GitHub</a>.</p>
</div>
</div>
<span id="document-notes/multiprocessing"></span><div class="section" id="multiprocessing-best-practices">
<h2>Multiprocessing best practices<a class="headerlink" href="#multiprocessing-best-practices" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="index.html#module-torch.multiprocessing" title="torch.multiprocessing"><code class="xref py py-mod docutils literal"><span class="pre">torch.multiprocessing</span></code></a> is a drop in replacement for Python&#8217;s
<a class="reference external" href="https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing" title="(in Python v2.7)"><code class="docutils literal"><span class="pre">multiprocessing</span></code></a> module. It supports the exact same operations,
but extends it, so that all tensors sent through a
<a class="reference external" href="https://docs.python.org/2/library/multiprocessing.html#multiprocessing.Queue" title="(in Python v2.7)"><code class="docutils literal"><span class="pre">multiprocessing.Queue</span></code></a>, will have their data moved into shared
memory and will only send a handle to another process.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When a <a class="reference internal" href="index.html#torch.autograd.Variable" title="torch.autograd.Variable"><code class="xref py py-class docutils literal"><span class="pre">Variable</span></code></a> is sent to another process, both
the <code class="xref py py-attr docutils literal"><span class="pre">Variable.data</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">Variable.grad.data</span></code> are going to be
shared.</p>
</div>
<p>This allows to implement various training methods, like Hogwild, A3C, or any
others that require asynchronous operation.</p>
<div class="section" id="sharing-cuda-tensors">
<h3>Sharing CUDA tensors<a class="headerlink" href="#sharing-cuda-tensors" title="Permalink to this headline">¶</a></h3>
<p>Sharing CUDA tensors between processes is supported only in Python 3, using
a <code class="docutils literal"><span class="pre">spawn</span></code> or <code class="docutils literal"><span class="pre">forkserver</span></code> start methods. <a class="reference external" href="https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing" title="(in Python v2.7)"><code class="docutils literal"><span class="pre">multiprocessing</span></code></a> in
Python 2 can only create subprocesses using <code class="docutils literal"><span class="pre">fork</span></code>, and it&#8217;s not supported
by the CUDA runtime.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">CUDA API requires that the allocation exported to other processes remains
valid as long as it&#8217;s used by them. You should be careful and ensure that
CUDA tensors you shared don&#8217;t go out of scope as long as it&#8217;s necessary.
This shouldn&#8217;t be a problem for sharing model parameters, but passing other
kinds of data should be done with care. Note that this restriction doesn&#8217;t
apply to shared CPU memory.</p>
</div>
<p>See also: <a class="reference internal" href="index.html#cuda-nn-dataparallel-instead"><span class="std std-ref">Use nn.DataParallel instead of multiprocessing</span></a></p>
</div>
<div class="section" id="best-practices-and-tips">
<h3>Best practices and tips<a class="headerlink" href="#best-practices-and-tips" title="Permalink to this headline">¶</a></h3>
<div class="section" id="avoiding-and-fighting-deadlocks">
<h4>Avoiding and fighting deadlocks<a class="headerlink" href="#avoiding-and-fighting-deadlocks" title="Permalink to this headline">¶</a></h4>
<p>There are a lot of things that can go wrong when a new process is spawned, with
the most common cause of deadlocks being background threads. If there&#8217;s any
thread that holds a lock or imports a module, and <code class="docutils literal"><span class="pre">fork</span></code> is called, it&#8217;s very
likely that the subprocess will be in a corrupted state and will deadlock or
fail in a different way. Note that even if you don&#8217;t, Python built in
libraries do - no need to look further than <a class="reference external" href="https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing" title="(in Python v2.7)"><code class="docutils literal"><span class="pre">multiprocessing</span></code></a>.
<a class="reference external" href="https://docs.python.org/2/library/multiprocessing.html#multiprocessing.Queue" title="(in Python v2.7)"><code class="docutils literal"><span class="pre">multiprocessing.Queue</span></code></a> is actually a very complex class, that
spawns multiple threads used to serialize, send and receive objects, and they
can cause aforementioned problems too. If you find yourself in such situation
try using a <code class="xref py py-class docutils literal"><span class="pre">multiprocessing.queues.SimpleQueue</span></code>, that doesn&#8217;t
use any additional threads.</p>
<p>We&#8217;re trying our best to make it easy for you and ensure these deadlocks don&#8217;t
happen but some things are out of our control. If you have any issues you can&#8217;t
cope with for a while, try reaching out on forums, and we&#8217;ll see if it&#8217;s an
issue we can fix.</p>
</div>
<div class="section" id="reuse-buffers-passed-through-a-queue">
<h4>Reuse buffers passed through a Queue<a class="headerlink" href="#reuse-buffers-passed-through-a-queue" title="Permalink to this headline">¶</a></h4>
<p>Remember that each time you put a <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal"><span class="pre">Tensor</span></code></a> into a
<a class="reference external" href="https://docs.python.org/2/library/multiprocessing.html#multiprocessing.Queue" title="(in Python v2.7)"><code class="docutils literal"><span class="pre">multiprocessing.Queue</span></code></a>, it has to be moved into shared memory.
If it&#8217;s already shared, it is a no-op, otherwise it will incur an additional
memory copy that can slow down the whole process. Even if you have a pool of
processes sending data to a single one, make it send the buffers back - this
is nearly free and will let you avoid a copy when sending next batch.</p>
</div>
<div class="section" id="asynchronous-multiprocess-training-e-g-hogwild">
<h4>Asynchronous multiprocess training (e.g. Hogwild)<a class="headerlink" href="#asynchronous-multiprocess-training-e-g-hogwild" title="Permalink to this headline">¶</a></h4>
<p>Using <a class="reference internal" href="index.html#module-torch.multiprocessing" title="torch.multiprocessing"><code class="xref py py-mod docutils literal"><span class="pre">torch.multiprocessing</span></code></a>, it is possible to train a model
asynchronously, with parameters either shared all the time, or being
periodically synchronized. In the first case, we recommend sending over the whole
model object, while in the latter, we advise to only send the
<a class="reference internal" href="index.html#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-meth docutils literal"><span class="pre">state_dict()</span></code></a>.</p>
<p>We recommend using <a class="reference external" href="https://docs.python.org/2/library/multiprocessing.html#multiprocessing.Queue" title="(in Python v2.7)"><code class="docutils literal"><span class="pre">multiprocessing.Queue</span></code></a> for passing all kinds
of PyTorch objects between processes. It is possible to e.g. inherit the tensors
and storages already in shared memory, when using the <code class="docutils literal"><span class="pre">fork</span></code> start method,
however it is very bug prone and should be used with care, and only by advanced
users. Queues, even though they&#8217;re sometimes a less elegant solution, will work
properly in all cases.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">You should be careful about having global statements, that are not guarded
with an <code class="docutils literal"><span class="pre">if</span> <span class="pre">__name__</span> <span class="pre">==</span> <span class="pre">'__main__'</span></code>. If a different start method than
<code class="docutils literal"><span class="pre">fork</span></code> is used, they will be executed in all subprocesses.</p>
</div>
<div class="section" id="hogwild">
<h5>Hogwild<a class="headerlink" href="#hogwild" title="Permalink to this headline">¶</a></h5>
<p>A concrete Hogwild implementation can be found in the <a class="reference external" href="https://github.com/pytorch/examples/tree/master/mnist_hogwild">examples repository</a>,
but to showcase the overall structure of the code, there&#8217;s also a minimal
example below as well:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>
<span class="kn">from</span> <span class="nn">model</span> <span class="k">import</span> <span class="n">MyModel</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="c1"># Construct data_loader, optimizer, etc.</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># This will update the shared parameters</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">num_processes</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
    <span class="c1"># NOTE: this is required for the ``fork`` method to work</span>
    <span class="n">model</span><span class="o">.</span><span class="n">share_memory</span><span class="p">()</span>
    <span class="n">processes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_processes</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">train</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">model</span><span class="p">,))</span>
        <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
        <span class="n">processes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">processes</span><span class="p">:</span>
      <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<span id="document-notes/serialization"></span><div class="section" id="serialization-semantics">
<h2>Serialization semantics<a class="headerlink" href="#serialization-semantics" title="Permalink to this headline">¶</a></h2>
<div class="section" id="best-practices">
<h3>Best practices<a class="headerlink" href="#best-practices" title="Permalink to this headline">¶</a></h3>
<div class="section" id="recommended-approach-for-saving-a-model">
<span id="recommend-saving-models"></span><h4>Recommended approach for saving a model<a class="headerlink" href="#recommended-approach-for-saving-a-model" title="Permalink to this headline">¶</a></h4>
<p>There are two main approaches for serializing and restoring a model.</p>
<p>The first (recommended) saves and loads only the model parameters:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">the_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
</pre></div>
</div>
<p>Then later:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">the_model</span> <span class="o">=</span> <span class="n">TheModelClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">the_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">))</span>
</pre></div>
</div>
<p>The second saves and loads the entire model:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">the_model</span><span class="p">,</span> <span class="n">PATH</span><span class="p">)</span>
</pre></div>
</div>
<p>Then later:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">the_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span>
</pre></div>
</div>
<p>However in this case, the serialized data is bound to the specific classes
and the exact directory structure used, so it can break in various ways when
used in other projects, or after some serious refactors.</p>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-torch"></span><div class="section" id="module-torch">
<span id="torch"></span><h2>torch<a class="headerlink" href="#module-torch" title="Permalink to this headline">¶</a></h2>
<p>The torch package contains data structures for multi-dimensional
tensors and mathematical operations over these are defined.
Additionally, it provides many utilities for efficient serializing of
Tensors and arbitrary types, and other useful utilities.</p>
<p>It has a CUDA counterpart, that enables you to run your tensor computations
on an NVIDIA GPU with compute capability &gt;= 2.0.</p>
<div class="section" id="tensors">
<h3>Tensors<a class="headerlink" href="#tensors" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.is_tensor">
<code class="descclassname">torch.</code><code class="descname">is_tensor</code><span class="sig-paren">(</span><em>obj</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.is_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if <cite>obj</cite> is a pytorch tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>obj</strong> (<em>Object</em>) &#8211; Object to test</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.is_storage">
<code class="descclassname">torch.</code><code class="descname">is_storage</code><span class="sig-paren">(</span><em>obj</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.is_storage" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if <cite>obj</cite> is a pytorch storage object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>obj</strong> (<em>Object</em>) &#8211; Object to test</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.set_default_tensor_type">
<code class="descclassname">torch.</code><code class="descname">set_default_tensor_type</code><span class="sig-paren">(</span><em>t</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.set_default_tensor_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torch.numel">
<code class="descclassname">torch.</code><code class="descname">numel</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &rarr; int<a class="headerlink" href="#torch.numel" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the total number of elements in the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">numel</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">120</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">numel</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">16</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.set_printoptions">
<code class="descclassname">torch.</code><code class="descname">set_printoptions</code><span class="sig-paren">(</span><em>precision=None</em>, <em>threshold=None</em>, <em>edgeitems=None</em>, <em>linewidth=None</em>, <em>profile=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.set_printoptions" title="Permalink to this definition">¶</a></dt>
<dd><p>Set options for printing. Items shamelessly taken from Numpy</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>precision</strong> &#8211; Number of digits of precision for floating point output
(default 8).</li>
<li><strong>threshold</strong> &#8211; Total number of array elements which trigger summarization
rather than full repr (default 1000).</li>
<li><strong>edgeitems</strong> &#8211; Number of array items in summary at beginning and end of
each dimension (default 3).</li>
<li><strong>linewidth</strong> &#8211; The number of characters per line for the purpose of
inserting line breaks (default 80). Thresholded matricies will
ignore this parameter.</li>
<li><strong>profile</strong> &#8211; Sane defaults for pretty printing. Can override with any of
the above options. (default, short, full)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<div class="section" id="creation-ops">
<h4>Creation Ops<a class="headerlink" href="#creation-ops" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.eye">
<code class="descclassname">torch.</code><code class="descname">eye</code><span class="sig-paren">(</span><em>n</em>, <em>m=None</em>, <em>out=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.eye" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>n</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of rows</li>
<li><strong>m</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Number of columns. If None, defaults to <cite>n</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">a 2-D tensor with ones on the diagonal and zeros elsewhere</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="go"> 1  0  0</span>
<span class="go"> 0  1  0</span>
<span class="go"> 0  0  1</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.from_numpy">
<code class="descclassname">torch.</code><code class="descname">from_numpy</code><span class="sig-paren">(</span><em>ndarray</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.from_numpy" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal"><span class="pre">Tensor</span></code></a> from a <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.12)"><code class="xref py py-class docutils literal"><span class="pre">numpy.ndarray</span></code></a>.</p>
<p>The returned tensor and <cite>ndarray</cite> share the same memory. Modifications to the
tensor will be reflected in the <cite>ndarray</cite> and vice versa. The returned tensor
is not resizable.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span>
<span class="go">torch.LongTensor([1, 2, 3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">array([-1,  2,  3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.linspace">
<code class="descclassname">torch.</code><code class="descname">linspace</code><span class="sig-paren">(</span><em>start</em>, <em>end</em>, <em>steps=100</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.linspace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a one-dimensional Tensor of <code class="xref py py-attr docutils literal"><span class="pre">steps</span></code>
equally spaced points between <code class="xref py py-attr docutils literal"><span class="pre">start</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">end</span></code></p>
<p>The output tensor is 1D of size <code class="xref py py-attr docutils literal"><span class="pre">steps</span></code></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>start</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The starting value for the set of points</li>
<li><strong>end</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The ending value for the set of points</li>
<li><strong>steps</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of points to sample between <code class="xref py py-attr docutils literal"><span class="pre">start</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">end</span></code></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="go">  3.0000</span>
<span class="go">  4.7500</span>
<span class="go">  6.5000</span>
<span class="go">  8.2500</span>
<span class="go"> 10.0000</span>
<span class="go">[torch.FloatTensor of size 5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="go">-10</span>
<span class="go"> -5</span>
<span class="go">  0</span>
<span class="go">  5</span>
<span class="go"> 10</span>
<span class="go">[torch.FloatTensor of size 5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mi">10</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="go">-10</span>
<span class="go"> -5</span>
<span class="go">  0</span>
<span class="go">  5</span>
<span class="go"> 10</span>
<span class="go">[torch.FloatTensor of size 5]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.logspace">
<code class="descclassname">torch.</code><code class="descname">logspace</code><span class="sig-paren">(</span><em>start</em>, <em>end</em>, <em>steps=100</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.logspace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a one-dimensional Tensor of <code class="xref py py-attr docutils literal"><span class="pre">steps</span></code> points
logarithmically spaced between <span class="math">\(10^{start}\)</span> and <span class="math">\(10^{end}\)</span></p>
<p>The output is a 1D tensor of size <code class="xref py py-attr docutils literal"><span class="pre">steps</span></code></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>start</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The starting value for the set of points</li>
<li><strong>end</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The ending value for the set of points</li>
<li><strong>steps</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of points to sample between <code class="xref py py-attr docutils literal"><span class="pre">start</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">end</span></code></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mi">10</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="go"> 1.0000e-10</span>
<span class="go"> 1.0000e-05</span>
<span class="go"> 1.0000e+00</span>
<span class="go"> 1.0000e+05</span>
<span class="go"> 1.0000e+10</span>
<span class="go">[torch.FloatTensor of size 5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="go">  1.2589</span>
<span class="go">  2.1135</span>
<span class="go">  3.5481</span>
<span class="go">  5.9566</span>
<span class="go"> 10.0000</span>
<span class="go">[torch.FloatTensor of size 5]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.ones">
<code class="descclassname">torch.</code><code class="descname">ones</code><span class="sig-paren">(</span><em>*sizes</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.ones" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor filled with the scalar value <cite>1</cite>, with the shape defined
by the varargs <code class="xref py py-attr docutils literal"><span class="pre">sizes</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>sizes</strong> (<em>int...</em>) &#8211; a set of ints defining the shape of the output Tensor.</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the result Tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="go"> 1  1  1</span>
<span class="go"> 1  1  1</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="go"> 1</span>
<span class="go"> 1</span>
<span class="go"> 1</span>
<span class="go"> 1</span>
<span class="go"> 1</span>
<span class="go">[torch.FloatTensor of size 5]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.rand">
<code class="descclassname">torch.</code><code class="descname">rand</code><span class="sig-paren">(</span><em>*sizes</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.rand" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor filled with random numbers from a uniform distribution
on the interval <span class="math">\([0, 1)\)</span></p>
<p>The shape of the Tensor is defined by the varargs <code class="xref py py-attr docutils literal"><span class="pre">sizes</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>sizes</strong> (<em>int...</em>) &#8211; a set of ints defining the shape of the output Tensor.</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the result Tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="go"> 0.9193</span>
<span class="go"> 0.3347</span>
<span class="go"> 0.3232</span>
<span class="go"> 0.7715</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="go"> 0.5010  0.5140  0.0719</span>
<span class="go"> 0.1435  0.5636  0.0538</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.randn">
<code class="descclassname">torch.</code><code class="descname">randn</code><span class="sig-paren">(</span><em>*sizes</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.randn" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor filled with random numbers from a normal distribution
with zero mean and variance of one.</p>
<p>The shape of the Tensor is defined by the varargs <code class="xref py py-attr docutils literal"><span class="pre">sizes</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>sizes</strong> (<em>int...</em>) &#8211; a set of ints defining the shape of the output Tensor.</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the result Tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="go">-0.1145</span>
<span class="go"> 0.0094</span>
<span class="go">-1.1717</span>
<span class="go"> 0.9846</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="go"> 1.4339  0.3351 -1.0999</span>
<span class="go"> 1.5458 -0.9643 -0.3558</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.randperm">
<code class="descclassname">torch.</code><code class="descname">randperm</code><span class="sig-paren">(</span><em>n</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; LongTensor<a class="headerlink" href="#torch.randperm" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a random permutation of integers from <code class="docutils literal"><span class="pre">0</span></code> to <code class="docutils literal"><span class="pre">n</span> <span class="pre">-</span> <span class="pre">1</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>n</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the upper bound (exclusive)</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="go"> 2</span>
<span class="go"> 1</span>
<span class="go"> 3</span>
<span class="go"> 0</span>
<span class="go">[torch.LongTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.arange">
<code class="descclassname">torch.</code><code class="descname">arange</code><span class="sig-paren">(</span><em>start</em>, <em>end</em>, <em>step=1</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.arange" title="Permalink to this definition">¶</a></dt>
<dd><p>Teturns a 1D Tensor of size <span class="math">\(floor((end - start) / step)\)</span> with values
from the interval <code class="docutils literal"><span class="pre">[start,</span> <span class="pre">end)</span></code> taken with step <code class="xref py py-attr docutils literal"><span class="pre">step</span></code> starting from <cite>start</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>start</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The starting value for the set of points</li>
<li><strong>end</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The ending value for the set of points</li>
<li><strong>step</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The gap between each pair of adjacent points</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="go"> 1</span>
<span class="go"> 2</span>
<span class="go"> 3</span>
<span class="go">[torch.FloatTensor of size 3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="go"> 1.0000</span>
<span class="go"> 1.5000</span>
<span class="go"> 2.0000</span>
<span class="go">[torch.FloatTensor of size 3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.range">
<code class="descclassname">torch.</code><code class="descname">range</code><span class="sig-paren">(</span><em>start</em>, <em>end</em>, <em>step=1</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.range" title="Permalink to this definition">¶</a></dt>
<dd><p>returns a 1D Tensor of size <span class="math">\(floor((end - start) / step) + 1\)</span> with values
from <code class="xref py py-attr docutils literal"><span class="pre">start</span></code> to <code class="xref py py-attr docutils literal"><span class="pre">end</span></code> with step <code class="xref py py-attr docutils literal"><span class="pre">step</span></code>. Step is the gap between two values in the tensor.
<span class="math">\(x_{i+1} = x_i + step\)</span></p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This function is deprecated in favor of <a class="reference internal" href="#torch.arange" title="torch.arange"><code class="xref py py-func docutils literal"><span class="pre">torch.arange()</span></code></a>.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>start</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The starting value for the set of points</li>
<li><strong>end</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The ending value for the set of points</li>
<li><strong>step</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The gap between each pair of adjacent points</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="go"> 1</span>
<span class="go"> 2</span>
<span class="go"> 3</span>
<span class="go"> 4</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="go"> 1.0000</span>
<span class="go"> 1.5000</span>
<span class="go"> 2.0000</span>
<span class="go"> 2.5000</span>
<span class="go"> 3.0000</span>
<span class="go"> 3.5000</span>
<span class="go"> 4.0000</span>
<span class="go">[torch.FloatTensor of size 7]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.zeros">
<code class="descclassname">torch.</code><code class="descname">zeros</code><span class="sig-paren">(</span><em>*sizes</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.zeros" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor filled with the scalar value <cite>0</cite>, with the shape defined
by the varargs <code class="xref py py-attr docutils literal"><span class="pre">sizes</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>sizes</strong> (<em>int...</em>) &#8211; a set of ints defining the shape of the output Tensor.</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the result Tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="go"> 0  0  0</span>
<span class="go"> 0  0  0</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="go"> 0</span>
<span class="go"> 0</span>
<span class="go"> 0</span>
<span class="go"> 0</span>
<span class="go"> 0</span>
<span class="go">[torch.FloatTensor of size 5]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="indexing-slicing-joining-mutating-ops">
<h4>Indexing, Slicing, Joining, Mutating Ops<a class="headerlink" href="#indexing-slicing-joining-mutating-ops" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.cat">
<code class="descclassname">torch.</code><code class="descname">cat</code><span class="sig-paren">(</span><em>seq</em>, <em>dim=0</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.cat" title="Permalink to this definition">¶</a></dt>
<dd><p>Concatenates the given sequence of <code class="xref py py-attr docutils literal"><span class="pre">seq</span></code> Tensors in the given dimension.</p>
<p><a class="reference internal" href="#torch.cat" title="torch.cat"><code class="xref py py-func docutils literal"><span class="pre">torch.cat()</span></code></a> can be seen as an inverse operation for <a class="reference internal" href="#torch.split" title="torch.split"><code class="xref py py-func docutils literal"><span class="pre">torch.split()</span></code></a> and <a class="reference internal" href="#torch.chunk" title="torch.chunk"><code class="xref py py-func docutils literal"><span class="pre">torch.chunk()</span></code></a></p>
<p><a class="reference internal" href="#torch.cat" title="torch.cat"><code class="xref py py-func docutils literal"><span class="pre">cat()</span></code></a> can be best understood via examples.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>seq</strong> (<em>sequence of Tensors</em>) &#8211; Can be any python sequence of <cite>Tensor</cite> of the same type.</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; The dimension over which the tensors are concatenated</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>

<span class="go"> 0.5983 -0.0341  2.4918</span>
<span class="go"> 1.5981 -0.5265 -0.8735</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>

<span class="go"> 0.5983 -0.0341  2.4918</span>
<span class="go"> 1.5981 -0.5265 -0.8735</span>
<span class="go"> 0.5983 -0.0341  2.4918</span>
<span class="go"> 1.5981 -0.5265 -0.8735</span>
<span class="go"> 0.5983 -0.0341  2.4918</span>
<span class="go"> 1.5981 -0.5265 -0.8735</span>
<span class="go">[torch.FloatTensor of size 6x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

<span class="go"> 0.5983 -0.0341  2.4918  0.5983 -0.0341  2.4918  0.5983 -0.0341  2.4918</span>
<span class="go"> 1.5981 -0.5265 -0.8735  1.5981 -0.5265 -0.8735  1.5981 -0.5265 -0.8735</span>
<span class="go">[torch.FloatTensor of size 2x9]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.chunk">
<code class="descclassname">torch.</code><code class="descname">chunk</code><span class="sig-paren">(</span><em>tensor</em>, <em>chunks</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.chunk" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits a tensor into a number of chunks along a given dimension.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; tensor to split.</li>
<li><strong>chunks</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; number of chunks to return.</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; dimension along which to split the tensor.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.gather">
<code class="descclassname">torch.</code><code class="descname">gather</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>index</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Gathers values along an axis specified by <cite>dim</cite>.</p>
<p>For a 3-D tensor the output is specified by:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># dim=0</span>
<span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># dim=1</span>
<span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span>  <span class="c1"># dim=3</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; The source tensor</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The axis along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) &#8211; The indices of elements to gather</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Destination tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]]))</span>
<span class="go"> 1  1</span>
<span class="go"> 4  3</span>
<span class="go">[torch.FloatTensor of size 2x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.index_select">
<code class="descclassname">torch.</code><code class="descname">index_select</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>index</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.index_select" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> which indexes the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> <cite>Tensor</cite> along dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code>
using the entries in <code class="xref py py-attr docutils literal"><span class="pre">index</span></code> which is a <cite>LongTensor</cite>.</p>
<p>The returned <cite>Tensor</cite> has the same number of dimensions as the original <cite>Tensor</cite>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The returned <cite>Tensor</cite> does <strong>not</strong> use the same storage as the original <cite>Tensor</cite></p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Input data</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the dimension in which we index</li>
<li><strong>index</strong> (<em>LongTensor</em>) &#8211; the 1D tensor containing the indices to index</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output argument</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>

<span class="go"> 1.2045  2.4084  0.4001  1.1372</span>
<span class="go"> 0.5596  1.5677  0.6219 -0.7954</span>
<span class="go"> 1.3635 -1.2313 -0.5414 -1.8478</span>
<span class="go">[torch.FloatTensor of size 3x4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>

<span class="go"> 1.2045  2.4084  0.4001  1.1372</span>
<span class="go"> 1.3635 -1.2313 -0.5414 -1.8478</span>
<span class="go">[torch.FloatTensor of size 2x4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>

<span class="go"> 1.2045  0.4001</span>
<span class="go"> 0.5596  0.6219</span>
<span class="go"> 1.3635 -0.5414</span>
<span class="go">[torch.FloatTensor of size 3x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.masked_select">
<code class="descclassname">torch.</code><code class="descname">masked_select</code><span class="sig-paren">(</span><em>input</em>, <em>mask</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.masked_select" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new 1D <cite>Tensor</cite> which indexes the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> <cite>Tensor</cite> according to
the binary mask <code class="xref py py-attr docutils literal"><span class="pre">mask</span></code> which is a <cite>ByteTensor</cite>.</p>
<p>The <code class="xref py py-attr docutils literal"><span class="pre">mask</span></code> tensor needs to have the same number of elements as
<code class="xref py py-attr docutils literal"><span class="pre">input</span></code>, but it&#8217;s shape or dimensionality are irrelevant.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The returned <cite>Tensor</cite> does <strong>not</strong> use the same storage as the original <cite>Tensor</cite></p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Input data</li>
<li><strong>mask</strong> (<em>ByteTensor</em>) &#8211; the tensor containing the binary mask to index with</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output argument</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>

<span class="go"> 1.2045  2.4084  0.4001  1.1372</span>
<span class="go"> 0.5596  1.5677  0.6219 -0.7954</span>
<span class="go"> 1.3635 -1.2313 -0.5414 -1.8478</span>
<span class="go">[torch.FloatTensor of size 3x4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span>

<span class="go"> 1  1  0  1</span>
<span class="go"> 1  1  1  0</span>
<span class="go"> 1  0  0  0</span>
<span class="go">[torch.ByteTensor of size 3x4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

<span class="go"> 1.2045</span>
<span class="go"> 2.4084</span>
<span class="go"> 1.1372</span>
<span class="go"> 0.5596</span>
<span class="go"> 1.5677</span>
<span class="go"> 0.6219</span>
<span class="go"> 1.3635</span>
<span class="go">[torch.FloatTensor of size 7]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nonzero">
<code class="descclassname">torch.</code><code class="descname">nonzero</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; LongTensor<a class="headerlink" href="#torch.nonzero" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor containing the indices of all non-zero elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.
Each row in the result contains the indices of a non-zero element in <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> has <cite>n</cite> dimensions, then the resulting indices Tensor
<code class="xref py py-attr docutils literal"><span class="pre">out</span></code> is of size <cite>z x n</cite>, where <cite>z</cite> is the total number of non-zero
elements in the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<em>LongTensor</em><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite> containing indices</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>

<span class="go"> 0</span>
<span class="go"> 1</span>
<span class="go"> 2</span>
<span class="go"> 4</span>
<span class="go">[torch.LongTensor of size 4x1]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
<span class="gp">... </span>                            <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
<span class="gp">... </span>                            <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
<span class="gp">... </span>                            <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span><span class="o">-</span><span class="mf">0.4</span><span class="p">]]))</span>

<span class="go"> 0  0</span>
<span class="go"> 1  1</span>
<span class="go"> 2  2</span>
<span class="go"> 3  3</span>
<span class="go">[torch.LongTensor of size 4x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.split">
<code class="descclassname">torch.</code><code class="descname">split</code><span class="sig-paren">(</span><em>tensor</em>, <em>split_size</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.split" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits the tensor into equally sized chunks (if possible).</p>
<p>Last chunk will be smaller if the tensor size along a given dimension
is not divisible by <code class="docutils literal"><span class="pre">split_size</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; tensor to split.</li>
<li><strong>split_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; size of a single chunk.</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; dimension along which to split the tensor.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.squeeze">
<code class="descclassname">torch.</code><code class="descname">squeeze</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>out=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.squeeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <cite>Tensor</cite> with all the dimensions of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> of size <cite>1</cite> removed.</p>
<p>If <cite>input</cite> is of shape: <span class="math">\((A x 1 x B x C x 1 x D)\)</span> then the <cite>out</cite> Tensor
will be of shape: <span class="math">\((A x B x C x D)\)</span></p>
<p>When <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> is given, a squeeze operation is done only in the given dimension.
If <cite>input</cite> is of shape: <span class="math">\((A x 1 x B)\)</span>, <cite>squeeze(input, 0)</cite> leaves the Tensor unchanged,
but <cite>squeeze(input, 1)</cite> will squeeze the tensor to the shape <span class="math">\((A x B)\)</span>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The returned Tensor shares the storage with the input Tensor,
so changing the contents of one will change the contents of the other.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; if given, the input will be squeezed only in this dimension</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">(2L, 1L, 2L, 1L, 2L)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">(2L, 2L, 2L)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">(2L, 1L, 2L, 1L, 2L)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">(2L, 2L, 1L, 2L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.stack">
<code class="descclassname">torch.</code><code class="descname">stack</code><span class="sig-paren">(</span><em>sequence</em>, <em>dim=0</em>, <em>out=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.stack" title="Permalink to this definition">¶</a></dt>
<dd><p>Concatenates sequence of tensors along a new dimension.</p>
<p>All tensors need to be of the same size.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>sequence</strong> (<em>Sequence</em>) &#8211; sequence of tensors to concatenate.</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; dimension to insert. Has to be between 0 and the number
of dimensions of concatenated tensors (inclusive).</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.t">
<code class="descclassname">torch.</code><code class="descname">t</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.t" title="Permalink to this definition">¶</a></dt>
<dd><p>Expects <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> to be a matrix (2D Tensor) and transposes dimensions 0 and 1.</p>
<p>Can be seen as a short-hand function for <cite>transpose(input, 0, 1)</cite></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>

<span class="go"> 0.4834  0.6907  1.3417</span>
<span class="go">-0.1300  0.5295  0.2321</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="go"> 0.4834 -0.1300</span>
<span class="go"> 0.6907  0.5295</span>
<span class="go"> 1.3417  0.2321</span>
<span class="go">[torch.FloatTensor of size 3x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.transpose">
<code class="descclassname">torch.</code><code class="descname">transpose</code><span class="sig-paren">(</span><em>input</em>, <em>dim0</em>, <em>dim1</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.transpose" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <cite>Tensor</cite> that is a transposed version of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.
The given dimensions <code class="xref py py-attr docutils literal"><span class="pre">dim0</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">dim1</span></code> are swapped.</p>
<p>The resulting <code class="xref py py-attr docutils literal"><span class="pre">out</span></code> Tensor shares it&#8217;s underlying storage with the
<code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor, so changing the content of one would change the content
of the other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>dim0</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The first dimension to be transposed</li>
<li><strong>dim1</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The second dimension to be transposed</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>

<span class="go"> 0.5983 -0.0341  2.4918</span>
<span class="go"> 1.5981 -0.5265 -0.8735</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="go"> 0.5983  1.5981</span>
<span class="go">-0.0341 -0.5265</span>
<span class="go"> 2.4918 -0.8735</span>
<span class="go">[torch.FloatTensor of size 3x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.unbind">
<code class="descclassname">torch.</code><code class="descname">unbind</code><span class="sig-paren">(</span><em>tensor</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.unbind" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes a tensor dimension.</p>
<p>Returns a tuple of all slices along a given dimension, already without it.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; tensor to unbind.</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; dimension to remove.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.unsqueeze">
<code class="descclassname">torch.</code><code class="descname">unsqueeze</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>out=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.unsqueeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with a dimension of size one inserted at the
specified position.</p>
<p>The returned tensor shares the same underlying data with this tensor.</p>
<p>A negative dim value can be used and will correspond to <span class="math">\(dim + input.dim() + 1\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The index at which to insert the singleton dimension</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go"> 1  2  3  4</span>
<span class="go">[torch.FloatTensor of size 1x4]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go"> 1</span>
<span class="go"> 2</span>
<span class="go"> 3</span>
<span class="go"> 4</span>
<span class="go">[torch.FloatTensor of size 4x1]</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="random-sampling">
<h3>Random sampling<a class="headerlink" href="#random-sampling" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.manual_seed">
<code class="descclassname">torch.</code><code class="descname">manual_seed</code><span class="sig-paren">(</span><em>seed</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.manual_seed" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the seed for generating random numbers. And returns a
<cite>torch._C.Generator</cite> object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#long" title="(in Python v2.7)"><em>long</em></a>) &#8211; The desired seed.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.initial_seed">
<code class="descclassname">torch.</code><code class="descname">initial_seed</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.initial_seed" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the initial seed for generating random numbers as a
python <cite>long</cite>.</p>
</dd></dl>

<dl class="function">
<dt id="torch.get_rng_state">
<code class="descclassname">torch.</code><code class="descname">get_rng_state</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.get_rng_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the random number generator state as a ByteTensor.</p>
</dd></dl>

<dl class="function">
<dt id="torch.set_rng_state">
<code class="descclassname">torch.</code><code class="descname">set_rng_state</code><span class="sig-paren">(</span><em>new_state</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.set_rng_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the random number generator state.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>new_state</strong> (<em>torch.ByteTensor</em>) &#8211; The desired state</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="data">
<dt id="torch.default_generator">
<code class="descclassname">torch.</code><code class="descname">default_generator</code><em class="property"> = &lt;torch._C.Generator object&gt;</em><a class="headerlink" href="#torch.default_generator" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torch.bernoulli">
<code class="descclassname">torch.</code><code class="descname">bernoulli</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.bernoulli" title="Permalink to this definition">¶</a></dt>
<dd><p>Draws binary random numbers (0 or 1) from a bernoulli distribution.</p>
<p>The <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor should be a tensor containing probabilities
to be used for drawing the binary random number.
Hence, all values in <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> have to be in the range:
<span class="math">\(0 &lt;= input_i &lt;= 1\)</span></p>
<p>The <cite>i-th</cite> element of the output tensor will draw a value <cite>1</cite> according
to the <cite>i-th</cite> probability value given in <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<p>The returned <code class="xref py py-attr docutils literal"><span class="pre">out</span></code> Tensor only has values 0 or 1 and is of the same
shape as <code class="xref py py-attr docutils literal"><span class="pre">input</span></code></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Probability values for the bernoulli distribution</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># generate a uniform random matrix with range [0, 1]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 0.7544  0.8140  0.9842</span>
<span class="go"> 0.5282  0.0595  0.6445</span>
<span class="go"> 0.1925  0.9553  0.9732</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go"> 1  1  1</span>
<span class="go"> 0  0  1</span>
<span class="go"> 0  1  1</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># probability of drawing &quot;1&quot; is 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go"> 1  1  1</span>
<span class="go"> 1  1  1</span>
<span class="go"> 1  1  1</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># probability of drawing &quot;1&quot; is 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go"> 0  0  0</span>
<span class="go"> 0  0  0</span>
<span class="go"> 0  0  0</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.multinomial">
<code class="descclassname">torch.</code><code class="descname">multinomial</code><span class="sig-paren">(</span><em>input</em>, <em>num_samples</em>, <em>replacement=False</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; LongTensor<a class="headerlink" href="#torch.multinomial" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor where each row
contains <code class="xref py py-attr docutils literal"><span class="pre">num_samples</span></code> indices sampled from the multinomial probability distribution
located in the corresponding row of Tensor <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The rows of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> do not need to sum to one (in which case we use the values
as weights), but must be non-negative and have a non-zero sum.</p>
</div>
<p>Indices are ordered from left to right according to when each was sampled
(first samples are placed in first column).</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is a vector, <code class="xref py py-attr docutils literal"><span class="pre">out</span></code> is a vector of size <cite>num_samples</cite>.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is a matrix with <cite>m</cite> rows, <code class="xref py py-attr docutils literal"><span class="pre">out</span></code> is an matrix of shape <cite>m × n</cite>.</p>
<p>If replacement is <cite>True</cite>, samples are drawn with replacement.</p>
<p>If not, they are drawn without replacement, which means that when a
sample index is drawn for a row, it cannot be drawn again for that row.</p>
<p>This implies the constraint that <code class="xref py py-attr docutils literal"><span class="pre">num_samples</span></code> must be lower than <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> length
(or number of columns of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> if it is a matrix).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Tensor containing probabilities</li>
<li><strong>num_samples</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; number of samples to draw</li>
<li><strong>replacement</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; Whether to draw with replacement or not</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="c1"># create a Tensor of weights</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="go"> 1</span>
<span class="go"> 2</span>
<span class="go"> 0</span>
<span class="go"> 0</span>
<span class="go">[torch.LongTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="go"> 1</span>
<span class="go"> 2</span>
<span class="go"> 1</span>
<span class="go"> 2</span>
<span class="go">[torch.LongTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.normal">
<code class="descclassname">torch.</code><code class="descname">normal</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.normal" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">normal</code><span class="sig-paren">(</span><em>means</em>, <em>std</em>, <em>out=None</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Returns a Tensor of random numbers drawn from separate normal distributions
who&#8217;s mean and standard deviation are given.</p>
<p>The <code class="xref py py-attr docutils literal"><span class="pre">means</span></code> is a Tensor with the mean of
each output element&#8217;s normal distribution</p>
<p>The <a class="reference internal" href="#torch.std" title="torch.std"><code class="xref py py-attr docutils literal"><span class="pre">std</span></code></a> is a Tensor with the standard deviation of
each output element&#8217;s normal distribution</p>
<p>The shapes of <code class="xref py py-attr docutils literal"><span class="pre">means</span></code> and <a class="reference internal" href="#torch.std" title="torch.std"><code class="xref py py-attr docutils literal"><span class="pre">std</span></code></a> don&#8217;t need to match.
The total number of elements in each Tensor need to be the same.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When the shapes do not match, the shape of <code class="xref py py-attr docutils literal"><span class="pre">means</span></code>
is used as the shape for the returned output Tensor</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>means</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the Tensor of per-element means</li>
<li><strong>std</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the Tensor of per-element standard deviations</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the optional result Tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">means</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="n">std</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">))</span>

 <span class="mf">1.5104</span>
 <span class="mf">1.6955</span>
 <span class="mf">2.4895</span>
 <span class="mf">4.9185</span>
 <span class="mf">4.9895</span>
 <span class="mf">6.9155</span>
 <span class="mf">7.3683</span>
 <span class="mf">8.1836</span>
 <span class="mf">8.7164</span>
 <span class="mf">9.8916</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">normal</code><span class="sig-paren">(</span><em>mean=0.0</em>, <em>std</em>, <em>out=None</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Similar to the function above, but the means are shared among all drawn elements.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>means</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; the mean for all distributions</li>
<li><strong>std</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the Tensor of per-element standard deviations</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the optional result Tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="go">  0.5723</span>
<span class="go">  0.0871</span>
<span class="go"> -0.3783</span>
<span class="go"> -2.5689</span>
<span class="go"> 10.7893</span>
<span class="go">[torch.FloatTensor of size 5]</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">normal</code><span class="sig-paren">(</span><em>means</em>, <em>std=1.0</em>, <em>out=None</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Similar to the function above, but the standard-deviations are shared among all drawn elements.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>means</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the Tensor of per-element means</li>
<li><strong>std</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; the standard deviation for all distributions</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the optional result Tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">means</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="go"> 1.1681</span>
<span class="go"> 2.8884</span>
<span class="go"> 3.7718</span>
<span class="go"> 2.5616</span>
<span class="go"> 4.2500</span>
<span class="go">[torch.FloatTensor of size 5]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="serialization">
<h3>Serialization<a class="headerlink" href="#serialization" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.save">
<code class="descclassname">torch.</code><code class="descname">save</code><span class="sig-paren">(</span><em>obj</em>, <em>f</em>, <em>pickle_module=&lt;module 'pickle' from '/data/users/gchanan/anaconda3/lib/python3.6/pickle.py'&gt;</em>, <em>pickle_protocol=2</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves an object to a disk file.</p>
<p>See also: <a class="reference internal" href="index.html#recommend-saving-models"><span class="std std-ref">Recommended approach for saving a model</span></a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>obj</strong> &#8211; saved object</li>
<li><strong>f</strong> &#8211; a file-like object (has to implement fileno that returns a file descriptor)
or a string containing a file name</li>
<li><strong>pickle_module</strong> &#8211; module used for pickling metadata and objects</li>
<li><strong>pickle_protocol</strong> &#8211; can be specified to override the default protocol</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.load">
<code class="descclassname">torch.</code><code class="descname">load</code><span class="sig-paren">(</span><em>f</em>, <em>map_location=None</em>, <em>pickle_module=&lt;module 'pickle' from '/data/users/gchanan/anaconda3/lib/python3.6/pickle.py'&gt;</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads an object saved with <a class="reference internal" href="#torch.save" title="torch.save"><code class="xref py py-func docutils literal"><span class="pre">torch.save()</span></code></a> from a file.</p>
<p>torch.load can dynamically remap storages to be loaded on a different device
using the map_location argument. If it&#8217;s a callable, it will be called with
two arguments: storage and location tag. It&#8217;s expected to either return a
storage that&#8217;s been moved to a different location, or None (and the location
will be resolved using the default method). If this argument is a dict it&#8217;s
expected to be a mapping from location tags used in a file, to location
tags of the current system.</p>
<p>By default the location tags are &#8216;cpu&#8217; for host tensors and &#8216;cuda:device_id&#8217;
(e.g. &#8216;cuda:2&#8217;) for cuda tensors. User extensions can register their own
tagging and deserialization methods using register_package.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>f</strong> &#8211; a file-like object (has to implement fileno that returns a file descriptor,
and must implement seek), or a string containing a file name</li>
<li><strong>map_location</strong> &#8211; a function or a dict specifying how to remap storage locations</li>
<li><strong>pickle_module</strong> &#8211; module used for unpickling metadata and objects (has to match
the pickle_module used to serialize file)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensors.pt&#39;</span><span class="p">)</span>
<span class="go"># Load all tensors onto the CPU</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensors.pt&#39;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="k">lambda</span> <span class="n">storage</span><span class="p">,</span> <span class="n">loc</span><span class="p">:</span> <span class="n">storage</span><span class="p">)</span>
<span class="go"># Map tensors from GPU 1 to GPU 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensors.pt&#39;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">:</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">})</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="parallelism">
<h3>Parallelism<a class="headerlink" href="#parallelism" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.get_num_threads">
<code class="descclassname">torch.</code><code class="descname">get_num_threads</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; int<a class="headerlink" href="#torch.get_num_threads" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the number of OpenMP threads used for parallelizing CPU operations</p>
</dd></dl>

<dl class="function">
<dt id="torch.set_num_threads">
<code class="descclassname">torch.</code><code class="descname">set_num_threads</code><span class="sig-paren">(</span><em>int</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.set_num_threads" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the number of OpenMP threads used for parallelizing CPU operations</p>
</dd></dl>

</div>
<div class="section" id="math-operations">
<h3>Math operations<a class="headerlink" href="#math-operations" title="Permalink to this headline">¶</a></h3>
<div class="section" id="pointwise-ops">
<h4>Pointwise Ops<a class="headerlink" href="#pointwise-ops" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.abs">
<code class="descclassname">torch.</code><code class="descname">abs</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.abs" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the element-wise absolute value of the given <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> a tensor.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
<span class="go">FloatTensor([1, 2, 3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.acos">
<code class="descclassname">torch.</code><code class="descname">acos</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.acos" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the arccosine  of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.6366</span>
<span class="go"> 0.2718</span>
<span class="go"> 0.4469</span>
<span class="go"> 1.3122</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">acos</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go"> 2.2608</span>
<span class="go"> 1.2956</span>
<span class="go"> 1.1075</span>
<span class="go">    nan</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.add">
<code class="descclassname">torch.</code><code class="descname">add</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.add" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">add</code><span class="sig-paren">(</span><em>input</em>, <em>value</em>, <em>out=None</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Adds the scalar <code class="xref py py-attr docutils literal"><span class="pre">value</span></code> to each element of the input <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>
and returns a new resulting tensor.</p>
<p><span class="math">\(out = tensor + value\)</span></p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is of type FloatTensor or DoubleTensor, <code class="xref py py-attr docutils literal"><span class="pre">value</span></code> must be a real number, otherwise it should be an
integer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>value</strong> (<em>Number</em>) &#8211; the number to be added to each element of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 0.4050</span>
<span class="go">-1.2227</span>
<span class="go"> 1.8688</span>
<span class="go">-0.4185</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

<span class="go"> 20.4050</span>
<span class="go"> 18.7773</span>
<span class="go"> 21.8688</span>
<span class="go"> 19.5815</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">add</code><span class="sig-paren">(</span><em>input</em>, <em>value=1</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Each element of the Tensor <code class="xref py py-attr docutils literal"><span class="pre">other</span></code> is multiplied by the scalar
<code class="xref py py-attr docutils literal"><span class="pre">value</span></code> and added to each element of the Tensor <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.
The resulting Tensor is returned.</p>
<p>The shapes of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">other</span></code> must be broadcastable.</p>
<p><span class="math">\(out = input + (other * value)\)</span></p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">other</span></code> is of type FloatTensor or DoubleTensor, <code class="xref py py-attr docutils literal"><span class="pre">value</span></code> must be a real number, otherwise it should be an
integer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the first input <cite>Tensor</cite></li>
<li><strong>value</strong> (<em>Number</em>) &#8211; the scalar multiplier for <code class="xref py py-attr docutils literal"><span class="pre">other</span></code></li>
<li><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the second input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.9310</span>
<span class="go"> 2.0330</span>
<span class="go"> 0.0852</span>
<span class="go">-0.2941</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>

<span class="go"> 1.0663  0.2544</span>
<span class="go">-0.1513  0.0749</span>
<span class="go">[torch.FloatTensor of size 2x2]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="go"> 9.7322</span>
<span class="go"> 4.5770</span>
<span class="go">-1.4279</span>
<span class="go"> 0.4552</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.addcdiv">
<code class="descclassname">torch.</code><code class="descname">addcdiv</code><span class="sig-paren">(</span><em>tensor</em>, <em>value=1</em>, <em>tensor1</em>, <em>tensor2</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.addcdiv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the element-wise division of <code class="xref py py-attr docutils literal"><span class="pre">tensor1</span></code> by <code class="xref py py-attr docutils literal"><span class="pre">tensor2</span></code>,
multiply the result by the scalar <code class="xref py py-attr docutils literal"><span class="pre">value</span></code> and add it to <code class="xref py py-attr docutils literal"><span class="pre">tensor</span></code>.</p>
<p>The number of elements must match, but sizes do not matter.</p>
<p>For inputs of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, <code class="xref py py-attr docutils literal"><span class="pre">value</span></code> must be a real number, otherwise an integer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the tensor to be added</li>
<li><strong>value</strong> (<em>Number</em><em>, </em><em>optional</em>) &#8211; multiplier for <cite>tensor1 ./ tensor2</cite></li>
<li><strong>tensor1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Numerator tensor</li>
<li><strong>tensor2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Denominator tensor</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">addcdiv</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">)</span>

<span class="go"> 0.0122 -0.0188 -0.2354</span>
<span class="go"> 0.7396 -1.5721  1.2878</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.addcmul">
<code class="descclassname">torch.</code><code class="descname">addcmul</code><span class="sig-paren">(</span><em>tensor</em>, <em>value=1</em>, <em>tensor1</em>, <em>tensor2</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.addcmul" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the element-wise multiplication of <code class="xref py py-attr docutils literal"><span class="pre">tensor1</span></code>
by <code class="xref py py-attr docutils literal"><span class="pre">tensor2</span></code>, multiply the result by the scalar <code class="xref py py-attr docutils literal"><span class="pre">value</span></code>
and add it to <code class="xref py py-attr docutils literal"><span class="pre">tensor</span></code>.</p>
<p>The number of elements must match, but sizes do not matter.</p>
<p>For inputs of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, <code class="xref py py-attr docutils literal"><span class="pre">value</span></code> must be a real number, otherwise an integer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the tensor to be added</li>
<li><strong>value</strong> (<em>Number</em><em>, </em><em>optional</em>) &#8211; multiplier for <cite>tensor1 .* tensor2</cite></li>
<li><strong>tensor1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; tensor to be multiplied</li>
<li><strong>tensor2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; tensor to be multiplied</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">addcmul</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">)</span>

<span class="go"> 0.0122 -0.0188 -0.2354</span>
<span class="go"> 0.7396 -1.5721  1.2878</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.asin">
<code class="descclassname">torch.</code><code class="descname">asin</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.asin" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the arcsine  of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">-0.6366</span>
<span class="go"> 0.2718</span>
<span class="go"> 0.4469</span>
<span class="go"> 1.3122</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">asin</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">-0.6900</span>
<span class="go"> 0.2752</span>
<span class="go"> 0.4633</span>
<span class="go">    nan</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.atan">
<code class="descclassname">torch.</code><code class="descname">atan</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.atan" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the arctangent  of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">-0.6366</span>
<span class="go"> 0.2718</span>
<span class="go"> 0.4469</span>
<span class="go"> 1.3122</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">atan</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">-0.5669</span>
<span class="go"> 0.2653</span>
<span class="go"> 0.4203</span>
<span class="go"> 0.9196</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.atan2">
<code class="descclassname">torch.</code><code class="descname">atan2</code><span class="sig-paren">(</span><em>input1</em>, <em>input2</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.atan2" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the arctangent of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input1</span></code>
and <code class="xref py py-attr docutils literal"><span class="pre">input2</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the first input <cite>Tensor</cite></li>
<li><strong>input2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the second input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">-0.6366</span>
<span class="go"> 0.2718</span>
<span class="go"> 0.4469</span>
<span class="go"> 1.3122</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">atan2</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<span class="go">-2.4167</span>
<span class="go"> 2.9755</span>
<span class="go"> 0.9363</span>
<span class="go"> 1.6613</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
<p>The shapes of <code class="xref py py-attr docutils literal"><span class="pre">input1</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">input2</span></code> must be broadcastable.</p>
</dd></dl>

<dl class="function">
<dt id="torch.ceil">
<code class="descclassname">torch.</code><code class="descname">ceil</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.ceil" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the ceil of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>,
the smallest integer greater than or equal to each element.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1.3869</span>
<span class="go"> 0.3912</span>
<span class="go">-0.8634</span>
<span class="go">-0.5468</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go"> 2</span>
<span class="go"> 1</span>
<span class="go">-0</span>
<span class="go">-0</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.clamp">
<code class="descclassname">torch.</code><code class="descname">clamp</code><span class="sig-paren">(</span><em>input</em>, <em>min</em>, <em>max</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.clamp" title="Permalink to this definition">¶</a></dt>
<dd><p>Clamp all elements in <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> into the range <cite>[min, max]</cite> and return a resulting Tensor.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>      <span class="o">|</span> <span class="nb">min</span><span class="p">,</span> <span class="k">if</span> <span class="n">x_i</span> <span class="o">&lt;</span> <span class="nb">min</span>
<span class="n">y_i</span> <span class="o">=</span> <span class="o">|</span> <span class="n">x_i</span><span class="p">,</span> <span class="k">if</span> <span class="nb">min</span> <span class="o">&lt;=</span> <span class="n">x_i</span> <span class="o">&lt;=</span> <span class="nb">max</span>
      <span class="o">|</span> <span class="nb">max</span><span class="p">,</span> <span class="k">if</span> <span class="n">x_i</span> <span class="o">&gt;</span> <span class="nb">max</span>
</pre></div>
</div>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, args <a class="reference internal" href="#torch.min" title="torch.min"><code class="xref py py-attr docutils literal"><span class="pre">min</span></code></a> and <a class="reference internal" href="#torch.max" title="torch.max"><code class="xref py py-attr docutils literal"><span class="pre">max</span></code></a> must be real numbers,
otherwise they should be integers</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>min</strong> (<em>Number</em>) &#8211; lower-bound of the range to be clamped to</li>
<li><strong>max</strong> (<em>Number</em>) &#8211; upper-bound of the range to be clamped to</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1.3869</span>
<span class="go"> 0.3912</span>
<span class="go">-0.8634</span>
<span class="go">-0.5468</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="go"> 0.5000</span>
<span class="go"> 0.3912</span>
<span class="go">-0.5000</span>
<span class="go">-0.5000</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">clamp</code><span class="sig-paren">(</span><em>input</em>, <em>*</em>, <em>min</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor</dt>
<dd></dd></dl>

<p>Clamps all elements in <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> to be larger or equal <a class="reference internal" href="#torch.min" title="torch.min"><code class="xref py py-attr docutils literal"><span class="pre">min</span></code></a>.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, <code class="xref py py-attr docutils literal"><span class="pre">value</span></code> should be a real number, otherwise it should
be an integer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>value</strong> (<em>Number</em>) &#8211; minimal value of each element in the output</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1.3869</span>
<span class="go"> 0.3912</span>
<span class="go">-0.8634</span>
<span class="go">-0.5468</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="go"> 1.3869</span>
<span class="go"> 0.5000</span>
<span class="go"> 0.5000</span>
<span class="go"> 0.5000</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">clamp</code><span class="sig-paren">(</span><em>input</em>, <em>*</em>, <em>max</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor</dt>
<dd></dd></dl>

<p>Clamps all elements in <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> to be smaller or equal <a class="reference internal" href="#torch.max" title="torch.max"><code class="xref py py-attr docutils literal"><span class="pre">max</span></code></a>.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, <code class="xref py py-attr docutils literal"><span class="pre">value</span></code> should be a real number, otherwise it should
be an integer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>value</strong> (<em>Number</em>) &#8211; maximal value of each element in the output</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1.3869</span>
<span class="go"> 0.3912</span>
<span class="go">-0.8634</span>
<span class="go">-0.5468</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="go"> 0.5000</span>
<span class="go"> 0.3912</span>
<span class="go">-0.8634</span>
<span class="go">-0.5468</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cos">
<code class="descclassname">torch.</code><code class="descname">cos</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.cos" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the cosine  of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">-0.6366</span>
<span class="go"> 0.2718</span>
<span class="go"> 0.4469</span>
<span class="go"> 1.3122</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go"> 0.8041</span>
<span class="go"> 0.9633</span>
<span class="go"> 0.9018</span>
<span class="go"> 0.2557</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cosh">
<code class="descclassname">torch.</code><code class="descname">cosh</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.cosh" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the hyperbolic cosine  of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">-0.6366</span>
<span class="go"> 0.2718</span>
<span class="go"> 0.4469</span>
<span class="go"> 1.3122</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go"> 1.2095</span>
<span class="go"> 1.0372</span>
<span class="go"> 1.1015</span>
<span class="go"> 1.9917</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.div">
<code class="descclassname">torch.</code><code class="descname">div</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.div" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">div</code><span class="sig-paren">(</span><em>input</em>, <em>value</em>, <em>out=None</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Divides each element of the input <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> with the scalar <code class="xref py py-attr docutils literal"><span class="pre">value</span></code> and returns a new resulting tensor.</p>
<p><span class="math">\(out = tensor / value\)</span></p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, <code class="xref py py-attr docutils literal"><span class="pre">value</span></code> should be a real number, otherwise it should
be an integer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>value</strong> (<em>Number</em>) &#8211; the number to be divided to each element of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.6147</span>
<span class="go">-1.1237</span>
<span class="go">-0.1604</span>
<span class="go">-0.6853</span>
<span class="go"> 0.1063</span>
<span class="go">[torch.FloatTensor of size 5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="go">-1.2294</span>
<span class="go">-2.2474</span>
<span class="go">-0.3208</span>
<span class="go">-1.3706</span>
<span class="go"> 0.2126</span>
<span class="go">[torch.FloatTensor of size 5]</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">div</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Each element of the Tensor <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is divided by each element of the Tensor <code class="xref py py-attr docutils literal"><span class="pre">other</span></code>.
The resulting Tensor is returned. The shapes of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">other</span></code> must be broadcastable.</p>
<p><span class="math">\(out_i = input_i / other_i\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the numerator <cite>Tensor</cite></li>
<li><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the denominator <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.1810  0.4017  0.2863 -0.1013</span>
<span class="go"> 0.6183  2.0696  0.9012 -1.5933</span>
<span class="go"> 0.5679  0.4743 -0.0117 -0.1266</span>
<span class="go">-0.1213  0.9629  0.2682  1.5968</span>
<span class="go">[torch.FloatTensor of size 4x4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>

<span class="go"> 0.8774  0.7650</span>
<span class="go"> 0.8866  1.4805</span>
<span class="go">-0.6490  1.1172</span>
<span class="go"> 1.4259 -0.8146</span>
<span class="go"> 1.4633 -0.1228</span>
<span class="go"> 0.4643 -0.6029</span>
<span class="go"> 0.3492  1.5270</span>
<span class="go"> 1.6103 -0.6291</span>
<span class="go">[torch.FloatTensor of size 8x2]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="go">-0.2062  0.5251  0.3229 -0.0684</span>
<span class="go">-0.9528  1.8525  0.6320  1.9559</span>
<span class="go"> 0.3881 -3.8625 -0.0253  0.2099</span>
<span class="go">-0.3473  0.6306  0.1666 -2.5381</span>
<span class="go">[torch.FloatTensor of size 4x4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.exp">
<code class="descclassname">torch.</code><code class="descname">exp</code><span class="sig-paren">(</span><em>tensor</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.exp" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the exponential of each element.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)]))</span>
<span class="go">torch.FloatTensor([1, 2])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.floor">
<code class="descclassname">torch.</code><code class="descname">floor</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.floor" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the floor of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>,
the largest integer less than or equal to each element.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1.3869</span>
<span class="go"> 0.3912</span>
<span class="go">-0.8634</span>
<span class="go">-0.5468</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go"> 1</span>
<span class="go"> 0</span>
<span class="go">-1</span>
<span class="go">-1</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.fmod">
<code class="descclassname">torch.</code><code class="descname">fmod</code><span class="sig-paren">(</span><em>input</em>, <em>divisor</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.fmod" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the element-wise remainder of division.</p>
<p>The dividend and divisor may contain both for integer and floating point
numbers. The remainder has the same sign as the dividend <cite>tensor</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; The dividend</li>
<li><strong>divisor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The divisor. This may be either a number or a
tensor of the same shape as the dividend.</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">torch.FloatTensor([-1, -0, -1, 1, 0, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="go">torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5])</span>
</pre></div>
</div>
<p>The shapes of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">divisor</span></code> must be broadcastable.</p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#torch.remainder" title="torch.remainder"><code class="xref py py-func docutils literal"><span class="pre">torch.remainder()</span></code></a>, which computes the element-wise remainder of
division equivalently to Python&#8217;s <cite>%</cite> operator</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.frac">
<code class="descclassname">torch.</code><code class="descname">frac</code><span class="sig-paren">(</span><em>tensor</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.frac" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the fractional portion of each element in <cite>tensor</cite>.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">frac</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2</span><span class="p">])</span>
<span class="go">torch.FloatTensor([0, 0.5, -0.2])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.lerp">
<code class="descclassname">torch.</code><code class="descname">lerp</code><span class="sig-paren">(</span><em>start</em>, <em>end</em>, <em>weight</em>, <em>out=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.lerp" title="Permalink to this definition">¶</a></dt>
<dd><p>Does a linear interpolation of two tensors <code class="xref py py-attr docutils literal"><span class="pre">start</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">end</span></code> based
on a scalar <code class="xref py py-attr docutils literal"><span class="pre">weight</span></code>: and returns the resulting <code class="xref py py-attr docutils literal"><span class="pre">out</span></code> Tensor.</p>
<p><span class="math">\(out_i = start_i + weight * (end_i - start_i)\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>start</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the <cite>Tensor</cite> with the starting points</li>
<li><strong>end</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the <cite>Tensor</cite> with the ending points</li>
<li><strong>weight</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; the weight for the interpolation formula</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">start</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">start</span>

<span class="go"> 1</span>
<span class="go"> 2</span>
<span class="go"> 3</span>
<span class="go"> 4</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">end</span>

<span class="go"> 10</span>
<span class="go"> 10</span>
<span class="go"> 10</span>
<span class="go"> 10</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">lerp</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="go"> 5.5000</span>
<span class="go"> 6.0000</span>
<span class="go"> 6.5000</span>
<span class="go"> 7.0000</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
<p>The shapes of <code class="xref py py-attr docutils literal"><span class="pre">start</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">end</span></code> must be broadcastable.</p>
</dd></dl>

<dl class="function">
<dt id="torch.log">
<code class="descclassname">torch.</code><code class="descname">log</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the natural logarithm of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.4183</span>
<span class="go"> 0.3722</span>
<span class="go">-0.3091</span>
<span class="go"> 0.4149</span>
<span class="go"> 0.5857</span>
<span class="go">[torch.FloatTensor of size 5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go">    nan</span>
<span class="go">-0.9883</span>
<span class="go">    nan</span>
<span class="go">-0.8797</span>
<span class="go">-0.5349</span>
<span class="go">[torch.FloatTensor of size 5]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.log1p">
<code class="descclassname">torch.</code><code class="descname">log1p</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.log1p" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the natural logarithm of (1 + <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>).</p>
<p><span class="math">\(y_i = log(x_i + 1)\)</span></p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function is more accurate than <a class="reference internal" href="#torch.log" title="torch.log"><code class="xref py py-func docutils literal"><span class="pre">torch.log()</span></code></a> for small values of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code></p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.4183</span>
<span class="go"> 0.3722</span>
<span class="go">-0.3091</span>
<span class="go"> 0.4149</span>
<span class="go"> 0.5857</span>
<span class="go">[torch.FloatTensor of size 5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go">-0.5418</span>
<span class="go"> 0.3164</span>
<span class="go">-0.3697</span>
<span class="go"> 0.3471</span>
<span class="go"> 0.4611</span>
<span class="go">[torch.FloatTensor of size 5]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.mul">
<code class="descclassname">torch.</code><code class="descname">mul</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.mul" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">mul</code><span class="sig-paren">(</span><em>input</em>, <em>value</em>, <em>out=None</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Multiplies each element of the input <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> with the scalar <code class="xref py py-attr docutils literal"><span class="pre">value</span></code> and returns a new resulting tensor.</p>
<p><span class="math">\(out = tensor * value\)</span></p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, <code class="xref py py-attr docutils literal"><span class="pre">value</span></code> should be a real number, otherwise it should
be an integer</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>value</strong> (<em>Number</em>) &#8211; the number to be multiplied to each element of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.9374</span>
<span class="go">-0.5254</span>
<span class="go">-0.6069</span>
<span class="go">[torch.FloatTensor of size 3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="go">-93.7411</span>
<span class="go">-52.5374</span>
<span class="go">-60.6908</span>
<span class="go">[torch.FloatTensor of size 3]</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">mul</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Each element of the Tensor <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is multiplied by each element of the Tensor <code class="xref py py-attr docutils literal"><span class="pre">other</span></code>.
The resulting Tensor is returned. The shapes of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">other</span></code> don&#8217;t need to match.
The total number of elements in each Tensor need to be the same.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When the shapes do not match, the shape of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is used as the shape for the returned output Tensor</p>
</div>
<p><span class="math">\(out_i = input_i * other_i\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the first multiplicand <cite>Tensor</cite></li>
<li><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the second multiplicand <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.7280  0.0598 -1.4327 -0.5825</span>
<span class="go">-0.1427 -0.0690  0.0821 -0.3270</span>
<span class="go">-0.9241  0.5110  0.4070 -1.1188</span>
<span class="go">-0.8308  0.7426 -0.6240 -1.1582</span>
<span class="go">[torch.FloatTensor of size 4x4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>

<span class="go"> 0.0430 -1.0775  0.6015  1.1647 -0.6549  0.0308 -0.1670  1.0742</span>
<span class="go">-1.2593  0.0292 -0.0849  0.4530  1.2404 -0.4659 -0.1840  0.5974</span>
<span class="go">[torch.FloatTensor of size 2x8]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="go">-0.0313 -0.0645 -0.8618 -0.6784</span>
<span class="go"> 0.0934 -0.0021 -0.0137 -0.3513</span>
<span class="go"> 1.1638  0.0149 -0.0346 -0.5068</span>
<span class="go">-1.0304 -0.3460  0.1148 -0.6919</span>
<span class="go">[torch.FloatTensor of size 4x4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.neg">
<code class="descclassname">torch.</code><code class="descname">neg</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.neg" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the negative of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<p><span class="math">\(out = -1 * input\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.4430</span>
<span class="go"> 1.1690</span>
<span class="go">-0.8836</span>
<span class="go">-0.4565</span>
<span class="go"> 0.2968</span>
<span class="go">[torch.FloatTensor of size 5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">neg</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go"> 0.4430</span>
<span class="go">-1.1690</span>
<span class="go"> 0.8836</span>
<span class="go"> 0.4565</span>
<span class="go">-0.2968</span>
<span class="go">[torch.FloatTensor of size 5]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.pow">
<code class="descclassname">torch.</code><code class="descname">pow</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.pow" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">pow</code><span class="sig-paren">(</span><em>input</em>, <em>exponent</em>, <em>out=None</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Takes the power of each element in <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> with <code class="xref py py-attr docutils literal"><span class="pre">exponent</span></code> and returns a Tensor with the result.</p>
<p><code class="xref py py-attr docutils literal"><span class="pre">exponent</span></code> can be either a single <code class="docutils literal"><span class="pre">float</span></code> number or a <code class="docutils literal"><span class="pre">Tensor</span></code>
with the same number of elements as <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<p>When <code class="xref py py-attr docutils literal"><span class="pre">exponent</span></code> is a scalar value, the operation applied is:</p>
<p><span class="math">\(out_i = x_i ^ {exponent}\)</span></p>
<p>When <code class="xref py py-attr docutils literal"><span class="pre">exponent</span></code> is a Tensor, the operation applied is:</p>
<p><span class="math">\(out_i = x_i ^ {exponent_i}\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>exponent</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the exponent value</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.5274</span>
<span class="go">-0.8232</span>
<span class="go">-2.1128</span>
<span class="go"> 1.7558</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="go"> 0.2781</span>
<span class="go"> 0.6776</span>
<span class="go"> 4.4640</span>
<span class="go"> 3.0829</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">exp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1</span>
<span class="go"> 2</span>
<span class="go"> 3</span>
<span class="go"> 4</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">exp</span>

<span class="go"> 1</span>
<span class="go"> 2</span>
<span class="go"> 3</span>
<span class="go"> 4</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">exp</span><span class="p">)</span>

<span class="go">   1</span>
<span class="go">   4</span>
<span class="go">  27</span>
<span class="go"> 256</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">pow</code><span class="sig-paren">(</span><em>base</em>, <em>input</em>, <em>out=None</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p><code class="xref py py-attr docutils literal"><span class="pre">base</span></code> is a scalar <code class="docutils literal"><span class="pre">float</span></code> value, and <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is a Tensor.
The returned Tensor <code class="xref py py-attr docutils literal"><span class="pre">out</span></code> is of the same shape as <code class="xref py py-attr docutils literal"><span class="pre">input</span></code></p>
<p>The operation applied is:</p>
<p><span class="math">\(out_i = base ^ {input_i}\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>base</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; the scalar base value for the power operation</li>
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the exponent <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">exp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">base</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">base</span><span class="p">,</span> <span class="n">exp</span><span class="p">)</span>

<span class="go">  2</span>
<span class="go">  4</span>
<span class="go">  8</span>
<span class="go"> 16</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.reciprocal">
<code class="descclassname">torch.</code><code class="descname">reciprocal</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.reciprocal" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the reciprocal of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>, i.e. <span class="math">\(1.0 / x\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1.3869</span>
<span class="go"> 0.3912</span>
<span class="go">-0.8634</span>
<span class="go">-0.5468</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go"> 0.7210</span>
<span class="go"> 2.5565</span>
<span class="go">-1.1583</span>
<span class="go">-1.8289</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.remainder">
<code class="descclassname">torch.</code><code class="descname">remainder</code><span class="sig-paren">(</span><em>input</em>, <em>divisor</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.remainder" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the element-wise remainder of division.</p>
<p>The divisor and dividend may contain both for integer and floating point
numbers. The remainder has the same sign as the divisor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; The dividend</li>
<li><strong>divisor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The divisor. This may be either a number or a
tensor of the same shape as the dividend.</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">torch.FloatTensor([1, 0, 1, 1, 0, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="go">torch.FloatTensor([1.0, 0.5, 0.0, 1.0, 0.5])</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#torch.fmod" title="torch.fmod"><code class="xref py py-func docutils literal"><span class="pre">torch.fmod()</span></code></a>, which computes the element-wise remainder of
division equivalently to the C library function <code class="docutils literal"><span class="pre">fmod()</span></code></p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.round">
<code class="descclassname">torch.</code><code class="descname">round</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.round" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with each of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> rounded to the closest integer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1.2290</span>
<span class="go"> 1.3409</span>
<span class="go">-0.5662</span>
<span class="go">-0.0899</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go"> 1</span>
<span class="go"> 1</span>
<span class="go">-1</span>
<span class="go">-0</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.rsqrt">
<code class="descclassname">torch.</code><code class="descname">rsqrt</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.rsqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the reciprocal of the square-root of each of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1.2290</span>
<span class="go"> 1.3409</span>
<span class="go">-0.5662</span>
<span class="go">-0.0899</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go"> 0.9020</span>
<span class="go"> 0.8636</span>
<span class="go">    nan</span>
<span class="go">    nan</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.sigmoid">
<code class="descclassname">torch.</code><code class="descname">sigmoid</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the sigmoid of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.4972</span>
<span class="go"> 1.3512</span>
<span class="go"> 0.1056</span>
<span class="go">-0.2650</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go"> 0.3782</span>
<span class="go"> 0.7943</span>
<span class="go"> 0.5264</span>
<span class="go"> 0.4341</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.sign">
<code class="descclassname">torch.</code><code class="descname">sign</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.sign" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the sign of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">-0.6366</span>
<span class="go"> 0.2718</span>
<span class="go"> 0.4469</span>
<span class="go"> 1.3122</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go">-1</span>
<span class="go"> 1</span>
<span class="go"> 1</span>
<span class="go"> 1</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.sin">
<code class="descclassname">torch.</code><code class="descname">sin</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.sin" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the sine of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">-0.6366</span>
<span class="go"> 0.2718</span>
<span class="go"> 0.4469</span>
<span class="go"> 1.3122</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">-0.5944</span>
<span class="go"> 0.2684</span>
<span class="go"> 0.4322</span>
<span class="go"> 0.9667</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.sinh">
<code class="descclassname">torch.</code><code class="descname">sinh</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.sinh" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the hyperbolic sine of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">-0.6366</span>
<span class="go"> 0.2718</span>
<span class="go"> 0.4469</span>
<span class="go"> 1.3122</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sinh</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">-0.6804</span>
<span class="go"> 0.2751</span>
<span class="go"> 0.4619</span>
<span class="go"> 1.7225</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.sqrt">
<code class="descclassname">torch.</code><code class="descname">sqrt</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.sqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the square-root of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1.2290</span>
<span class="go"> 1.3409</span>
<span class="go">-0.5662</span>
<span class="go">-0.0899</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go"> 1.1086</span>
<span class="go"> 1.1580</span>
<span class="go">    nan</span>
<span class="go">    nan</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.tan">
<code class="descclassname">torch.</code><code class="descname">tan</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.tan" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the tangent of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">-0.6366</span>
<span class="go"> 0.2718</span>
<span class="go"> 0.4469</span>
<span class="go"> 1.3122</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">-0.7392</span>
<span class="go"> 0.2786</span>
<span class="go"> 0.4792</span>
<span class="go"> 3.7801</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.tanh">
<code class="descclassname">torch.</code><code class="descname">tanh</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the hyperbolic tangent of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">-0.6366</span>
<span class="go"> 0.2718</span>
<span class="go"> 0.4469</span>
<span class="go"> 1.3122</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">-0.5625</span>
<span class="go"> 0.2653</span>
<span class="go"> 0.4193</span>
<span class="go"> 0.8648</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.trunc">
<code class="descclassname">torch.</code><code class="descname">trunc</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.trunc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <cite>Tensor</cite> with the truncated integer values of the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.4972</span>
<span class="go"> 1.3512</span>
<span class="go"> 0.1056</span>
<span class="go">-0.2650</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">trunc</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go">-0</span>
<span class="go"> 1</span>
<span class="go"> 0</span>
<span class="go">-0</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="reduction-ops">
<h4>Reduction Ops<a class="headerlink" href="#reduction-ops" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.cumprod">
<code class="descclassname">torch.</code><code class="descname">cumprod</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.cumprod" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the cumulative product of elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> in the dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code>.</p>
<p>For example, if <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is a vector of size N, the result will also be a vector of size N, with elements:
<span class="math">\(y_i = x_1 * x_2 * x_3 * ... * x_i\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the dimension to do the operation over</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1.1148</span>
<span class="go"> 1.8423</span>
<span class="go"> 1.4143</span>
<span class="go">-0.4403</span>
<span class="go"> 1.2859</span>
<span class="go">-1.2514</span>
<span class="go">-0.4748</span>
<span class="go"> 1.1735</span>
<span class="go">-1.6332</span>
<span class="go">-0.4272</span>
<span class="go">[torch.FloatTensor of size 10]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="go"> 1.1148</span>
<span class="go"> 2.0537</span>
<span class="go"> 2.9045</span>
<span class="go">-1.2788</span>
<span class="go">-1.6444</span>
<span class="go"> 2.0578</span>
<span class="go">-0.9770</span>
<span class="go">-1.1466</span>
<span class="go"> 1.8726</span>
<span class="go">-0.8000</span>
<span class="go">[torch.FloatTensor of size 10]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="go"> 1.1148</span>
<span class="go"> 2.0537</span>
<span class="go"> 2.9045</span>
<span class="go">-1.2788</span>
<span class="go">-1.6444</span>
<span class="go">-0.0000</span>
<span class="go"> 0.0000</span>
<span class="go"> 0.0000</span>
<span class="go">-0.0000</span>
<span class="go"> 0.0000</span>
<span class="go">[torch.FloatTensor of size 10]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cumsum">
<code class="descclassname">torch.</code><code class="descname">cumsum</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.cumsum" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the cumulative sum of elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> in the dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code>.</p>
<p>For example, if <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is a vector of size N, the result will also be a vector of size N, with elements:
<span class="math">\(y_i = x_1 + x_2 + x_3 + ... + x_i\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the dimension to do the operation over</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.6039</span>
<span class="go">-0.2214</span>
<span class="go">-0.3705</span>
<span class="go">-0.0169</span>
<span class="go"> 1.3415</span>
<span class="go">-0.1230</span>
<span class="go"> 0.9719</span>
<span class="go"> 0.6081</span>
<span class="go">-0.1286</span>
<span class="go"> 1.0947</span>
<span class="go">[torch.FloatTensor of size 10]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="go">-0.6039</span>
<span class="go">-0.8253</span>
<span class="go">-1.1958</span>
<span class="go">-1.2127</span>
<span class="go"> 0.1288</span>
<span class="go"> 0.0058</span>
<span class="go"> 0.9777</span>
<span class="go"> 1.5858</span>
<span class="go"> 1.4572</span>
<span class="go"> 2.5519</span>
<span class="go">[torch.FloatTensor of size 10]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.dist">
<code class="descclassname">torch.</code><code class="descname">dist</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>p=2</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.dist" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the p-norm of (<code class="xref py py-attr docutils literal"><span class="pre">input</span></code> - <code class="xref py py-attr docutils literal"><span class="pre">other</span></code>)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the Right-hand-side input <cite>Tensor</cite></li>
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; The norm to be computed.</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>

<span class="go"> 0.2505</span>
<span class="go">-0.4571</span>
<span class="go">-0.3733</span>
<span class="go"> 0.7807</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span>

<span class="go"> 0.7782</span>
<span class="go">-0.5185</span>
<span class="go"> 1.4106</span>
<span class="go">-2.4063</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="go">3.302832063224223</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">3.3677282206393286</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">inf</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">5.560028076171875</span>
</pre></div>
</div>
<p>The shapes of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">other</span></code> must be broadcastable.</p>
</dd></dl>

<dl class="function">
<dt id="torch.mean">
<code class="descclassname">torch.</code><code class="descname">mean</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.mean" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">mean</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &rarr; float</dt>
<dd></dd></dl>

<p>Returns the mean value of all elements in the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.2946 -0.9143  2.1809</span>
<span class="go">[torch.FloatTensor of size 1x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">0.32398951053619385</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">mean</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor</dt>
<dd></dd></dl>

<p>Returns the mean value of each row of the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor in the given dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code>.</p>
<p>The output Tensor is of the same size as <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> where it is of size 1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the dimension to reduce</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the result Tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-1.2738 -0.3058  0.1230 -1.9615</span>
<span class="go"> 0.8771 -0.5430 -0.9233  0.9879</span>
<span class="go"> 1.4107  0.0317 -0.6823  0.2255</span>
<span class="go">-1.3854  0.4953 -0.2160  0.2435</span>
<span class="go">[torch.FloatTensor of size 4x4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="go">-0.8545</span>
<span class="go"> 0.0997</span>
<span class="go"> 0.2464</span>
<span class="go">-0.2157</span>
<span class="go">[torch.FloatTensor of size 4x1]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.median">
<code class="descclassname">torch.</code><code class="descname">median</code><span class="sig-paren">(</span><em>input</em>, <em>dim=-1</em>, <em>values=None</em>, <em>indices=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.median" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the median value of each row of the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor in the given dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code>.
Also returns the index location of the median value as a <cite>LongTensor</cite>.</p>
<p>By default, <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> is the last dimension of the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor.</p>
<p>The output Tensors are of the same size as <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> where it is of size 1.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function is not defined for <code class="docutils literal"><span class="pre">torch.cuda.Tensor</span></code> yet.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the dimension to reduce</li>
<li><strong>values</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the result Tensor</li>
<li><strong>indices</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the result index Tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> -0.6891 -0.6662</span>
<span class="go"> 0.2697  0.7412</span>
<span class="go"> 0.5254 -0.7402</span>
<span class="go"> 0.5528 -0.2399</span>
<span class="go">[torch.FloatTensor of size 4x2]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 0.4056 -0.3372  1.0973 -2.4884  0.4334</span>
<span class="go"> 2.1336  0.3841  0.1404 -0.1821 -0.7646</span>
<span class="go">-0.2403  1.3975 -2.0068  0.1298  0.0212</span>
<span class="go">-1.5371 -0.7257 -0.4871 -0.2359 -1.1724</span>
<span class="go">[torch.FloatTensor of size 4x5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">(</span>
<span class="go"> 0.4056</span>
<span class="go"> 0.1404</span>
<span class="go"> 0.0212</span>
<span class="go">-0.7257</span>
<span class="go">[torch.FloatTensor of size 4x1]</span>
<span class="go">,</span>
<span class="go"> 0</span>
<span class="go"> 2</span>
<span class="go"> 4</span>
<span class="go"> 1</span>
<span class="go">[torch.LongTensor of size 4x1]</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.mode">
<code class="descclassname">torch.</code><code class="descname">mode</code><span class="sig-paren">(</span><em>input</em>, <em>dim=-1</em>, <em>values=None</em>, <em>indices=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.mode" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the mode value of each row of the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor in the given dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code>.
Also returns the index location of the mode value as a <cite>LongTensor</cite>.</p>
<p>By default, <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> is the last dimension of the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor.</p>
<p>The output Tensors are of the same size as <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> where it is of size 1.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function is not defined for <code class="docutils literal"><span class="pre">torch.cuda.Tensor</span></code> yet.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the dimension to reduce</li>
<li><strong>values</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the result Tensor</li>
<li><strong>indices</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the result index Tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> -0.6891 -0.6662</span>
<span class="go"> 0.2697  0.7412</span>
<span class="go"> 0.5254 -0.7402</span>
<span class="go"> 0.5528 -0.2399</span>
<span class="go">[torch.FloatTensor of size 4x2]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 0.4056 -0.3372  1.0973 -2.4884  0.4334</span>
<span class="go"> 2.1336  0.3841  0.1404 -0.1821 -0.7646</span>
<span class="go">-0.2403  1.3975 -2.0068  0.1298  0.0212</span>
<span class="go">-1.5371 -0.7257 -0.4871 -0.2359 -1.1724</span>
<span class="go">[torch.FloatTensor of size 4x5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">(</span>
<span class="go">-2.4884</span>
<span class="go">-0.7646</span>
<span class="go">-2.0068</span>
<span class="go">-1.5371</span>
<span class="go">[torch.FloatTensor of size 4x1]</span>
<span class="go">,</span>
<span class="go"> 3</span>
<span class="go"> 4</span>
<span class="go"> 2</span>
<span class="go"> 0</span>
<span class="go">[torch.LongTensor of size 4x1]</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.norm">
<code class="descclassname">torch.</code><code class="descname">norm</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.norm" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">norm</code><span class="sig-paren">(</span><em>input</em>, <em>p=2</em><span class="sig-paren">)</span> &rarr; float</dt>
<dd></dd></dl>

<p>Returns the p-norm of the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; the exponent value in the norm formulation</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.4376 -0.5328  0.9547</span>
<span class="go">[torch.FloatTensor of size 1x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">1.0338925067372466</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">norm</code><span class="sig-paren">(</span><em>input</em>, <em>p</em>, <em>dim</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor</dt>
<dd></dd></dl>

<p>Returns the p-norm of each row of the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor in the given dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code>.</p>
<p>The output Tensor is of the same size as <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> where it is of size 1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; the exponent value in the norm formulation</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the dimension to reduce</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the result Tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.6891 -0.6662</span>
<span class="go"> 0.2697  0.7412</span>
<span class="go"> 0.5254 -0.7402</span>
<span class="go"> 0.5528 -0.2399</span>
<span class="go">[torch.FloatTensor of size 4x2]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="go"> 0.9585</span>
<span class="go"> 0.7888</span>
<span class="go"> 0.9077</span>
<span class="go"> 0.6026</span>
<span class="go">[torch.FloatTensor of size 4x1]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="go"> 2</span>
<span class="go"> 2</span>
<span class="go"> 2</span>
<span class="go"> 2</span>
<span class="go">[torch.FloatTensor of size 4x1]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.prod">
<code class="descclassname">torch.</code><code class="descname">prod</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.prod" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">prod</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &rarr; float</dt>
<dd></dd></dl>

<p>Returns the product of all elements in the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 0.6170  0.3546  0.0253</span>
<span class="go">[torch.FloatTensor of size 1x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">0.005537458061418483</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">prod</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor</dt>
<dd></dd></dl>

<p>Returns the product of each row of the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor in the given dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code>.</p>
<p>The output Tensor is of the same size as <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> where it is of size 1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the dimension to reduce</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the result Tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 0.1598 -0.6884</span>
<span class="go">-0.1831 -0.4412</span>
<span class="go">-0.9925 -0.6244</span>
<span class="go">-0.2416 -0.8080</span>
<span class="go">[torch.FloatTensor of size 4x2]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="go">-0.1100</span>
<span class="go"> 0.0808</span>
<span class="go"> 0.6197</span>
<span class="go"> 0.1952</span>
<span class="go">[torch.FloatTensor of size 4x1]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.std">
<code class="descclassname">torch.</code><code class="descname">std</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.std" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">std</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &rarr; float</dt>
<dd></dd></dl>

<p>Returns the standard-deviation of all elements in the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-1.3063  1.4182 -0.3061</span>
<span class="go">[torch.FloatTensor of size 1x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">1.3782334731508061</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">std</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor</dt>
<dd></dd></dl>

<p>Returns the standard-deviation of each row of the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor in the given dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code>.</p>
<p>The output Tensor is of the same size as <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> where it is of size 1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the dimension to reduce</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the result Tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 0.1889 -2.4856  0.0043  1.8169</span>
<span class="go">-0.7701 -0.4682 -2.2410  0.4098</span>
<span class="go"> 0.1919 -1.1856 -1.0361  0.9085</span>
<span class="go"> 0.0173  1.0662  0.2143 -0.5576</span>
<span class="go">[torch.FloatTensor of size 4x4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="go"> 1.7756</span>
<span class="go"> 1.1025</span>
<span class="go"> 1.0045</span>
<span class="go"> 0.6725</span>
<span class="go">[torch.FloatTensor of size 4x1]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.sum">
<code class="descclassname">torch.</code><code class="descname">sum</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.sum" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">sum</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &rarr; float</dt>
<dd></dd></dl>

<p>Returns the sum of all elements in the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 0.6170  0.3546  0.0253</span>
<span class="go">[torch.FloatTensor of size 1x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">0.9969287421554327</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">sum</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor</dt>
<dd></dd></dl>

<p>Returns the sum of each row of the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor in the given dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code>.</p>
<p>The output Tensor is of the same size as <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> where it is of size 1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the dimension to reduce</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the result Tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.4640  0.0609  0.1122  0.4784</span>
<span class="go">-1.3063  1.6443  0.4714 -0.7396</span>
<span class="go">-1.3561 -0.1959  1.0609 -1.9855</span>
<span class="go"> 2.6833  0.5746 -0.5709 -0.4430</span>
<span class="go">[torch.FloatTensor of size 4x4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="go"> 0.1874</span>
<span class="go"> 0.0698</span>
<span class="go">-2.4767</span>
<span class="go"> 2.2440</span>
<span class="go">[torch.FloatTensor of size 4x1]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.var">
<code class="descclassname">torch.</code><code class="descname">var</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.var" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">var</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &rarr; float</dt>
<dd></dd></dl>

<p>Returns the variance of all elements in the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-1.3063  1.4182 -0.3061</span>
<span class="go">[torch.FloatTensor of size 1x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">1.899527506513334</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">var</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor</dt>
<dd></dd></dl>

<p>Returns the variance of each row of the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor in the given dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code>.</p>
<p>The output Tensor is of the same size as <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> where it is of size 1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the dimension to reduce</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the result Tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-1.2738 -0.3058  0.1230 -1.9615</span>
<span class="go"> 0.8771 -0.5430 -0.9233  0.9879</span>
<span class="go"> 1.4107  0.0317 -0.6823  0.2255</span>
<span class="go">-1.3854  0.4953 -0.2160  0.2435</span>
<span class="go">[torch.FloatTensor of size 4x4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="go"> 0.8859</span>
<span class="go"> 0.9509</span>
<span class="go"> 0.7548</span>
<span class="go"> 0.6949</span>
<span class="go">[torch.FloatTensor of size 4x1]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="comparison-ops">
<h4>Comparison Ops<a class="headerlink" href="#comparison-ops" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.eq">
<code class="descclassname">torch.</code><code class="descname">eq</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.eq" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes element-wise equality</p>
<p>The second argument can be a number or a tensor of the same shape and
type as the first argument.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Tensor to compare</li>
<li><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; Tensor or value to compare</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor. Must be a <cite>ByteTensor</cite> or the same type as <cite>tensor</cite>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><dl class="docutils">
<dt>a <code class="docutils literal"><span class="pre">torch.ByteTensor</span></code> containing a 1 at each location where the tensors are equal and</dt>
<dd><p class="first last">a 0 at every other location</p>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]))</span>
<span class="go">1  0</span>
<span class="go">0  1</span>
<span class="go">[torch.ByteTensor of size 2x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.equal">
<code class="descclassname">torch.</code><code class="descname">equal</code><span class="sig-paren">(</span><em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &rarr; bool<a class="headerlink" href="#torch.equal" title="Permalink to this definition">¶</a></dt>
<dd><p>True if two tensors have the same size and elements, False otherwise.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.ge">
<code class="descclassname">torch.</code><code class="descname">ge</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.ge" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes <cite>tensor &gt;= other</cite> element-wise.</p>
<p>The second argument can be a number or a tensor of the same shape and
type as the first argument.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Tensor to compare</li>
<li><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; Tensor or value to compare</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor. Must be a <cite>ByteTensor</cite> or the same type as <cite>tensor</cite>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">a <code class="docutils literal"><span class="pre">torch.ByteTensor</span></code> containing a 1 at each location where comparison is true.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]))</span>
<span class="go"> 1  1</span>
<span class="go"> 0  1</span>
<span class="go">[torch.ByteTensor of size 2x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.gt">
<code class="descclassname">torch.</code><code class="descname">gt</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.gt" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes <cite>tensor &gt; other</cite> element-wise.</p>
<p>The second argument can be a number or a tensor of the same shape and
type as the first argument.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Tensor to compare</li>
<li><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; Tensor or value to compare</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor. Must be a <cite>ByteTensor</cite> or the same type as <cite>tensor</cite>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">a <code class="docutils literal"><span class="pre">torch.ByteTensor</span></code> containing a 1 at each location where comparison is true.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]))</span>
<span class="go"> 0  1</span>
<span class="go"> 0  0</span>
<span class="go">[torch.ByteTensor of size 2x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.kthvalue">
<code class="descclassname">torch.</code><code class="descname">kthvalue</code><span class="sig-paren">(</span><em>input</em>, <em>k</em>, <em>dim=None</em>, <em>out=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.kthvalue" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the <code class="xref py py-attr docutils literal"><span class="pre">k`th</span> <span class="pre">smallest</span> <span class="pre">element</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">given</span> <span class="pre">:attr:`input</span></code> Tensor along a given dimension.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> is not given, the last dimension of the <cite>input</cite> is chosen.</p>
<p>A tuple of <cite>(values, indices)</cite> is returned, where the <cite>indices</cite> is the indices of
the kth-smallest element in the original <cite>input</cite> Tensor in dimention <cite>dim</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>k</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; k for the k-th smallest element</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; The dimension to sort along</li>
<li><strong>out</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; The output tuple of (Tensor, LongTensor)
can be optionally given to be used as output buffers</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>

<span class="go"> 1</span>
<span class="go"> 2</span>
<span class="go"> 3</span>
<span class="go"> 4</span>
<span class="go"> 5</span>
<span class="go">[torch.FloatTensor of size 5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">kthvalue</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="go">(</span>
<span class="go"> 4</span>
<span class="go">[torch.FloatTensor of size 1]</span>
<span class="go">,</span>
<span class="go"> 3</span>
<span class="go">[torch.LongTensor of size 1]</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.le">
<code class="descclassname">torch.</code><code class="descname">le</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.le" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes <cite>tensor &lt;= other</cite> element-wise.</p>
<p>The second argument can be a number or a tensor of the same shape and
type as the first argument.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Tensor to compare</li>
<li><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; Tensor or value to compare</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor. Must be a <cite>ByteTensor</cite> or the same type as <cite>tensor</cite>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">a <code class="docutils literal"><span class="pre">torch.ByteTensor</span></code> containing a 1 at each location where comparison is true.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]))</span>
<span class="go"> 1  0</span>
<span class="go"> 1  1</span>
<span class="go">[torch.ByteTensor of size 2x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.lt">
<code class="descclassname">torch.</code><code class="descname">lt</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.lt" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes <cite>tensor &lt; other</cite> element-wise.</p>
<p>The second argument can be a number or a tensor of the same shape and
type as the first argument.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Tensor to compare</li>
<li><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; Tensor or value to compare</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor. Must be a <cite>ByteTensor</cite> or the same type as <cite>tensor</cite>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">a <code class="docutils literal"><span class="pre">torch.ByteTensor</span></code> containing a 1 at each location where comparison is true.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">lt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]))</span>
<span class="go"> 0  0</span>
<span class="go"> 1  0</span>
<span class="go">[torch.ByteTensor of size 2x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.max">
<code class="descclassname">torch.</code><code class="descname">max</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.max" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">max</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &rarr; float</dt>
<dd></dd></dl>

<p>Returns the maximum value of all elements in the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 0.4729 -0.2266 -0.2085</span>
<span class="go">[torch.FloatTensor of size 1x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">0.4729</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">max</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>max=None</em>, <em>max_indices=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Returns the maximum value of each row of the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor in the given dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code>.
Also returns the index location of each maximum value found.</p>
<p>The output Tensors are of the same size as <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> where they are of size 1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the dimension to reduce</li>
<li><strong>max</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the result Tensor with maximum values in dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code></li>
<li><strong>max_indices</strong> (<em>LongTensor</em><em>, </em><em>optional</em>) &#8211; the result Tensor with the index locations of the
maximum values in dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;</span> <span class="n">a</span>

<span class="mf">0.0692</span>  <span class="mf">0.3142</span>  <span class="mf">1.2513</span> <span class="o">-</span><span class="mf">0.5428</span>
<span class="mf">0.9288</span>  <span class="mf">0.8552</span> <span class="o">-</span><span class="mf">0.2073</span>  <span class="mf">0.6409</span>
<span class="mf">1.0695</span> <span class="o">-</span><span class="mf">0.0101</span> <span class="o">-</span><span class="mf">2.4507</span> <span class="o">-</span><span class="mf">1.2230</span>
<span class="mf">0.7426</span> <span class="o">-</span><span class="mf">0.7666</span>  <span class="mf">0.4862</span> <span class="o">-</span><span class="mf">0.6628</span>
<span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">4</span><span class="n">x4</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">(</span>
 <span class="mf">1.2513</span>
 <span class="mf">0.9288</span>
 <span class="mf">1.0695</span>
 <span class="mf">0.7426</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">4</span><span class="n">x1</span><span class="p">]</span>
<span class="p">,</span>
 <span class="mi">2</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">4</span><span class="n">x1</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">max</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor</dt>
<dd></dd></dl>

<p>Each element of the Tensor <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is compared with the corresponding element of the Tensor <code class="xref py py-attr docutils literal"><span class="pre">other</span></code>
and an element-wise <cite>max</cite> is taken.</p>
<p>The shapes of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">other</span></code> don&#8217;t need to match.
The total number of elements in each Tensor need to be the same.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When the shapes do not match, the shape of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is used as the shape for the returned output Tensor</p>
</div>
<p><span class="math">\(out_i = max(tensor_i, other_i)\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the second input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1.3869</span>
<span class="go"> 0.3912</span>
<span class="go">-0.8634</span>
<span class="go">-0.5468</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>

<span class="go"> 1.0067</span>
<span class="go">-0.8010</span>
<span class="go"> 0.6258</span>
<span class="go"> 0.3627</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="go"> 1.3869</span>
<span class="go"> 0.3912</span>
<span class="go"> 0.6258</span>
<span class="go"> 0.3627</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.min">
<code class="descclassname">torch.</code><code class="descname">min</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.min" title="Permalink to this definition">¶</a></dt>
<dd><dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">min</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &rarr; float</dt>
<dd></dd></dl>

<p>Returns the minimum value of all elements in the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 0.4729 -0.2266 -0.2085</span>
<span class="go">[torch.FloatTensor of size 1x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="go">-0.22663167119026184</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">min</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>min=None</em>, <em>min_indices=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span></dt>
<dd></dd></dl>

<p>Returns the minimum value of each row of the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor in the given dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code>.
Also returns the index location of each minimum value found.</p>
<p>The output Tensors are of the same size as <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> except in the dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> where they are of size 1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the dimension to reduce</li>
<li><strong>min</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the result Tensor with minimum values in dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code></li>
<li><strong>min_indices</strong> (<em>LongTensor</em><em>, </em><em>optional</em>) &#8211; the result Tensor with the index locations of the
minimum values in dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;</span> <span class="n">a</span>

<span class="mf">0.0692</span>  <span class="mf">0.3142</span>  <span class="mf">1.2513</span> <span class="o">-</span><span class="mf">0.5428</span>
<span class="mf">0.9288</span>  <span class="mf">0.8552</span> <span class="o">-</span><span class="mf">0.2073</span>  <span class="mf">0.6409</span>
<span class="mf">1.0695</span> <span class="o">-</span><span class="mf">0.0101</span> <span class="o">-</span><span class="mf">2.4507</span> <span class="o">-</span><span class="mf">1.2230</span>
<span class="mf">0.7426</span> <span class="o">-</span><span class="mf">0.7666</span>  <span class="mf">0.4862</span> <span class="o">-</span><span class="mf">0.6628</span>
<span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">4</span><span class="n">x4</span><span class="p">]</span>

<span class="o">&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="mf">0.5428</span>
<span class="mf">0.2073</span>
<span class="mf">2.4507</span>
<span class="mf">0.7666</span>
<span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">4</span><span class="n">x1</span><span class="p">]</span>

<span class="mi">3</span>
<span class="mi">2</span>
<span class="mi">2</span>
<span class="mi">1</span>
<span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">4</span><span class="n">x1</span><span class="p">]</span>
</pre></div>
</div>
<dl class="function">
<dt>
<code class="descclassname">torch.</code><code class="descname">min</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor</dt>
<dd></dd></dl>

<p>Each element of the Tensor <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is compared with the corresponding element of the Tensor <code class="xref py py-attr docutils literal"><span class="pre">other</span></code>
and an element-wise <cite>min</cite> is taken. The resulting Tensor is returned.</p>
<p>The shapes of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">other</span></code> don&#8217;t need to match.
The total number of elements in each Tensor need to be the same.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When the shapes do not match, the shape of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is used as the shape for the returned output Tensor</p>
</div>
<p><span class="math">\(out_i = min(tensor_i, other_i)\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the second input <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1.3869</span>
<span class="go"> 0.3912</span>
<span class="go">-0.8634</span>
<span class="go">-0.5468</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>

<span class="go"> 1.0067</span>
<span class="go">-0.8010</span>
<span class="go"> 0.6258</span>
<span class="go"> 0.3627</span>
<span class="go">[torch.FloatTensor of size 4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="go"> 1.0067</span>
<span class="go">-0.8010</span>
<span class="go">-0.8634</span>
<span class="go">-0.5468</span>
<span class="go">[torch.FloatTensor of size 4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.ne">
<code class="descclassname">torch.</code><code class="descname">ne</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.ne" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes <cite>tensor != other</cite> element-wise.</p>
<p>The second argument can be a number or a tensor of the same shape and
type as the first argument.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Tensor to compare</li>
<li><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; Tensor or value to compare</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor. Must be a <cite>ByteTensor</cite> or the same type as <cite>tensor</cite>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">a <code class="docutils literal"><span class="pre">torch.ByteTensor</span></code> containing a 1 at each location where comparison is true.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ne</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]))</span>
<span class="go"> 0  1</span>
<span class="go"> 1  0</span>
<span class="go">[torch.ByteTensor of size 2x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.sort">
<code class="descclassname">torch.</code><code class="descname">sort</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>descending=False</em>, <em>out=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.sort" title="Permalink to this definition">¶</a></dt>
<dd><p>Sorts the elements of the <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor along a given dimension in ascending order by value.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> is not given, the last dimension of the <cite>input</cite> is chosen.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">descending</span></code> is <cite>True</cite> then the elements are sorted in descending order by value.</p>
<p>A tuple of (sorted_tensor, sorted_indices) is returned, where the sorted_indices are the
indices of the elements in the original <cite>input</cite> Tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; The dimension to sort along</li>
<li><strong>descending</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; Controls the sorting order (ascending or descending)</li>
<li><strong>out</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; The output tuple of (Tensor, LongTensor)
can be optionally given to be used as output buffers</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span>

<span class="go">-1.6747  0.0610  0.1190  1.4137</span>
<span class="go">-1.4782  0.7159  1.0341  1.3678</span>
<span class="go">-0.3324 -0.0782  0.3518  0.4763</span>
<span class="go">[torch.FloatTensor of size 3x4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span>

<span class="go"> 0  1  3  2</span>
<span class="go"> 2  1  0  3</span>
<span class="go"> 3  1  0  2</span>
<span class="go">[torch.LongTensor of size 3x4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span>

<span class="go">-1.6747 -0.0782 -1.4782 -0.3324</span>
<span class="go"> 0.3518  0.0610  0.4763  0.1190</span>
<span class="go"> 1.0341  0.7159  1.4137  1.3678</span>
<span class="go">[torch.FloatTensor of size 3x4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span>

<span class="go"> 0  2  1  2</span>
<span class="go"> 2  0  2  0</span>
<span class="go"> 1  1  0  1</span>
<span class="go">[torch.LongTensor of size 3x4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.topk">
<code class="descclassname">torch.</code><code class="descname">topk</code><span class="sig-paren">(</span><em>input</em>, <em>k</em>, <em>dim=None</em>, <em>largest=True</em>, <em>sorted=True</em>, <em>out=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.topk" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the <code class="xref py py-attr docutils literal"><span class="pre">k</span></code> largest elements of the given <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> Tensor along a given dimension.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> is not given, the last dimension of the <cite>input</cite> is chosen.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">largest</span></code> is <cite>False</cite> then the <cite>k</cite> smallest elements are returned.</p>
<p>A tuple of <cite>(values, indices)</cite> is returned, where the <cite>indices</cite> are the indices of
the elements in the original <cite>input</cite> Tensor.</p>
<p>The boolean option <code class="xref py py-attr docutils literal"><span class="pre">sorted</span></code> if <cite>True</cite>, will make sure that the returned <cite>k</cite>
elements are themselves sorted</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>k</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the k in &#8220;top-k&#8221;</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; The dimension to sort along</li>
<li><strong>largest</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; Controls whether to return largest or smallest elements</li>
<li><strong>sorted</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; Controls whether to return the elements in sorted order</li>
<li><strong>out</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; The output tuple of (Tensor, LongTensor)
can be optionally given to be used as output buffers</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>

<span class="go"> 1</span>
<span class="go"> 2</span>
<span class="go"> 3</span>
<span class="go"> 4</span>
<span class="go"> 5</span>
<span class="go">[torch.FloatTensor of size 5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">(</span>
<span class="go"> 5</span>
<span class="go"> 4</span>
<span class="go"> 3</span>
<span class="go">[torch.FloatTensor of size 3]</span>
<span class="go">,</span>
<span class="go"> 4</span>
<span class="go"> 3</span>
<span class="go"> 2</span>
<span class="go">[torch.LongTensor of size 3]</span>
<span class="go">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">largest</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">(</span>
<span class="go"> 1</span>
<span class="go"> 2</span>
<span class="go"> 3</span>
<span class="go">[torch.FloatTensor of size 3]</span>
<span class="go">,</span>
<span class="go"> 0</span>
<span class="go"> 1</span>
<span class="go"> 2</span>
<span class="go">[torch.LongTensor of size 3]</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="other-operations">
<h4>Other Operations<a class="headerlink" href="#other-operations" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.cross">
<code class="descclassname">torch.</code><code class="descname">cross</code><span class="sig-paren">(</span><em>input</em>, <em>other</em>, <em>dim=-1</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.cross" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the cross product of vectors in dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">other</span></code>.</p>
<p><code class="xref py py-attr docutils literal"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">other</span></code> must have the same size, and the size of their <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> dimension should be 3.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code> is not given, it defaults to the first dimension found with the size 3.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>other</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the second input <cite>Tensor</cite></li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; the dimension to take the cross-product in.</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-0.6652 -1.0116 -0.6857</span>
<span class="go"> 0.2286  0.4446 -0.5272</span>
<span class="go"> 0.0476  0.2321  1.9991</span>
<span class="go"> 0.6199  1.1924 -0.9397</span>
<span class="go">[torch.FloatTensor of size 4x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>

<span class="go">-0.1042 -1.1156  0.1947</span>
<span class="go"> 0.9947  0.1149  0.4701</span>
<span class="go">-1.0108  0.8319 -0.0750</span>
<span class="go"> 0.9045 -1.3754  1.0976</span>
<span class="go">[torch.FloatTensor of size 4x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cross</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="go">-0.9619  0.2009  0.6367</span>
<span class="go"> 0.2696 -0.6318 -0.4160</span>
<span class="go">-1.6805 -2.0171  0.2741</span>
<span class="go"> 0.0163 -1.5304 -1.9311</span>
<span class="go">[torch.FloatTensor of size 4x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cross</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="go">-0.9619  0.2009  0.6367</span>
<span class="go"> 0.2696 -0.6318 -0.4160</span>
<span class="go">-1.6805 -2.0171  0.2741</span>
<span class="go"> 0.0163 -1.5304 -1.9311</span>
<span class="go">[torch.FloatTensor of size 4x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.diag">
<code class="descclassname">torch.</code><code class="descname">diag</code><span class="sig-paren">(</span><em>input</em>, <em>diagonal=0</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.diag" title="Permalink to this definition">¶</a></dt>
<dd><ul class="simple">
<li>If <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is a vector (1D Tensor), then returns a 2D square Tensor with the elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>
as the diagonal.</li>
<li>If <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> is a matrix (2D Tensor), then returns a 1D Tensor with the diagonal elements of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</li>
</ul>
<p>The argument <code class="xref py py-attr docutils literal"><span class="pre">diagonal</span></code> controls which diagonal to consider.</p>
<ul class="simple">
<li><code class="xref py py-attr docutils literal"><span class="pre">diagonal</span></code> = 0, is the main diagonal.</li>
<li><code class="xref py py-attr docutils literal"><span class="pre">diagonal</span></code> &gt; 0, is above the main diagonal.</li>
<li><code class="xref py py-attr docutils literal"><span class="pre">diagonal</span></code> &lt; 0, is below the main diagonal.</li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>diagonal</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; the diagonal to consider</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<p>Get the square matrix where the input vector is the diagonal:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1.0480</span>
<span class="go">-2.3405</span>
<span class="go">-1.1138</span>
<span class="go">[torch.FloatTensor of size 3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go"> 1.0480  0.0000  0.0000</span>
<span class="go"> 0.0000 -2.3405  0.0000</span>
<span class="go"> 0.0000  0.0000 -1.1138</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="go"> 0.0000  1.0480  0.0000  0.0000</span>
<span class="go"> 0.0000  0.0000 -2.3405  0.0000</span>
<span class="go"> 0.0000  0.0000  0.0000 -1.1138</span>
<span class="go"> 0.0000  0.0000  0.0000  0.0000</span>
<span class="go">[torch.FloatTensor of size 4x4]</span>
</pre></div>
</div>
<p>Get the k-th diagonal of a given matrix:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">-1.5328 -1.3210 -1.5204</span>
<span class="go"> 0.8596  0.0471 -0.2239</span>
<span class="go">-0.6617  0.0146 -1.0817</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="go">-1.5328</span>
<span class="go"> 0.0471</span>
<span class="go">-1.0817</span>
<span class="go">[torch.FloatTensor of size 3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="go">-1.3210</span>
<span class="go">-0.2239</span>
<span class="go">[torch.FloatTensor of size 2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.histc">
<code class="descclassname">torch.</code><code class="descname">histc</code><span class="sig-paren">(</span><em>input</em>, <em>bins=100</em>, <em>min=0</em>, <em>max=0</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.histc" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the histogram of a tensor.</p>
<p>The elements are sorted into equal width bins between <cite>min</cite> and <cite>max</cite>. If <cite>min</cite>
and <cite>max</cite> are both zero, the minimum and maximum values of the data are used.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Input data</li>
<li><strong>bins</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of histogram bins</li>
<li><strong>min</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Lower end of the range (inclusive)</li>
<li><strong>max</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Upper end of the range (inclusive)</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output argument</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">the histogram</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a></p>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">histc</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="go">FloatTensor([0, 2, 1, 0])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.renorm">
<code class="descclassname">torch.</code><code class="descname">renorm</code><span class="sig-paren">(</span><em>input</em>, <em>p</em>, <em>dim</em>, <em>maxnorm</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.renorm" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Tensor where each sub-tensor of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code> along dimension <code class="xref py py-attr docutils literal"><span class="pre">dim</span></code>
is normalized such that the <cite>p</cite>-norm of the sub-tensor is lower than the value <code class="xref py py-attr docutils literal"><span class="pre">maxnorm</span></code></p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If the norm of a row is lower than <cite>maxnorm</cite>, the row is unchanged</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; The input Tensor</li>
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The power for the norm computation</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The dimension to slice over to get the sub-tensors</li>
<li><strong>maxnorm</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The maximum norm to keep each sub-tensor under</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>

<span class="go"> 1  1  1</span>
<span class="go"> 2  2  2</span>
<span class="go"> 3  3  3</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">renorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="go"> 1.0000  1.0000  1.0000</span>
<span class="go"> 1.6667  1.6667  1.6667</span>
<span class="go"> 1.6667  1.6667  1.6667</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.trace">
<code class="descclassname">torch.</code><code class="descname">trace</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &rarr; float<a class="headerlink" href="#torch.trace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the sum of the elements of the diagonal of the input 2D matrix.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>

<span class="go"> 1  2  3</span>
<span class="go"> 4  5  6</span>
<span class="go"> 7  8  9</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">15.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.tril">
<code class="descclassname">torch.</code><code class="descname">tril</code><span class="sig-paren">(</span><em>input</em>, <em>k=0</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.tril" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the lower triangular part of the matrix (2D Tensor) <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>,
the other elements of the result Tensor <code class="xref py py-attr docutils literal"><span class="pre">out</span></code> are set to 0.</p>
<p>The lower triangular part of the matrix is defined as the elements on and below the diagonal.</p>
<p>The argument <code class="xref py py-attr docutils literal"><span class="pre">k</span></code> controls which diagonal to consider.</p>
<ul class="simple">
<li><code class="xref py py-attr docutils literal"><span class="pre">k</span></code> = 0, is the main diagonal.</li>
<li><code class="xref py py-attr docutils literal"><span class="pre">k</span></code> &gt; 0, is above the main diagonal.</li>
<li><code class="xref py py-attr docutils literal"><span class="pre">k</span></code> &lt; 0, is below the main diagonal.</li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>k</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; the diagonal to consider</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1.3225  1.7304  1.4573</span>
<span class="go">-0.3052 -0.3111 -0.1809</span>
<span class="go"> 1.2469  0.0064 -1.6250</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go"> 1.3225  0.0000  0.0000</span>
<span class="go">-0.3052 -0.3111  0.0000</span>
<span class="go"> 1.2469  0.0064 -1.6250</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="go"> 1.3225  1.7304  0.0000</span>
<span class="go">-0.3052 -0.3111 -0.1809</span>
<span class="go"> 1.2469  0.0064 -1.6250</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">k</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="go"> 0.0000  0.0000  0.0000</span>
<span class="go">-0.3052  0.0000  0.0000</span>
<span class="go"> 1.2469  0.0064  0.0000</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.triu">
<code class="descclassname">torch.</code><code class="descname">triu</code><span class="sig-paren">(</span><em>input</em>, <em>k=0</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.triu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the upper triangular part of the matrix (2D Tensor) <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>,
the other elements of the result Tensor <code class="xref py py-attr docutils literal"><span class="pre">out</span></code> are set to 0.</p>
<p>The upper triangular part of the matrix is defined as the elements on and above the diagonal.</p>
<p>The argument <code class="xref py py-attr docutils literal"><span class="pre">k</span></code> controls which diagonal to consider.</p>
<ul class="simple">
<li><code class="xref py py-attr docutils literal"><span class="pre">k</span></code> = 0, is the main diagonal.</li>
<li><code class="xref py py-attr docutils literal"><span class="pre">k</span></code> &gt; 0, is above the main diagonal.</li>
<li><code class="xref py py-attr docutils literal"><span class="pre">k</span></code> &lt; 0, is below the main diagonal.</li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input <cite>Tensor</cite></li>
<li><strong>k</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; the diagonal to consider</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; The result <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go"> 1.3225  1.7304  1.4573</span>
<span class="go">-0.3052 -0.3111 -0.1809</span>
<span class="go"> 1.2469  0.0064 -1.6250</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="go"> 1.3225  1.7304  1.4573</span>
<span class="go"> 0.0000 -0.3111 -0.1809</span>
<span class="go"> 0.0000  0.0000 -1.6250</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="go"> 0.0000  1.7304  1.4573</span>
<span class="go"> 0.0000  0.0000 -0.1809</span>
<span class="go"> 0.0000  0.0000  0.0000</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">k</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="go"> 1.3225  1.7304  1.4573</span>
<span class="go">-0.3052 -0.3111 -0.1809</span>
<span class="go"> 0.0000  0.0064 -1.6250</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="blas-and-lapack-operations">
<h4>BLAS and LAPACK Operations<a class="headerlink" href="#blas-and-lapack-operations" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.addbmm">
<code class="descclassname">torch.</code><code class="descname">addbmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.addbmm" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a batch matrix-matrix product of matrices stored
in <code class="xref py py-attr docutils literal"><span class="pre">batch1</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">batch2</span></code>,
with a reduced add step (all matrix multiplications get accumulated
along the first dimension).
<code class="xref py py-attr docutils literal"><span class="pre">mat</span></code> is added to the final result.</p>
<p><code class="xref py py-attr docutils literal"><span class="pre">batch1</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">batch2</span></code> must be 3D Tensors each containing the
same number of matrices.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">batch1</span></code> is a <cite>b x n x m</cite> Tensor, <code class="xref py py-attr docutils literal"><span class="pre">batch2</span></code> is a <cite>b x m x p</cite>
Tensor, <code class="xref py py-attr docutils literal"><span class="pre">out</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">mat</span></code> will be <cite>n x p</cite> Tensors.</p>
<p>In other words,
<span class="math">\(res = (beta * M) + (alpha * sum(batch1_i &#64; batch2_i, i = 0, b))\)</span></p>
<p>For inputs of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, args <cite>beta</cite> and <cite>alpha</cite> must be real numbers, otherwise they should
be integers</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>beta</strong> (<em>Number</em><em>, </em><em>optional</em>) &#8211; multiplier for <code class="xref py py-attr docutils literal"><span class="pre">mat</span></code></li>
<li><strong>mat</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; matrix to be added</li>
<li><strong>alpha</strong> (<em>Number</em><em>, </em><em>optional</em>) &#8211; multiplier for <cite>batch1 &#64; batch2</cite></li>
<li><strong>batch1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; First batch of matrices to be multiplied</li>
<li><strong>batch2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Second batch of matrices to be multiplied</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">addbmm</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">batch1</span><span class="p">,</span> <span class="n">batch2</span><span class="p">)</span>

<span class="go"> -3.1162  11.0071   7.3102   0.1824  -7.6892</span>
<span class="go">  1.8265   6.0739   0.4589  -0.5641  -5.4283</span>
<span class="go"> -9.3387  -0.1794  -1.2318  -6.8841  -4.7239</span>
<span class="go">[torch.FloatTensor of size 3x5]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.addmm">
<code class="descclassname">torch.</code><code class="descname">addmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>mat1</em>, <em>mat2</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.addmm" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a matrix multiplication of the matrices <code class="xref py py-attr docutils literal"><span class="pre">mat1</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">mat2</span></code>.
The matrix <code class="xref py py-attr docutils literal"><span class="pre">mat</span></code> is added to the final result.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">mat1</span></code> is a <cite>n x m</cite> Tensor, <code class="xref py py-attr docutils literal"><span class="pre">mat2</span></code> is a <cite>m x p</cite> Tensor,
<code class="xref py py-attr docutils literal"><span class="pre">out</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">mat</span></code> will be <cite>n x p</cite> Tensors.</p>
<p><cite>alpha</cite> and <cite>beta</cite> are scaling factors on <cite>mat1 &#64; mat2</cite> and <cite>mat</cite> respectively.</p>
<p>In other words,
<span class="math">\(out = (beta * M) + (alpha * mat1 &#64; mat2)\)</span></p>
<p>For inputs of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, args <code class="xref py py-attr docutils literal"><span class="pre">beta</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">alpha</span></code> must be real numbers, otherwise
they should be integers</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>beta</strong> (<em>Number</em><em>, </em><em>optional</em>) &#8211; multiplier for <code class="xref py py-attr docutils literal"><span class="pre">mat</span></code></li>
<li><strong>mat</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; matrix to be added</li>
<li><strong>alpha</strong> (<em>Number</em><em>, </em><em>optional</em>) &#8211; multiplier for <cite>mat1 &#64; mat2</cite></li>
<li><strong>mat1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; First matrix to be multiplied</li>
<li><strong>mat2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Second matrix to be multiplied</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mat1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mat2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">addmm</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">mat1</span><span class="p">,</span> <span class="n">mat2</span><span class="p">)</span>

<span class="go">-0.4095 -1.9703  1.3561</span>
<span class="go"> 5.7674 -4.9760  2.7378</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.addmv">
<code class="descclassname">torch.</code><code class="descname">addmv</code><span class="sig-paren">(</span><em>beta=1</em>, <em>tensor</em>, <em>alpha=1</em>, <em>mat</em>, <em>vec</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.addmv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a matrix-vector product of the matrix <code class="xref py py-attr docutils literal"><span class="pre">mat</span></code> and
the vector <code class="xref py py-attr docutils literal"><span class="pre">vec</span></code>.
The vector <code class="xref py py-attr docutils literal"><span class="pre">tensor</span></code> is added to the final result.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">mat</span></code> is a <cite>n x m</cite> Tensor, <code class="xref py py-attr docutils literal"><span class="pre">vec</span></code> is a 1D Tensor of size <cite>m</cite>,
<code class="xref py py-attr docutils literal"><span class="pre">out</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">tensor</span></code> will be 1D of size <cite>n</cite>.</p>
<p><cite>alpha</cite> and <cite>beta</cite> are scaling factors on <cite>mat * vec</cite> and <cite>tensor</cite> respectively.</p>
<p>In other words:</p>
<p><span class="math">\(out = (beta * tensor) + (alpha * (mat &#64; vec2))\)</span></p>
<p>For inputs of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, args <code class="xref py py-attr docutils literal"><span class="pre">beta</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">alpha</span></code> must be real numbers, otherwise
they should be integers</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>beta</strong> (<em>Number</em><em>, </em><em>optional</em>) &#8211; multiplier for <code class="xref py py-attr docutils literal"><span class="pre">tensor</span></code></li>
<li><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; vector to be added</li>
<li><strong>alpha</strong> (<em>Number</em><em>, </em><em>optional</em>) &#8211; multiplier for <cite>mat &#64; vec</cite></li>
<li><strong>mat</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; matrix to be multiplied</li>
<li><strong>vec</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; vector to be multiplied</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">addmv</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">mat</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span>

<span class="go">-2.0939</span>
<span class="go">-2.2950</span>
<span class="go">[torch.FloatTensor of size 2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.addr">
<code class="descclassname">torch.</code><code class="descname">addr</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>vec1</em>, <em>vec2</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.addr" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the outer-product of vectors <code class="xref py py-attr docutils literal"><span class="pre">vec1</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">vec2</span></code>
and adds it to the matrix <code class="xref py py-attr docutils literal"><span class="pre">mat</span></code>.</p>
<p>Optional values <code class="xref py py-attr docutils literal"><span class="pre">beta</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">alpha</span></code> are scalars that multiply
<code class="xref py py-attr docutils literal"><span class="pre">mat</span></code> and <span class="math">\((vec1 \otimes vec2)\)</span> respectively</p>
<p>In other words,
<span class="math">\(out = (beta * mat) + (alpha * vec1 \otimes vec2)\)</span></p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">vec1</span></code> is a vector of size <cite>n</cite> and <code class="xref py py-attr docutils literal"><span class="pre">vec2</span></code> is a vector of size <cite>m</cite>,
then <code class="xref py py-attr docutils literal"><span class="pre">mat</span></code> must be a matrix of size <cite>n x m</cite></p>
<p>For inputs of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, args <code class="xref py py-attr docutils literal"><span class="pre">beta</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">alpha</span></code> must be real numbers, otherwise
they should be integers</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>beta</strong> (<em>Number</em><em>, </em><em>optional</em>) &#8211; Multiplier for <code class="xref py py-attr docutils literal"><span class="pre">mat</span></code></li>
<li><strong>mat</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Matrix to be added</li>
<li><strong>alpha</strong> (<em>Number</em><em>, </em><em>optional</em>) &#8211; Multiplier for outer product of for <code class="xref py py-attr docutils literal"><span class="pre">vec1</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">vec2</span></code></li>
<li><strong>vec1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; First vector of the outer product</li>
<li><strong>vec2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Second vector of the outer product</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vec1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vec2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">addr</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">)</span>
<span class="go"> 1  2</span>
<span class="go"> 2  4</span>
<span class="go"> 3  6</span>
<span class="go">[torch.FloatTensor of size 3x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.baddbmm">
<code class="descclassname">torch.</code><code class="descname">baddbmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.baddbmm" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a batch matrix-matrix product of matrices in <code class="xref py py-attr docutils literal"><span class="pre">batch1</span></code>
and <code class="xref py py-attr docutils literal"><span class="pre">batch2</span></code>.
<code class="xref py py-attr docutils literal"><span class="pre">mat</span></code> is added to the final result.</p>
<p><code class="xref py py-attr docutils literal"><span class="pre">batch1</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">batch2</span></code> must be 3D Tensors each containing the same
number of matrices.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">batch1</span></code> is a <cite>b x n x m</cite> Tensor, <code class="xref py py-attr docutils literal"><span class="pre">batch2</span></code> is a <cite>b x m x p</cite>
Tensor, <code class="xref py py-attr docutils literal"><span class="pre">out</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">mat</span></code> will be <cite>b x n x p</cite> Tensors.</p>
<p>In other words,
<span class="math">\(res_i = (beta * M_i) + (alpha * batch1_i \times batch2_i)\)</span></p>
<p>For inputs of type <cite>FloatTensor</cite> or <cite>DoubleTensor</cite>, args <code class="xref py py-attr docutils literal"><span class="pre">beta</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">alpha</span></code> must be real numbers, otherwise
they should be integers</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>beta</strong> (<em>Number</em><em>, </em><em>optional</em>) &#8211; multiplier for <code class="xref py py-attr docutils literal"><span class="pre">mat</span></code></li>
<li><strong>mat</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; tensor to be added</li>
<li><strong>alpha</strong> (<em>Number</em><em>, </em><em>optional</em>) &#8211; multiplier for <cite>batch1 &#64; batch2</cite></li>
<li><strong>batch1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; First batch of matrices to be multiplied</li>
<li><strong>batch2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Second batch of matrices to be multiplied</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">baddbmm</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">batch1</span><span class="p">,</span> <span class="n">batch2</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([10, 3, 5])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.bmm">
<code class="descclassname">torch.</code><code class="descname">bmm</code><span class="sig-paren">(</span><em>batch1</em>, <em>batch2</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.bmm" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a batch matrix-matrix product of matrices stored in <code class="xref py py-attr docutils literal"><span class="pre">batch1</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">batch2</span></code>.</p>
<p><code class="xref py py-attr docutils literal"><span class="pre">batch1</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">batch2</span></code> must be 3D Tensors each containing the same number of matrices.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">batch1</span></code> is a <cite>b x n x m</cite> Tensor, <code class="xref py py-attr docutils literal"><span class="pre">batch2</span></code> is a <cite>b x m x p</cite> Tensor,
<code class="xref py py-attr docutils literal"><span class="pre">out</span></code> will be a <cite>b x n x p</cite> Tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>batch1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; First batch of matrices to be multiplied</li>
<li><strong>batch2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Second batch of matrices to be multiplied</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">batch1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">batch1</span><span class="p">,</span> <span class="n">batch2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([10, 3, 5])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.btrifact">
<code class="descclassname">torch.</code><code class="descname">btrifact</code><span class="sig-paren">(</span><em>A</em>, <em>info=None</em><span class="sig-paren">)</span> &rarr; Tensor, IntTensor<a class="headerlink" href="#torch.btrifact" title="Permalink to this definition">¶</a></dt>
<dd><p>Batch LU factorization.</p>
<p>Returns a tuple containing the LU factorization and pivots.
The optional argument <cite>info</cite> provides information if the
factorization succeeded for each minibatch example.
The info values are from dgetrf and a non-zero value indicates an error occurred.
The specific values are from cublas if cuda is being used, otherwise LAPACK.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>A</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; tensor to factor.</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A_LU</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">btrifact</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.btrisolve">
<code class="descclassname">torch.</code><code class="descname">btrisolve</code><span class="sig-paren">(</span><em>b</em>, <em>LU_data</em>, <em>LU_pivots</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.btrisolve" title="Permalink to this definition">¶</a></dt>
<dd><p>Batch LU solve.</p>
<p>Returns the LU solve of the linear system Ax = b.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>b</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; RHS tensor.</li>
<li><strong>LU_data</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Pivoted LU factorization of A from btrifact.</li>
<li><strong>LU_pivots</strong> (<em>IntTensor</em>) &#8211; Pivots of the LU factorization.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A_LU</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">btrifact</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">btrisolve</span><span class="p">(</span><span class="o">*</span><span class="n">A_LU</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
<span class="go">6.664001874625056e-08</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.dot">
<code class="descclassname">torch.</code><code class="descname">dot</code><span class="sig-paren">(</span><em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &rarr; float<a class="headerlink" href="#torch.dot" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the dot product (inner product) of two tensors. Both tensors are
treated as 1-D vectors.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">7.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.eig">
<code class="descclassname">torch.</code><code class="descname">eig</code><span class="sig-paren">(</span><em>a</em>, <em>eigenvectors=False</em>, <em>out=None) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.eig" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the eigenvalues and eigenvectors of a real square matrix.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>a</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; A square matrix for which the eigenvalues and eigenvectors will
be computed</li>
<li><strong>eigenvectors</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; <cite>True</cite> to compute both eigenvalues and eigenvectors.
Otherwise, only eigenvalues will be computed.</li>
<li><strong>out</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Output tensors</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p>tuple containing</p>
<blockquote>
<div><ul class="simple">
<li><strong>e</strong> (<em>Tensor</em>): the right eigenvalues of <code class="docutils literal"><span class="pre">a</span></code></li>
<li><strong>v</strong> (<em>Tensor</em>): the eigenvectors of <code class="docutils literal"><span class="pre">a</span></code> if <code class="docutils literal"><span class="pre">eigenvectors`</span> <span class="pre">is</span> <span class="pre">``True</span></code>; otherwise an empty tensor</li>
</ul>
</div></blockquote>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">(<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.gels">
<code class="descclassname">torch.</code><code class="descname">gels</code><span class="sig-paren">(</span><em>B</em>, <em>A</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.gels" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the solution to the least squares and least norm problems for a full
rank <span class="math">\(m\)</span> by <span class="math">\(n\)</span> matrix <span class="math">\(A\)</span>.</p>
<p>If <span class="math">\(m &gt;= n\)</span>, <a class="reference internal" href="#torch.gels" title="torch.gels"><code class="xref py py-func docutils literal"><span class="pre">gels()</span></code></a> solves the least-squares problem:</p>
<div class="math">
\[\begin{array}{ll}
\mbox{minimize} &amp; \|AX-B\|_F.
\end{array}\]</div>
<p>If <span class="math">\(m &lt; n\)</span>, <a class="reference internal" href="#torch.gels" title="torch.gels"><code class="xref py py-func docutils literal"><span class="pre">gels()</span></code></a> solves the least-norm problem:</p>
<div class="math">
\[\begin{array}{ll}
\mbox{minimize} &amp; \|X\|_F &amp; \mbox{subject to} &amp; AX = B.
\end{array}\]</div>
<p>The first <span class="math">\(n\)</span> rows of the returned matrix <span class="math">\(X\)</span> contains the
solution. The remaining rows contain residual information: the euclidean norm
of each column starting at row <span class="math">\(n\)</span> is the residual for the corresponding
column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>B</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; The matrix <span class="math">\(B\)</span></li>
<li><strong>A</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; The <span class="math">\(m\)</span> by <span class="math">\(n\)</span> matrix <span class="math">\(A\)</span></li>
<li><strong>out</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Optional destination tensor</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p>tuple containing:</p>
<blockquote>
<div><ul class="simple">
<li><strong>X</strong> (<em>Tensor</em>): the least squares solution</li>
<li><strong>qr</strong> (<em>Tensor</em>): the details of the QR factorization</li>
</ul>
</div></blockquote>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">(<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a>, <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The returned matrices will always be tranposed, irrespective of the strides
of the input matrices. That is, they will have stride <cite>(1, m)</cite> instead of
<cite>(m, 1)</cite>.</p>
</div>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">],</span>
<span class="go">                      [ 12, 14],</span>
<span class="go">                      [ 14, 12],</span>
<span class="go">                      [ 16, 16],</span>
<span class="go">                      [ 18, 16]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gels</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>
<span class="go">2.0000  1.0000</span>
<span class="go">1.0000  1.0000</span>
<span class="go">1.0000  2.0000</span>
<span class="go">[torch.FloatTensor of size 3x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.geqrf">
<code class="descclassname">torch.</code><code class="descname">geqrf</code><span class="sig-paren">(</span><em>input</em>, <em>out=None) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.geqrf" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a low-level function for calling LAPACK directly.</p>
<p>You&#8217;ll generally want to use <a class="reference internal" href="#torch.qr" title="torch.qr"><code class="xref py py-func docutils literal"><span class="pre">torch.qr()</span></code></a> instead.</p>
<p>Computes a QR decomposition of <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>, but without constructing <cite>Q</cite> and <cite>R</cite> as explicit separate matrices.</p>
<p>Rather, this directly calls the underlying LAPACK function <cite>?geqrf</cite>
which produces a sequence of &#8216;elementary reflectors&#8217;.</p>
<p>See <a class="reference external" href="https://software.intel.com/en-us/node/521004">LAPACK documentation</a> for further details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input matrix</li>
<li><strong>out</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; The result tuple of (Tensor, Tensor)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.ger">
<code class="descclassname">torch.</code><code class="descname">ger</code><span class="sig-paren">(</span><em>vec1</em>, <em>vec2</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.ger" title="Permalink to this definition">¶</a></dt>
<dd><p>Outer product of <code class="xref py py-attr docutils literal"><span class="pre">vec1</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">vec2</span></code>.
If <code class="xref py py-attr docutils literal"><span class="pre">vec1</span></code> is a vector of size <cite>n</cite> and <code class="xref py py-attr docutils literal"><span class="pre">vec2</span></code> is a vector of size <cite>m</cite>,
then <code class="xref py py-attr docutils literal"><span class="pre">out</span></code> must be a matrix of size <cite>n x m</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>vec1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; 1D input vector</li>
<li><strong>vec2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; 1D input vector</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; optional output matrix</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ger</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span>

<span class="go">  1   2   3</span>
<span class="go">  2   4   6</span>
<span class="go">  3   6   9</span>
<span class="go">  4   8  12</span>
<span class="go">[torch.FloatTensor of size 4x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.gesv">
<code class="descclassname">torch.</code><code class="descname">gesv</code><span class="sig-paren">(</span><em>B</em>, <em>A</em>, <em>out=None) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.gesv" title="Permalink to this definition">¶</a></dt>
<dd><p><cite>X, LU = torch.gesv(B, A)</cite> returns the solution to the system of linear
equations represented by <span class="math">\(AX = B\)</span></p>
<p><cite>LU</cite> contains <cite>L</cite> and <cite>U</cite> factors for LU factorization of <cite>A</cite>.</p>
<p><code class="xref py py-attr docutils literal"><span class="pre">A</span></code> has to be a square and non-singular matrix (2D Tensor).</p>
<p>If <cite>A</cite> is an <cite>m x m</cite> matrix and <cite>B</cite> is <cite>m x k</cite>,
the result <cite>LU</cite> is <cite>m x m</cite> and <cite>X</cite> is <cite>m x k</cite> .</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Irrespective of the original strides, the returned matrices
<cite>X</cite> and <cite>LU</cite> will be transposed, i.e. with strides <cite>(1, m)</cite>
instead of <cite>(m, 1)</cite>.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>B</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; input matrix of <cite>m x k</cite> dimensions</li>
<li><strong>A</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; input square matrix of <cite>m x m</cite> dimensions</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; optional output matrix</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">6.80</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.11</span><span class="p">,</span>  <span class="mf">5.66</span><span class="p">,</span>  <span class="mf">5.97</span><span class="p">,</span>  <span class="mf">8.23</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="o">-</span><span class="mf">6.05</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.30</span><span class="p">,</span>  <span class="mf">5.36</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.44</span><span class="p">,</span>  <span class="mf">1.08</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="o">-</span><span class="mf">0.45</span><span class="p">,</span>  <span class="mf">2.58</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.70</span><span class="p">,</span>  <span class="mf">0.27</span><span class="p">,</span>  <span class="mf">9.04</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mf">8.32</span><span class="p">,</span>  <span class="mf">2.71</span><span class="p">,</span>  <span class="mf">4.35</span><span class="p">,</span>  <span class="o">-</span><span class="mf">7.17</span><span class="p">,</span>  <span class="mf">2.14</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="o">-</span><span class="mf">9.67</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.14</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.26</span><span class="p">,</span>  <span class="mf">6.08</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.87</span><span class="p">]])</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">4.02</span><span class="p">,</span>  <span class="mf">6.19</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.22</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.57</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.03</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="o">-</span><span class="mf">1.56</span><span class="p">,</span>  <span class="mf">4.00</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.67</span><span class="p">,</span>  <span class="mf">1.75</span><span class="p">,</span>  <span class="mf">2.86</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mf">9.81</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.09</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.57</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.61</span><span class="p">,</span>  <span class="mf">8.99</span><span class="p">]])</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">LU</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gesv</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">X</span><span class="p">))</span>
<span class="go">9.250057093890353e-06</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.inverse">
<code class="descclassname">torch.</code><code class="descname">inverse</code><span class="sig-paren">(</span><em>input</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.inverse" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes the inverse of the square matrix <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Irrespective of the original strides, the returned matrix will be transposed,
i.e. with strides <cite>(1, m)</cite> instead of <cite>(m, 1)</cite></p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input 2D square <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; the optional output <cite>Tensor</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>

<span class="go"> 0.7800  0.2267  0.7855  0.9479  0.5914  0.7119  0.4437  0.9131  0.1289  0.1982</span>
<span class="go"> 0.0045  0.0425  0.2229  0.4626  0.6210  0.0207  0.6338  0.7067  0.6381  0.8196</span>
<span class="go"> 0.8350  0.7810  0.8526  0.9364  0.7504  0.2737  0.0694  0.5899  0.8516  0.3883</span>
<span class="go"> 0.6280  0.6016  0.5357  0.2936  0.7827  0.2772  0.0744  0.2627  0.6326  0.9153</span>
<span class="go"> 0.7897  0.0226  0.3102  0.0198  0.9415  0.9896  0.3528  0.9397  0.2074  0.6980</span>
<span class="go"> 0.5235  0.6119  0.6522  0.3399  0.3205  0.5555  0.8454  0.3792  0.4927  0.6086</span>
<span class="go"> 0.1048  0.0328  0.5734  0.6318  0.9802  0.4458  0.0979  0.3320  0.3701  0.0909</span>
<span class="go"> 0.2616  0.3485  0.4370  0.5620  0.5291  0.8295  0.7693  0.1807  0.0650  0.8497</span>
<span class="go"> 0.1655  0.2192  0.6913  0.0093  0.0178  0.3064  0.6715  0.5101  0.2561  0.3396</span>
<span class="go"> 0.4370  0.4695  0.8333  0.1180  0.4266  0.4161  0.0699  0.4263  0.8865  0.2578</span>
<span class="go">[torch.FloatTensor of size 10x10]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span>

<span class="go"> 1.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000</span>
<span class="go"> 0.0000  1.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000 -0.0000 -0.0000</span>
<span class="go"> 0.0000  0.0000  1.0000 -0.0000 -0.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000</span>
<span class="go"> 0.0000  0.0000  0.0000  1.0000  0.0000  0.0000  0.0000 -0.0000 -0.0000  0.0000</span>
<span class="go"> 0.0000  0.0000 -0.0000 -0.0000  1.0000  0.0000  0.0000 -0.0000 -0.0000 -0.0000</span>
<span class="go"> 0.0000  0.0000  0.0000 -0.0000  0.0000  1.0000 -0.0000 -0.0000 -0.0000 -0.0000</span>
<span class="go"> 0.0000  0.0000  0.0000 -0.0000  0.0000  0.0000  1.0000  0.0000 -0.0000  0.0000</span>
<span class="go"> 0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000  1.0000 -0.0000  0.0000</span>
<span class="go">-0.0000  0.0000 -0.0000 -0.0000  0.0000  0.0000 -0.0000 -0.0000  1.0000 -0.0000</span>
<span class="go">-0.0000  0.0000 -0.0000 -0.0000 -0.0000  0.0000 -0.0000 -0.0000  0.0000  1.0000</span>
<span class="go">[torch.FloatTensor of size 10x10]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">10</span><span class="p">)))</span> <span class="c1"># Max nonzero</span>
<span class="go">5.096662789583206e-07</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.mm">
<code class="descclassname">torch.</code><code class="descname">mm</code><span class="sig-paren">(</span><em>mat1</em>, <em>mat2</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.mm" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a matrix multiplication of the matrices <code class="xref py py-attr docutils literal"><span class="pre">mat1</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">mat2</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">mat1</span></code> is a <cite>n x m</cite> Tensor, <code class="xref py py-attr docutils literal"><span class="pre">mat2</span></code> is a <cite>m x p</cite> Tensor, <code class="xref py py-attr docutils literal"><span class="pre">out</span></code> will be a <cite>n x p</cite> Tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>mat1</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; First matrix to be multiplied</li>
<li><strong>mat2</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Second matrix to be multiplied</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mat1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mat2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">mat1</span><span class="p">,</span> <span class="n">mat2</span><span class="p">)</span>
<span class="go"> 0.0519 -0.3304  1.2232</span>
<span class="go"> 4.3910 -5.1498  2.7571</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.mv">
<code class="descclassname">torch.</code><code class="descname">mv</code><span class="sig-paren">(</span><em>mat</em>, <em>vec</em>, <em>out=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.mv" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a matrix-vector product of the matrix <code class="xref py py-attr docutils literal"><span class="pre">mat</span></code> and the vector <code class="xref py py-attr docutils literal"><span class="pre">vec</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">mat</span></code> is a <cite>n x m</cite> Tensor, <code class="xref py py-attr docutils literal"><span class="pre">vec</span></code> is a 1D Tensor of size <cite>m</cite>, <code class="xref py py-attr docutils literal"><span class="pre">out</span></code> will be 1D of size <cite>n</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>mat</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; matrix to be multiplied</li>
<li><strong>vec</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; vector to be multiplied</li>
<li><strong>out</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; Output tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mv</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span>
<span class="go">-2.0939</span>
<span class="go">-2.2950</span>
<span class="go">[torch.FloatTensor of size 2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.orgqr">
<code class="descclassname">torch.</code><code class="descname">orgqr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.orgqr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torch.ormqr">
<code class="descclassname">torch.</code><code class="descname">ormqr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.ormqr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torch.potrf">
<code class="descclassname">torch.</code><code class="descname">potrf</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.potrf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torch.potri">
<code class="descclassname">torch.</code><code class="descname">potri</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.potri" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torch.potrs">
<code class="descclassname">torch.</code><code class="descname">potrs</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.potrs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torch.pstrf">
<code class="descclassname">torch.</code><code class="descname">pstrf</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.pstrf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="torch.qr">
<code class="descclassname">torch.</code><code class="descname">qr</code><span class="sig-paren">(</span><em>input</em>, <em>out=None) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.qr" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the QR decomposition of a matrix <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>: returns matrices
<cite>q</cite> and <cite>r</cite> such that <span class="math">\(x = q * r\)</span>, with <cite>q</cite> being an orthogonal matrix
and <cite>r</cite> being an upper triangular matrix.</p>
<p>This returns the thin (reduced) QR factorization.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">precision may be lost if the magnitudes of the elements of <cite>input</cite> are large</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">while it should always give you a valid decomposition, it may not
give you the same one across platforms - it will depend on your
LAPACK implementation.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Irrespective of the original strides, the returned matrix <cite>q</cite> will be
transposed, i.e. with strides <cite>(1, m)</cite> instead of <cite>(m, 1)</cite>.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input 2D <cite>Tensor</cite></li>
<li><strong>out</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; A tuple of Q and R Tensors</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">12</span><span class="p">,</span> <span class="o">-</span><span class="mi">51</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">167</span><span class="p">,</span> <span class="o">-</span><span class="mi">68</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="o">-</span><span class="mi">41</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span>

<span class="go">-0.8571  0.3943  0.3314</span>
<span class="go">-0.4286 -0.9029 -0.0343</span>
<span class="go"> 0.2857 -0.1714  0.9429</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span>

<span class="go"> -14.0000  -21.0000   14.0000</span>
<span class="go">   0.0000 -175.0000   70.0000</span>
<span class="go">   0.0000    0.0000  -35.0000</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span>

<span class="go">  12  -51    4</span>
<span class="go">   6  167  -68</span>
<span class="go">  -4   24  -41</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">q</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span>

<span class="go"> 1 -0  0</span>
<span class="go">-0  1  0</span>
<span class="go"> 0  0  1</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.svd">
<code class="descclassname">torch.</code><code class="descname">svd</code><span class="sig-paren">(</span><em>input</em>, <em>some=True</em>, <em>out=None) -&gt; (Tensor</em>, <em>Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.svd" title="Permalink to this definition">¶</a></dt>
<dd><p><cite>U, S, V = torch.svd(A)</cite> returns the singular value decomposition of a
real matrix <cite>A</cite> of size <cite>(n x m)</cite> such that <span class="math">\(A = USV'*\)</span>.</p>
<p><cite>U</cite> is of shape <cite>n x n</cite></p>
<p><cite>S</cite> is of shape <cite>n x m</cite></p>
<p><cite>V</cite> is of shape <cite>m x m</cite>.</p>
<p><code class="xref py py-attr docutils literal"><span class="pre">some</span></code> represents the number of singular values to be computed.
If <cite>some=True</cite>, it computes some and <cite>some=False</cite> computes all.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Irrespective of the original strides, the returned matrix <cite>U</cite>
will be transposed, i.e. with strides <cite>(1, n)</cite> instead of <cite>(n, 1)</cite>.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input 2D Tensor</li>
<li><strong>some</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; controls the number of singular values to be computed</li>
<li><strong>out</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; the result tuple</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">8.79</span><span class="p">,</span>  <span class="mf">6.11</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.15</span><span class="p">,</span>  <span class="mf">9.57</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.49</span><span class="p">,</span>  <span class="mf">9.84</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mf">9.93</span><span class="p">,</span>  <span class="mf">6.91</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.93</span><span class="p">,</span>  <span class="mf">1.64</span><span class="p">,</span>  <span class="mf">4.02</span><span class="p">,</span>  <span class="mf">0.15</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mf">9.83</span><span class="p">,</span>  <span class="mf">5.04</span><span class="p">,</span>  <span class="mf">4.86</span><span class="p">,</span>  <span class="mf">8.83</span><span class="p">,</span>  <span class="mf">9.80</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.99</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mf">5.45</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.27</span><span class="p">,</span>  <span class="mf">4.85</span><span class="p">,</span>  <span class="mf">0.74</span><span class="p">,</span> <span class="mf">10.00</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.02</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mf">3.16</span><span class="p">,</span>  <span class="mf">7.98</span><span class="p">,</span>  <span class="mf">3.01</span><span class="p">,</span>  <span class="mf">5.80</span><span class="p">,</span>  <span class="mf">4.27</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.31</span><span class="p">]])</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>

<span class="go">  8.7900   9.9300   9.8300   5.4500   3.1600</span>
<span class="go">  6.1100   6.9100   5.0400  -0.2700   7.9800</span>
<span class="go"> -9.1500  -7.9300   4.8600   4.8500   3.0100</span>
<span class="go">  9.5700   1.6400   8.8300   0.7400   5.8000</span>
<span class="go"> -3.4900   4.0200   9.8000  10.0000   4.2700</span>
<span class="go">  9.8400   0.1500  -8.9900  -6.0200  -5.3100</span>
<span class="go">[torch.FloatTensor of size 6x5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">u</span>

<span class="go">-0.5911  0.2632  0.3554  0.3143  0.2299</span>
<span class="go">-0.3976  0.2438 -0.2224 -0.7535 -0.3636</span>
<span class="go">-0.0335 -0.6003 -0.4508  0.2334 -0.3055</span>
<span class="go">-0.4297  0.2362 -0.6859  0.3319  0.1649</span>
<span class="go">-0.4697 -0.3509  0.3874  0.1587 -0.5183</span>
<span class="go"> 0.2934  0.5763 -0.0209  0.3791 -0.6526</span>
<span class="go">[torch.FloatTensor of size 6x5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span>

<span class="go"> 27.4687</span>
<span class="go"> 22.6432</span>
<span class="go">  8.5584</span>
<span class="go">  5.9857</span>
<span class="go">  2.0149</span>
<span class="go">[torch.FloatTensor of size 5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span>

<span class="go">-0.2514  0.8148 -0.2606  0.3967 -0.2180</span>
<span class="go">-0.3968  0.3587  0.7008 -0.4507  0.1402</span>
<span class="go">-0.6922 -0.2489 -0.2208  0.2513  0.5891</span>
<span class="go">-0.3662 -0.3686  0.3859  0.4342 -0.6265</span>
<span class="go">-0.4076 -0.0980 -0.4932 -0.6227 -0.4396</span>
<span class="go">[torch.FloatTensor of size 5x5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">)),</span> <span class="n">v</span><span class="o">.</span><span class="n">t</span><span class="p">()))</span>
<span class="go">8.934150226306685e-06</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.symeig">
<code class="descclassname">torch.</code><code class="descname">symeig</code><span class="sig-paren">(</span><em>input</em>, <em>eigenvectors=False</em>, <em>upper=True</em>, <em>out=None) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.symeig" title="Permalink to this definition">¶</a></dt>
<dd><p><cite>e, V = torch.symeig(input)</cite> returns eigenvalues and eigenvectors
of a symmetric real matrix <code class="xref py py-attr docutils literal"><span class="pre">input</span></code>.</p>
<p><cite>input</cite> and <cite>V</cite> are <cite>m x m</cite> matrices and <cite>e</cite> is a <cite>m</cite> dimensional vector.</p>
<p>This function calculates all eigenvalues (and vectors) of <cite>input</cite>
such that <cite>input = V diag(e) V&#8217;</cite></p>
<p>The boolean argument <code class="xref py py-attr docutils literal"><span class="pre">eigenvectors</span></code> defines computation of
eigenvectors or eigenvalues only.</p>
<p>If it is <cite>False</cite>, only eigenvalues are computed. If it is <cite>True</cite>,
both eigenvalues and eigenvectors are computed.</p>
<p>Since the input matrix <cite>input</cite> is supposed to be symmetric,
only the upper triangular portion is used by default.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">upper</span></code> is <cite>False</cite>, then lower triangular portion is used.</p>
<p>Note: Irrespective of the original strides, the returned matrix <cite>V</cite> will
be transposed, i.e. with strides <cite>(1, m)</cite> instead of <cite>(m, 1)</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the input symmetric matrix</li>
<li><strong>eigenvectors</strong> (<em>boolean</em><em>, </em><em>optional</em>) &#8211; controls whether eigenvectors have to be computed</li>
<li><strong>upper</strong> (<em>boolean</em><em>, </em><em>optional</em>) &#8211; controls whether to consider upper-triangular or lower-triangular region</li>
<li><strong>out</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; The result tuple of (Tensor, Tensor)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span> <span class="mf">1.96</span><span class="p">,</span>  <span class="mf">0.00</span><span class="p">,</span>  <span class="mf">0.00</span><span class="p">,</span>  <span class="mf">0.00</span><span class="p">,</span>  <span class="mf">0.00</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="o">-</span><span class="mf">6.49</span><span class="p">,</span>  <span class="mf">3.80</span><span class="p">,</span>  <span class="mf">0.00</span><span class="p">,</span>  <span class="mf">0.00</span><span class="p">,</span>  <span class="mf">0.00</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="o">-</span><span class="mf">0.47</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.39</span><span class="p">,</span>  <span class="mf">4.17</span><span class="p">,</span>  <span class="mf">0.00</span><span class="p">,</span>  <span class="mf">0.00</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="o">-</span><span class="mf">7.20</span><span class="p">,</span>  <span class="mf">1.50</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.51</span><span class="p">,</span>  <span class="mf">5.70</span><span class="p">,</span>  <span class="mf">0.00</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="o">-</span><span class="mf">0.65</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.34</span><span class="p">,</span>  <span class="mf">2.67</span><span class="p">,</span>  <span class="mf">1.80</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.10</span><span class="p">]])</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">symeig</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">e</span>

<span class="go">-11.0656</span>
<span class="go"> -6.2287</span>
<span class="go">  0.8640</span>
<span class="go">  8.8655</span>
<span class="go"> 16.0948</span>
<span class="go">[torch.FloatTensor of size 5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span>

<span class="go">-0.2981 -0.6075  0.4026 -0.3745  0.4896</span>
<span class="go">-0.5078 -0.2880 -0.4066 -0.3572 -0.6053</span>
<span class="go">-0.0816 -0.3843 -0.6600  0.5008  0.3991</span>
<span class="go">-0.0036 -0.4467  0.4553  0.6204 -0.4564</span>
<span class="go">-0.8041  0.4480  0.1725  0.3108  0.1622</span>
<span class="go">[torch.FloatTensor of size 5x5]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.trtrs">
<code class="descclassname">torch.</code><code class="descname">trtrs</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.trtrs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>
</div>
<span id="document-tensors"></span><div class="section" id="torch-tensor">
<h2>torch.Tensor<a class="headerlink" href="#torch-tensor" title="Permalink to this headline">¶</a></h2>
<p>A <a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal"><span class="pre">torch.Tensor</span></code></a> is a multi-dimensional matrix containing elements of
a single data type.</p>
<p>Torch defines seven CPU tensor types and eight GPU tensor types:</p>
<table border="1" class="docutils">
<colgroup>
<col width="29%" />
<col width="33%" />
<col width="39%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Data type</th>
<th class="head">CPU tensor</th>
<th class="head">GPU tensor</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>32-bit floating point</td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.FloatTensor</span></code></td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.cuda.FloatTensor</span></code></td>
</tr>
<tr class="row-odd"><td>64-bit floating point</td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.DoubleTensor</span></code></td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.cuda.DoubleTensor</span></code></td>
</tr>
<tr class="row-even"><td>16-bit floating point</td>
<td>N/A</td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.cuda.HalfTensor</span></code></td>
</tr>
<tr class="row-odd"><td>8-bit integer (unsigned)</td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.ByteTensor</span></code></td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.cuda.ByteTensor</span></code></td>
</tr>
<tr class="row-even"><td>8-bit integer (signed)</td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.CharTensor</span></code></td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.cuda.CharTensor</span></code></td>
</tr>
<tr class="row-odd"><td>16-bit integer (signed)</td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.ShortTensor</span></code></td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.cuda.ShortTensor</span></code></td>
</tr>
<tr class="row-even"><td>32-bit integer (signed)</td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.IntTensor</span></code></td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.cuda.IntTensor</span></code></td>
</tr>
<tr class="row-odd"><td>64-bit integer (signed)</td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.LongTensor</span></code></td>
<td><code class="xref py py-class docutils literal"><span class="pre">torch.cuda.LongTensor</span></code></td>
</tr>
</tbody>
</table>
<p>The <a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal"><span class="pre">torch.Tensor</span></code></a> constructor is an alias for the default tensor type
(<code class="xref py py-class docutils literal"><span class="pre">torch.FloatTensor</span></code>).</p>
<p>A tensor can be constructed from a Python <a class="reference external" href="https://docs.python.org/2/library/functions.html#list" title="(in Python v2.7)"><code class="xref py py-class docutils literal"><span class="pre">list</span></code></a> or sequence:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="go">1  2  3</span>
<span class="go">4  5  6</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>
</pre></div>
</div>
<p>An empty tensor can be constructed by specifying its size:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="go">0  0  0  0</span>
<span class="go">0  0  0  0</span>
<span class="go">[torch.IntTensor of size 2x4]</span>
</pre></div>
</div>
<p>The contents of a tensor can be accessed and modified using Python&#8217;s indexing
and slicing notation:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
<span class="go">6.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go"> 1  8  3</span>
<span class="go"> 4  5  6</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>
</pre></div>
</div>
<p>Each tensor has an associated <code class="xref py py-class docutils literal"><span class="pre">torch.Storage</span></code>, which holds its data.
The tensor class provides multi-dimensional, <a class="reference external" href="https://en.wikipedia.org/wiki/Stride_of_an_array">strided</a>
view of a storage and defines numeric operations on it.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Methods which mutate a tensor are marked with an underscore suffix.
For example, <code class="xref py py-func docutils literal"><span class="pre">torch.FloatTensor.abs_()</span></code> computes the absolute value
in-place and returns the modified tensor, while <code class="xref py py-func docutils literal"><span class="pre">torch.FloatTensor.abs()</span></code>
computes the result in a new tensor.</p>
</div>
<dl class="class">
<dt id="torch.Tensor">
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><a class="headerlink" href="#torch.Tensor" title="Permalink to this definition">¶</a></dt>
<dt>
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><span class="sig-paren">(</span><em>*sizes</em><span class="sig-paren">)</span></dt>
<dt>
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><span class="sig-paren">(</span><em>size</em><span class="sig-paren">)</span></dt>
<dt>
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><span class="sig-paren">(</span><em>sequence</em><span class="sig-paren">)</span></dt>
<dt>
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><span class="sig-paren">(</span><em>ndarray</em><span class="sig-paren">)</span></dt>
<dt>
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span></dt>
<dt>
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">Tensor</code><span class="sig-paren">(</span><em>storage</em><span class="sig-paren">)</span></dt>
<dd><p>Creates a new tensor from an optional size or data.</p>
<p>If no arguments are given, an empty zero-dimensional tensor is returned.
If a <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.12)"><code class="xref py py-class docutils literal"><span class="pre">numpy.ndarray</span></code></a>, <a class="reference internal" href="#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal"><span class="pre">torch.Tensor</span></code></a>, or <code class="xref py py-class docutils literal"><span class="pre">torch.Storage</span></code>
is given, a new tensor that shares the same data is returned. If a Python
sequence is given, a new tensor is created from a copy of the sequence.</p>
<dl class="method">
<dt id="torch.Tensor.abs">
<code class="descname">abs</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.abs" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.abs" title="torch.abs"><code class="xref py py-func docutils literal"><span class="pre">torch.abs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.abs_">
<code class="descname">abs_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.abs_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.abs" title="torch.Tensor.abs"><code class="xref py py-meth docutils literal"><span class="pre">abs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.acos">
<code class="descname">acos</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.acos" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.acos" title="torch.acos"><code class="xref py py-func docutils literal"><span class="pre">torch.acos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.acos_">
<code class="descname">acos_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.acos_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.acos" title="torch.Tensor.acos"><code class="xref py py-meth docutils literal"><span class="pre">acos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.add">
<code class="descname">add</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.add" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.add" title="torch.add"><code class="xref py py-func docutils literal"><span class="pre">torch.add()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.add_">
<code class="descname">add_</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.add_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.add" title="torch.Tensor.add"><code class="xref py py-meth docutils literal"><span class="pre">add()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addbmm">
<code class="descname">addbmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.addbmm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.addbmm" title="torch.addbmm"><code class="xref py py-func docutils literal"><span class="pre">torch.addbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addbmm_">
<code class="descname">addbmm_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.addbmm_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addbmm" title="torch.Tensor.addbmm"><code class="xref py py-meth docutils literal"><span class="pre">addbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcdiv">
<code class="descname">addcdiv</code><span class="sig-paren">(</span><em>value=1</em>, <em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.addcdiv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.addcdiv" title="torch.addcdiv"><code class="xref py py-func docutils literal"><span class="pre">torch.addcdiv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcdiv_">
<code class="descname">addcdiv_</code><span class="sig-paren">(</span><em>value=1</em>, <em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.addcdiv_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addcdiv" title="torch.Tensor.addcdiv"><code class="xref py py-meth docutils literal"><span class="pre">addcdiv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcmul">
<code class="descname">addcmul</code><span class="sig-paren">(</span><em>value=1</em>, <em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.addcmul" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.addcmul" title="torch.addcmul"><code class="xref py py-func docutils literal"><span class="pre">torch.addcmul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addcmul_">
<code class="descname">addcmul_</code><span class="sig-paren">(</span><em>value=1</em>, <em>tensor1</em>, <em>tensor2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.addcmul_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addcmul" title="torch.Tensor.addcmul"><code class="xref py py-meth docutils literal"><span class="pre">addcmul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmm">
<code class="descname">addmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>mat1</em>, <em>mat2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.addmm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.addmm" title="torch.addmm"><code class="xref py py-func docutils literal"><span class="pre">torch.addmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmm_">
<code class="descname">addmm_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>mat</em>, <em>alpha=1</em>, <em>mat1</em>, <em>mat2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.addmm_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addmm" title="torch.Tensor.addmm"><code class="xref py py-meth docutils literal"><span class="pre">addmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmv">
<code class="descname">addmv</code><span class="sig-paren">(</span><em>beta=1</em>, <em>tensor</em>, <em>alpha=1</em>, <em>mat</em>, <em>vec</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.addmv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.addmv" title="torch.addmv"><code class="xref py py-func docutils literal"><span class="pre">torch.addmv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addmv_">
<code class="descname">addmv_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>tensor</em>, <em>alpha=1</em>, <em>mat</em>, <em>vec</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.addmv_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addmv" title="torch.Tensor.addmv"><code class="xref py py-meth docutils literal"><span class="pre">addmv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addr">
<code class="descname">addr</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>vec1</em>, <em>vec2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.addr" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.addr" title="torch.addr"><code class="xref py py-func docutils literal"><span class="pre">torch.addr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.addr_">
<code class="descname">addr_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>vec1</em>, <em>vec2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.addr_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.addr" title="torch.Tensor.addr"><code class="xref py py-meth docutils literal"><span class="pre">addr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.apply_">
<code class="descname">apply_</code><span class="sig-paren">(</span><em>callable</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.apply_" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the function <code class="xref py py-attr docutils literal"><span class="pre">callable</span></code> to each element in the tensor, replacing
each element with the value returned by <code class="xref py py-attr docutils literal"><span class="pre">callable</span></code>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function only works with CPU tensors and should not be used in code
sections that require high performance.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.asin">
<code class="descname">asin</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.asin" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.asin" title="torch.asin"><code class="xref py py-func docutils literal"><span class="pre">torch.asin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.asin_">
<code class="descname">asin_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.asin_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.asin" title="torch.Tensor.asin"><code class="xref py py-meth docutils literal"><span class="pre">asin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan">
<code class="descname">atan</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.atan" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.atan" title="torch.atan"><code class="xref py py-func docutils literal"><span class="pre">torch.atan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan2">
<code class="descname">atan2</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.atan2" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.atan2" title="torch.atan2"><code class="xref py py-func docutils literal"><span class="pre">torch.atan2()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan2_">
<code class="descname">atan2_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.atan2_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.atan2" title="torch.Tensor.atan2"><code class="xref py py-meth docutils literal"><span class="pre">atan2()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.atan_">
<code class="descname">atan_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.atan_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.atan" title="torch.Tensor.atan"><code class="xref py py-meth docutils literal"><span class="pre">atan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.baddbmm">
<code class="descname">baddbmm</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.baddbmm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.baddbmm" title="torch.baddbmm"><code class="xref py py-func docutils literal"><span class="pre">torch.baddbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.baddbmm_">
<code class="descname">baddbmm_</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1</em>, <em>batch1</em>, <em>batch2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.baddbmm_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.baddbmm" title="torch.Tensor.baddbmm"><code class="xref py py-meth docutils literal"><span class="pre">baddbmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bernoulli">
<code class="descname">bernoulli</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.bernoulli" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.bernoulli" title="torch.bernoulli"><code class="xref py py-func docutils literal"><span class="pre">torch.bernoulli()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bernoulli_">
<code class="descname">bernoulli_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.bernoulli_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.bernoulli" title="torch.Tensor.bernoulli"><code class="xref py py-meth docutils literal"><span class="pre">bernoulli()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.bmm">
<code class="descname">bmm</code><span class="sig-paren">(</span><em>batch2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.bmm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.bmm" title="torch.bmm"><code class="xref py py-func docutils literal"><span class="pre">torch.bmm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.byte">
<code class="descname">byte</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.byte" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this tensor to byte type</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cauchy_">
<code class="descname">cauchy_</code><span class="sig-paren">(</span><em>median=0</em>, <em>sigma=1</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.cauchy_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the tensor with numbers drawn from the Cauchy distribution:</p>
<div class="math">
\[P(x) = \dfrac{1}{\pi} \dfrac{\sigma}{(x - median)^2 + \sigma^2}\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ceil">
<code class="descname">ceil</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.ceil" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.ceil" title="torch.ceil"><code class="xref py py-func docutils literal"><span class="pre">torch.ceil()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ceil_">
<code class="descname">ceil_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.ceil_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.ceil" title="torch.Tensor.ceil"><code class="xref py py-meth docutils literal"><span class="pre">ceil()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.char">
<code class="descname">char</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.char" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this tensor to char type</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.chunk">
<code class="descname">chunk</code><span class="sig-paren">(</span><em>n_chunks</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.chunk" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits this tensor into a tuple of tensors.</p>
<p>See <a class="reference internal" href="index.html#torch.chunk" title="torch.chunk"><code class="xref py py-func docutils literal"><span class="pre">torch.chunk()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.clamp">
<code class="descname">clamp</code><span class="sig-paren">(</span><em>min</em>, <em>max</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.clamp" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.clamp" title="torch.clamp"><code class="xref py py-func docutils literal"><span class="pre">torch.clamp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.clamp_">
<code class="descname">clamp_</code><span class="sig-paren">(</span><em>min</em>, <em>max</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.clamp_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.clamp" title="torch.Tensor.clamp"><code class="xref py py-meth docutils literal"><span class="pre">clamp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of the tensor. The copy has the same size and data type as the
original tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.contiguous">
<code class="descname">contiguous</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.contiguous" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a contiguous Tensor containing the same data as this tensor. If this
tensor is contiguous, this function returns the original tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.copy_">
<code class="descname">copy_</code><span class="sig-paren">(</span><em>src</em>, <em>async=False</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.copy_" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the elements from <code class="xref py py-attr docutils literal"><span class="pre">src</span></code> into this tensor and returns this tensor.</p>
<p>The source tensor should have the same number of elements as this tensor. It
may be of a different data type or reside on a different device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>src</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Source tensor to copy</li>
<li><strong>async</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If True and this copy is between CPU and GPU, then the copy
may occur asynchronously with respect to the host. For other
copies, this argument has no effect.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cos">
<code class="descname">cos</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.cos" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.cos" title="torch.cos"><code class="xref py py-func docutils literal"><span class="pre">torch.cos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cos_">
<code class="descname">cos_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.cos_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.cos" title="torch.Tensor.cos"><code class="xref py py-meth docutils literal"><span class="pre">cos()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cosh">
<code class="descname">cosh</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.cosh" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.cosh" title="torch.cosh"><code class="xref py py-func docutils literal"><span class="pre">torch.cosh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cosh_">
<code class="descname">cosh_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.cosh_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.cosh" title="torch.Tensor.cosh"><code class="xref py py-meth docutils literal"><span class="pre">cosh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cpu">
<code class="descname">cpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a CPU copy of this tensor if it&#8217;s not already on the CPU</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cross">
<code class="descname">cross</code><span class="sig-paren">(</span><em>other</em>, <em>dim=-1</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.cross" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.cross" title="torch.cross"><code class="xref py py-func docutils literal"><span class="pre">torch.cross()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cuda">
<code class="descname">cuda</code><span class="sig-paren">(</span><em>device=None</em>, <em>async=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this object in CUDA memory.</p>
<p>If this object is already in CUDA memory and on the correct device, then
no copy is performed and the original object is returned.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>device</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The destination GPU id. Defaults to the current device.</li>
<li><strong>async</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If True and the source is in pinned memory, the copy will
be asynchronous with respect to the host. Otherwise, the
argument has no effect.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cumprod">
<code class="descname">cumprod</code><span class="sig-paren">(</span><em>dim</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.cumprod" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.cumprod" title="torch.cumprod"><code class="xref py py-func docutils literal"><span class="pre">torch.cumprod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.cumsum">
<code class="descname">cumsum</code><span class="sig-paren">(</span><em>dim</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.cumsum" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.cumsum" title="torch.cumsum"><code class="xref py py-func docutils literal"><span class="pre">torch.cumsum()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.data_ptr">
<code class="descname">data_ptr</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; int<a class="headerlink" href="#torch.Tensor.data_ptr" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the address of the first element of this tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.diag">
<code class="descname">diag</code><span class="sig-paren">(</span><em>diagonal=0</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.diag" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.diag" title="torch.diag"><code class="xref py py-func docutils literal"><span class="pre">torch.diag()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dim">
<code class="descname">dim</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; int<a class="headerlink" href="#torch.Tensor.dim" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of dimensions of this tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dist">
<code class="descname">dist</code><span class="sig-paren">(</span><em>other</em>, <em>p=2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.dist" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.dist" title="torch.dist"><code class="xref py py-func docutils literal"><span class="pre">torch.dist()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.div">
<code class="descname">div</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.div" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.div" title="torch.div"><code class="xref py py-func docutils literal"><span class="pre">torch.div()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.div_">
<code class="descname">div_</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.div_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.div" title="torch.Tensor.div"><code class="xref py py-meth docutils literal"><span class="pre">div()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.dot">
<code class="descname">dot</code><span class="sig-paren">(</span><em>tensor2</em><span class="sig-paren">)</span> &rarr; float<a class="headerlink" href="#torch.Tensor.dot" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.dot" title="torch.dot"><code class="xref py py-func docutils literal"><span class="pre">torch.dot()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.double">
<code class="descname">double</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this tensor to double type</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.eig">
<code class="descname">eig</code><span class="sig-paren">(</span><em>eigenvectors=False) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.eig" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.eig" title="torch.eig"><code class="xref py py-func docutils literal"><span class="pre">torch.eig()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.element_size">
<code class="descname">element_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; int<a class="headerlink" href="#torch.Tensor.element_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the size in bytes of an individual element.</p>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">()</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
<span class="go">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ByteTensor</span><span class="p">()</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
<span class="go">1</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.eq">
<code class="descname">eq</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.eq" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.eq" title="torch.eq"><code class="xref py py-func docutils literal"><span class="pre">torch.eq()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.eq_">
<code class="descname">eq_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.eq_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.eq" title="torch.Tensor.eq"><code class="xref py py-meth docutils literal"><span class="pre">eq()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.equal">
<code class="descname">equal</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &rarr; bool<a class="headerlink" href="#torch.Tensor.equal" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.equal" title="torch.equal"><code class="xref py py-func docutils literal"><span class="pre">torch.equal()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.exp">
<code class="descname">exp</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.exp" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.exp" title="torch.exp"><code class="xref py py-func docutils literal"><span class="pre">torch.exp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.exp_">
<code class="descname">exp_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.exp_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.exp" title="torch.Tensor.exp"><code class="xref py py-meth docutils literal"><span class="pre">exp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.expand">
<code class="descname">expand</code><span class="sig-paren">(</span><em>tensor</em>, <em>sizes</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.expand" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new view of the tensor with singleton dimensions expanded
to a larger size.</p>
<p>Tensor can be also expanded to a larger number of dimensions, and the
new ones will be appended at the front.</p>
<p>Expanding a tensor does not allocate new memory, but only creates a
new view on the existing tensor where a dimension of size one is
expanded to a larger size by setting the <code class="docutils literal"><span class="pre">stride</span></code> to 0. Any dimension
of size 1 can be expanded to an arbitrary value without allocating new
memory.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*sizes</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) &#8211; The desired expanded size</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="go"> 1  1  1  1</span>
<span class="go"> 2  2  2  2</span>
<span class="go"> 3  3  3  3</span>
<span class="go">[torch.FloatTensor of size 3x4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.expand_as">
<code class="descname">expand_as</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.expand_as" title="Permalink to this definition">¶</a></dt>
<dd><p>Expands this tensor to the size of the specified tensor.</p>
<p>This is equivalent to:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.exponential_">
<code class="descname">exponential_</code><span class="sig-paren">(</span><em>lambd=1</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.exponential_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills this tensor with elements drawn from the exponential distribution:</p>
<div class="math">
\[P(x) = \lambda e^{-\lambda x}\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.fill_">
<code class="descname">fill_</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.fill_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills this tensor with the specified value.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.float">
<code class="descname">float</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this tensor to float type</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.floor">
<code class="descname">floor</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.floor" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.floor" title="torch.floor"><code class="xref py py-func docutils literal"><span class="pre">torch.floor()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.floor_">
<code class="descname">floor_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.floor_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.floor" title="torch.Tensor.floor"><code class="xref py py-meth docutils literal"><span class="pre">floor()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.fmod">
<code class="descname">fmod</code><span class="sig-paren">(</span><em>divisor</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.fmod" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.fmod" title="torch.fmod"><code class="xref py py-func docutils literal"><span class="pre">torch.fmod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.fmod_">
<code class="descname">fmod_</code><span class="sig-paren">(</span><em>divisor</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.fmod_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.fmod" title="torch.Tensor.fmod"><code class="xref py py-meth docutils literal"><span class="pre">fmod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.frac">
<code class="descname">frac</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.frac" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.frac" title="torch.frac"><code class="xref py py-func docutils literal"><span class="pre">torch.frac()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.frac_">
<code class="descname">frac_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.frac_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.frac" title="torch.Tensor.frac"><code class="xref py py-meth docutils literal"><span class="pre">frac()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gather">
<code class="descname">gather</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.gather" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.gather" title="torch.gather"><code class="xref py py-func docutils literal"><span class="pre">torch.gather()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ge">
<code class="descname">ge</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.ge" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.ge" title="torch.ge"><code class="xref py py-func docutils literal"><span class="pre">torch.ge()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ge_">
<code class="descname">ge_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.ge_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.ge" title="torch.Tensor.ge"><code class="xref py py-meth docutils literal"><span class="pre">ge()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gels">
<code class="descname">gels</code><span class="sig-paren">(</span><em>A</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.gels" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.gels" title="torch.gels"><code class="xref py py-func docutils literal"><span class="pre">torch.gels()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.geometric_">
<code class="descname">geometric_</code><span class="sig-paren">(</span><em>p</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.geometric_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills this tensor with elements drawn from the geometric distribution:</p>
<div class="math">
\[P(X=k) = (1 - p)^{k - 1} p\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.geqrf">
<code class="descname">geqrf</code><span class="sig-paren">(</span><em>) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.geqrf" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.geqrf" title="torch.geqrf"><code class="xref py py-func docutils literal"><span class="pre">torch.geqrf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ger">
<code class="descname">ger</code><span class="sig-paren">(</span><em>vec2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.ger" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.ger" title="torch.ger"><code class="xref py py-func docutils literal"><span class="pre">torch.ger()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gesv">
<code class="descname">gesv</code><span class="sig-paren">(</span><em>A</em><span class="sig-paren">)</span> &rarr; Tensor, Tensor<a class="headerlink" href="#torch.Tensor.gesv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.gesv" title="torch.gesv"><code class="xref py py-func docutils literal"><span class="pre">torch.gesv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gt">
<code class="descname">gt</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.gt" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.gt" title="torch.gt"><code class="xref py py-func docutils literal"><span class="pre">torch.gt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.gt_">
<code class="descname">gt_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.gt_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.gt" title="torch.Tensor.gt"><code class="xref py py-meth docutils literal"><span class="pre">gt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.half">
<code class="descname">half</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this tensor to half-precision float type</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.histc">
<code class="descname">histc</code><span class="sig-paren">(</span><em>bins=100</em>, <em>min=0</em>, <em>max=0</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.histc" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.histc" title="torch.histc"><code class="xref py py-func docutils literal"><span class="pre">torch.histc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index">
<code class="descname">index</code><span class="sig-paren">(</span><em>m</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.index" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects elements from this tensor using a binary mask or along a given
dimension. The expression <code class="docutils literal"><span class="pre">tensor.index(m)</span></code> is equivalent to <code class="docutils literal"><span class="pre">tensor[m]</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>m</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><em>ByteTensor</em><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#slice" title="(in Python v2.7)"><em>slice</em></a>) &#8211; The dimension or mask used to select elements</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_add_">
<code class="descname">index_add_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>tensor</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.index_add_" title="Permalink to this definition">¶</a></dt>
<dd><p>Accumulate the elements of tensor into the original tensor by adding to the
indices in the order given in index. The shape of tensor must exactly match the
elements indexed or an error will be raised.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Dimension along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) &#8211; Indices to select from tensor</li>
<li><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Tensor containing values to add</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">  2   3   4</span>
<span class="go">  8   9  10</span>
<span class="go">  5   6   7</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_copy_">
<code class="descname">index_copy_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>tensor</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.index_copy_" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the elements of tensor into the original tensor by selecting the
indices in the order given in index. The shape of tensor must exactly match the
elements indexed or an error will be raised.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Dimension along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) &#8211; Indices to select from tensor</li>
<li><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Tensor containing values to copy</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_copy_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go"> 1  2  3</span>
<span class="go"> 7  8  9</span>
<span class="go"> 4  5  6</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_fill_">
<code class="descname">index_fill_</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em>, <em>val</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.index_fill_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the elements of the original tensor with value <code class="xref py py-attr docutils literal"><span class="pre">val</span></code> by selecting
the indices in the order given in index.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Dimension along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) &#8211; Indices</li>
<li><strong>val</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; Value to fill</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">index_fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">-1  2 -1</span>
<span class="go">-1  5 -1</span>
<span class="go">-1  8 -1</span>
<span class="go">[torch.FloatTensor of size 3x3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.index_select">
<code class="descname">index_select</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.index_select" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.index_select" title="torch.index_select"><code class="xref py py-func docutils literal"><span class="pre">torch.index_select()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.int">
<code class="descname">int</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.int" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this tensor to int type</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.inverse">
<code class="descname">inverse</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.inverse" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.inverse" title="torch.inverse"><code class="xref py py-func docutils literal"><span class="pre">torch.inverse()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_contiguous">
<code class="descname">is_contiguous</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; bool<a class="headerlink" href="#torch.Tensor.is_contiguous" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if this tensor is contiguous in memory in C order.</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.Tensor.is_cuda">
<code class="descname">is_cuda</code><a class="headerlink" href="#torch.Tensor.is_cuda" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_pinned">
<code class="descname">is_pinned</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.is_pinned" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns true if this tensor resides in pinned memory</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_set_to">
<code class="descname">is_set_to</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span> &rarr; bool<a class="headerlink" href="#torch.Tensor.is_set_to" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns True if this object refers to the same <code class="docutils literal"><span class="pre">THTensor</span></code> object from the
Torch C API as the given tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.is_signed">
<code class="descname">is_signed</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.is_signed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.kthvalue">
<code class="descname">kthvalue</code><span class="sig-paren">(</span><em>k</em>, <em>dim=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.kthvalue" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.kthvalue" title="torch.kthvalue"><code class="xref py py-func docutils literal"><span class="pre">torch.kthvalue()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.le">
<code class="descname">le</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.le" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.le" title="torch.le"><code class="xref py py-func docutils literal"><span class="pre">torch.le()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.le_">
<code class="descname">le_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.le_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.le" title="torch.Tensor.le"><code class="xref py py-meth docutils literal"><span class="pre">le()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lerp">
<code class="descname">lerp</code><span class="sig-paren">(</span><em>start</em>, <em>end</em>, <em>weight</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.lerp" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.lerp" title="torch.lerp"><code class="xref py py-func docutils literal"><span class="pre">torch.lerp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lerp_">
<code class="descname">lerp_</code><span class="sig-paren">(</span><em>start</em>, <em>end</em>, <em>weight</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.lerp_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.lerp" title="torch.Tensor.lerp"><code class="xref py py-meth docutils literal"><span class="pre">lerp()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log">
<code class="descname">log</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.log" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.log" title="torch.log"><code class="xref py py-func docutils literal"><span class="pre">torch.log()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log1p">
<code class="descname">log1p</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.log1p" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.log1p" title="torch.log1p"><code class="xref py py-func docutils literal"><span class="pre">torch.log1p()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log1p_">
<code class="descname">log1p_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.log1p_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.log1p" title="torch.Tensor.log1p"><code class="xref py py-meth docutils literal"><span class="pre">log1p()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log_">
<code class="descname">log_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.log_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.log" title="torch.Tensor.log"><code class="xref py py-meth docutils literal"><span class="pre">log()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.log_normal_">
<code class="descname">log_normal_</code><span class="sig-paren">(</span><em>mean=1</em>, <em>std=2</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.log_normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills this tensor with numbers samples from the log-normal distribution
parameterized by the given mean (µ) and standard deviation (σ). Note that
<a class="reference internal" href="index.html#torch.mean" title="torch.mean"><code class="xref py py-attr docutils literal"><span class="pre">mean</span></code></a> and <code class="xref py py-attr docutils literal"><span class="pre">stdv</span></code> are the mean and standard deviation of the
underlying normal distribution, and not of the returned distribution:</p>
<div class="math">
\[P(x) = \dfrac{1}{x \sigma \sqrt{2\pi}} e^{-\dfrac{(\ln x - \mu)^2}{2\sigma^2}}\]</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.long">
<code class="descname">long</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.long" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this tensor to long type</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lt">
<code class="descname">lt</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.lt" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.lt" title="torch.lt"><code class="xref py py-func docutils literal"><span class="pre">torch.lt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.lt_">
<code class="descname">lt_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.lt_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.lt" title="torch.Tensor.lt"><code class="xref py py-meth docutils literal"><span class="pre">lt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.map_">
<code class="descname">map_</code><span class="sig-paren">(</span><em>tensor</em>, <em>callable</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.map_" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="xref py py-attr docutils literal"><span class="pre">callable</span></code> for each element in this tensor and the given tensor
and stores the results in this tensor. The <code class="xref py py-attr docutils literal"><span class="pre">callable</span></code> should have the
signature:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">callable</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">number</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_copy_">
<code class="descname">masked_copy_</code><span class="sig-paren">(</span><em>mask</em>, <em>source</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.masked_copy_" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies elements from <code class="xref py py-attr docutils literal"><span class="pre">source</span></code> into this tensor at positions where the
<code class="xref py py-attr docutils literal"><span class="pre">mask</span></code> is one. The <code class="xref py py-attr docutils literal"><span class="pre">mask</span></code> should have the same number of elements
as this tensor. The <code class="xref py py-attr docutils literal"><span class="pre">source</span></code> should have at least as many elements as the
number of ones in <code class="xref py py-attr docutils literal"><span class="pre">mask</span></code></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>mask</strong> (<em>ByteTensor</em>) &#8211; The binary mask</li>
<li><strong>source</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; The tensor to copy from</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <code class="xref py py-attr docutils literal"><span class="pre">mask</span></code> operates on the <code class="xref py py-attr docutils literal"><span class="pre">self</span></code> tensor, not on the given
<code class="xref py py-attr docutils literal"><span class="pre">source</span></code> tensor.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_fill_">
<code class="descname">masked_fill_</code><span class="sig-paren">(</span><em>mask</em>, <em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.masked_fill_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills elements of this tensor with <code class="xref py py-attr docutils literal"><span class="pre">value</span></code> where <code class="xref py py-attr docutils literal"><span class="pre">mask</span></code> is one.
The <code class="xref py py-attr docutils literal"><span class="pre">mask</span></code> should have the same number of elements as this tensor, but
the shape may differ.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>mask</strong> (<em>ByteTensor</em>) &#8211; The binary mask</li>
<li><strong>value</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; The value to fill</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.masked_select">
<code class="descname">masked_select</code><span class="sig-paren">(</span><em>mask</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.masked_select" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.masked_select" title="torch.masked_select"><code class="xref py py-func docutils literal"><span class="pre">torch.masked_select()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.max">
<code class="descname">max</code><span class="sig-paren">(</span><em>dim=None) -&gt; float or (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.max" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.max" title="torch.max"><code class="xref py py-func docutils literal"><span class="pre">torch.max()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mean">
<code class="descname">mean</code><span class="sig-paren">(</span><em>dim=None) -&gt; float or (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.mean" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.mean" title="torch.mean"><code class="xref py py-func docutils literal"><span class="pre">torch.mean()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.median">
<code class="descname">median</code><span class="sig-paren">(</span><em>dim=-1</em>, <em>values=None</em>, <em>indices=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.median" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.median" title="torch.median"><code class="xref py py-func docutils literal"><span class="pre">torch.median()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.min">
<code class="descname">min</code><span class="sig-paren">(</span><em>dim=None) -&gt; float or (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.min" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.min" title="torch.min"><code class="xref py py-func docutils literal"><span class="pre">torch.min()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mm">
<code class="descname">mm</code><span class="sig-paren">(</span><em>mat2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.mm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.mm" title="torch.mm"><code class="xref py py-func docutils literal"><span class="pre">torch.mm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mode">
<code class="descname">mode</code><span class="sig-paren">(</span><em>dim=-1</em>, <em>values=None</em>, <em>indices=None) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.mode" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.mode" title="torch.mode"><code class="xref py py-func docutils literal"><span class="pre">torch.mode()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mul">
<code class="descname">mul</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.mul" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.mul" title="torch.mul"><code class="xref py py-func docutils literal"><span class="pre">torch.mul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mul_">
<code class="descname">mul_</code><span class="sig-paren">(</span><em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.mul_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.mul" title="torch.Tensor.mul"><code class="xref py py-meth docutils literal"><span class="pre">mul()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.multinomial">
<code class="descname">multinomial</code><span class="sig-paren">(</span><em>num_samples</em>, <em>replacement=False</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.multinomial" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.multinomial" title="torch.multinomial"><code class="xref py py-func docutils literal"><span class="pre">torch.multinomial()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.mv">
<code class="descname">mv</code><span class="sig-paren">(</span><em>vec</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.mv" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.mv" title="torch.mv"><code class="xref py py-func docutils literal"><span class="pre">torch.mv()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.narrow">
<code class="descname">narrow</code><span class="sig-paren">(</span><em>dimension</em>, <em>start</em>, <em>length</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.narrow" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor that is a narrowed version of this tensor. The dimension
<a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal"><span class="pre">dim</span></code></a> is narrowed from <code class="xref py py-attr docutils literal"><span class="pre">start</span></code> to <code class="xref py py-attr docutils literal"><span class="pre">start</span> <span class="pre">+</span> <span class="pre">length</span></code>. The
returned tensor and this tensor share the same underlying storage.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dimension</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The dimension along which to narrow</li>
<li><strong>start</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The starting dimension</li>
<li><strong>length</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; </li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go"> 1  2  3</span>
<span class="go"> 4  5  6</span>
<span class="go">[torch.FloatTensor of size 2x3]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go"> 2  3</span>
<span class="go"> 5  6</span>
<span class="go"> 8  9</span>
<span class="go">[torch.FloatTensor of size 3x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ndimension">
<code class="descname">ndimension</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; int<a class="headerlink" href="#torch.Tensor.ndimension" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-meth docutils literal"><span class="pre">dim()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ne">
<code class="descname">ne</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.ne" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.ne" title="torch.ne"><code class="xref py py-func docutils literal"><span class="pre">torch.ne()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ne_">
<code class="descname">ne_</code><span class="sig-paren">(</span><em>other</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.ne_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.ne" title="torch.Tensor.ne"><code class="xref py py-meth docutils literal"><span class="pre">ne()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.neg">
<code class="descname">neg</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.neg" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.neg" title="torch.neg"><code class="xref py py-func docutils literal"><span class="pre">torch.neg()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.neg_">
<code class="descname">neg_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.neg_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.neg" title="torch.Tensor.neg"><code class="xref py py-meth docutils literal"><span class="pre">neg()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.nelement">
<code class="descname">nelement</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; int<a class="headerlink" href="#torch.Tensor.nelement" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#torch.Tensor.numel" title="torch.Tensor.numel"><code class="xref py py-meth docutils literal"><span class="pre">numel()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.new">
<code class="descname">new</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.new" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs a new tensor of the same data type.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.nonzero">
<code class="descname">nonzero</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; LongTensor<a class="headerlink" href="#torch.Tensor.nonzero" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.nonzero" title="torch.nonzero"><code class="xref py py-func docutils literal"><span class="pre">torch.nonzero()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.norm">
<code class="descname">norm</code><span class="sig-paren">(</span><em>p=2</em><span class="sig-paren">)</span> &rarr; float<a class="headerlink" href="#torch.Tensor.norm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.norm" title="torch.norm"><code class="xref py py-func docutils literal"><span class="pre">torch.norm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.normal_">
<code class="descname">normal_</code><span class="sig-paren">(</span><em>mean=0</em>, <em>std=1</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.normal_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills this tensor with elements samples from the normal distribution
parameterized by <a class="reference internal" href="index.html#torch.mean" title="torch.mean"><code class="xref py py-attr docutils literal"><span class="pre">mean</span></code></a> and <a class="reference internal" href="index.html#torch.std" title="torch.std"><code class="xref py py-attr docutils literal"><span class="pre">std</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.numel">
<code class="descname">numel</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; int<a class="headerlink" href="#torch.Tensor.numel" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.numel" title="torch.numel"><code class="xref py py-func docutils literal"><span class="pre">torch.numel()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.numpy">
<code class="descname">numpy</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; ndarray<a class="headerlink" href="#torch.Tensor.numpy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns this tensor as a NumPy <code class="xref py py-class docutils literal"><span class="pre">ndarray</span></code>. This tensor and the returned
<code class="xref py py-class docutils literal"><span class="pre">ndarray</span></code> share the same underlying storage. Changes to this tensor will
be reflected in the <code class="xref py py-class docutils literal"><span class="pre">ndarray</span></code> and vice versa.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.orgqr">
<code class="descname">orgqr</code><span class="sig-paren">(</span><em>input2</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.orgqr" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.orgqr" title="torch.orgqr"><code class="xref py py-func docutils literal"><span class="pre">torch.orgqr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.ormqr">
<code class="descname">ormqr</code><span class="sig-paren">(</span><em>input2</em>, <em>input3</em>, <em>left=True</em>, <em>transpose=False</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.ormqr" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.ormqr" title="torch.ormqr"><code class="xref py py-func docutils literal"><span class="pre">torch.ormqr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.permute">
<code class="descname">permute</code><span class="sig-paren">(</span><em>*dims</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.permute" title="Permalink to this definition">¶</a></dt>
<dd><p>Permute the dimensions of this tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*dims</strong> (<em>int...</em>) &#8211; The desired ordering of dimensions</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 3, 5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([5, 2, 3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pin_memory">
<code class="descname">pin_memory</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.pin_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the tensor to pinned memory, if it&#8217;s not already pinned.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.potrf">
<code class="descname">potrf</code><span class="sig-paren">(</span><em>upper=True</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.potrf" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.potrf" title="torch.potrf"><code class="xref py py-func docutils literal"><span class="pre">torch.potrf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.potri">
<code class="descname">potri</code><span class="sig-paren">(</span><em>upper=True</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.potri" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.potri" title="torch.potri"><code class="xref py py-func docutils literal"><span class="pre">torch.potri()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.potrs">
<code class="descname">potrs</code><span class="sig-paren">(</span><em>input2</em>, <em>upper=True</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.potrs" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.potrs" title="torch.potrs"><code class="xref py py-func docutils literal"><span class="pre">torch.potrs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pow">
<code class="descname">pow</code><span class="sig-paren">(</span><em>exponent</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.pow" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.pow" title="torch.pow"><code class="xref py py-func docutils literal"><span class="pre">torch.pow()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pow_">
<code class="descname">pow_</code><span class="sig-paren">(</span><em>exponent</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.pow_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.pow" title="torch.Tensor.pow"><code class="xref py py-meth docutils literal"><span class="pre">pow()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.prod">
<code class="descname">prod</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; float<a class="headerlink" href="#torch.Tensor.prod" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.prod" title="torch.prod"><code class="xref py py-func docutils literal"><span class="pre">torch.prod()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.pstrf">
<code class="descname">pstrf</code><span class="sig-paren">(</span><em>upper=True</em>, <em>tol=-1) -&gt; (Tensor</em>, <em>IntTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.pstrf" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.pstrf" title="torch.pstrf"><code class="xref py py-func docutils literal"><span class="pre">torch.pstrf()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.qr">
<code class="descname">qr</code><span class="sig-paren">(</span><em>) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.qr" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.qr" title="torch.qr"><code class="xref py py-func docutils literal"><span class="pre">torch.qr()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.random_">
<code class="descname">random_</code><span class="sig-paren">(</span><em>from=0</em>, <em>to=None</em>, <em>*</em>, <em>generator=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.random_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills this tensor with numbers sampled from the uniform distribution or
discrete uniform distribution over [from, to - 1]. If not specified, the
values are only bounded by this tensor&#8217;s data type.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.reciprocal">
<code class="descname">reciprocal</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.reciprocal" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.reciprocal" title="torch.reciprocal"><code class="xref py py-func docutils literal"><span class="pre">torch.reciprocal()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.reciprocal_">
<code class="descname">reciprocal_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.reciprocal_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.reciprocal" title="torch.Tensor.reciprocal"><code class="xref py py-meth docutils literal"><span class="pre">reciprocal()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.remainder">
<code class="descname">remainder</code><span class="sig-paren">(</span><em>divisor</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.remainder" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.remainder" title="torch.remainder"><code class="xref py py-func docutils literal"><span class="pre">torch.remainder()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.remainder_">
<code class="descname">remainder_</code><span class="sig-paren">(</span><em>divisor</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.remainder_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.remainder" title="torch.Tensor.remainder"><code class="xref py py-meth docutils literal"><span class="pre">remainder()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.renorm">
<code class="descname">renorm</code><span class="sig-paren">(</span><em>p</em>, <em>dim</em>, <em>maxnorm</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.renorm" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.renorm" title="torch.renorm"><code class="xref py py-func docutils literal"><span class="pre">torch.renorm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.renorm_">
<code class="descname">renorm_</code><span class="sig-paren">(</span><em>p</em>, <em>dim</em>, <em>maxnorm</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.renorm_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.renorm" title="torch.Tensor.renorm"><code class="xref py py-meth docutils literal"><span class="pre">renorm()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.repeat">
<code class="descname">repeat</code><span class="sig-paren">(</span><em>*sizes</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.repeat" title="Permalink to this definition">¶</a></dt>
<dd><p>Repeats this tensor along the specified dimensions.</p>
<p>Unlike <a class="reference internal" href="#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-meth docutils literal"><span class="pre">expand()</span></code></a>, this function copies the tensor&#8217;s data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>*sizes</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) &#8211; The number of times to repeat this tensor along each dimension</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go"> 1  2  3  1  2  3</span>
<span class="go"> 1  2  3  1  2  3</span>
<span class="go"> 1  2  3  1  2  3</span>
<span class="go"> 1  2  3  1  2  3</span>
<span class="go">[torch.FloatTensor of size 4x6]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 2, 3])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.resize_">
<code class="descname">resize_</code><span class="sig-paren">(</span><em>*sizes</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.resize_" title="Permalink to this definition">¶</a></dt>
<dd><p>Resizes this tensor to the specified size. If the number of elements is
larger than the current storage size, then the underlying storage is resized
to fit the new number of elements. If the number of elements is smaller, the
underlying storage is not changed. Existing elements are preserved but any new
memory is uninitialized.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>sizes</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) &#8211; The desired size</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go"> 1  2</span>
<span class="go"> 3  4</span>
<span class="go">[torch.FloatTensor of size 2x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.resize_as_">
<code class="descname">resize_as_</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.resize_as_" title="Permalink to this definition">¶</a></dt>
<dd><p>Resizes the current tensor to be the same size as the specified tensor. This is
equivalent to:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.round">
<code class="descname">round</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.round" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.round" title="torch.round"><code class="xref py py-func docutils literal"><span class="pre">torch.round()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.round_">
<code class="descname">round_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.round_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.round" title="torch.Tensor.round"><code class="xref py py-meth docutils literal"><span class="pre">round()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.rsqrt">
<code class="descname">rsqrt</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.rsqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.rsqrt" title="torch.rsqrt"><code class="xref py py-func docutils literal"><span class="pre">torch.rsqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.rsqrt_">
<code class="descname">rsqrt_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.rsqrt_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.rsqrt" title="torch.Tensor.rsqrt"><code class="xref py py-meth docutils literal"><span class="pre">rsqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.scatter_">
<code class="descname">scatter_</code><span class="sig-paren">(</span><em>input</em>, <em>dim</em>, <em>index</em>, <em>src</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.scatter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Writes all values from the Tensor <code class="xref py py-attr docutils literal"><span class="pre">src</span></code> into self at the indices specified
in the <a class="reference internal" href="#torch.Tensor.index" title="torch.Tensor.index"><code class="xref py py-attr docutils literal"><span class="pre">index</span></code></a> Tensor. The indices are specified with respect to the
given dimension, dim, in the manner described in <a class="reference internal" href="#torch.Tensor.gather" title="torch.Tensor.gather"><code class="xref py py-meth docutils literal"><span class="pre">gather()</span></code></a>.</p>
<p>Note that, as for gather, the values of index must be between <cite>0</cite> and <cite>(self.size(dim) -1)</cite>
inclusive and all values in a row along the specified dimension must be unique.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; The source tensor</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The axis along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) &#8211; The indices of elements to scatter</li>
<li><strong>src</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; The source element(s) to scatter</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>

<span class="go"> 0.4319  0.6500  0.4080  0.8760  0.2355</span>
<span class="go"> 0.2609  0.4711  0.8486  0.8573  0.1029</span>
<span class="go">[torch.FloatTensor of size 2x5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">x</span><span class="p">)</span>

<span class="go"> 0.4319  0.4711  0.8486  0.8760  0.2355</span>
<span class="go"> 0.0000  0.6500  0.0000  0.8573  0.0000</span>
<span class="go"> 0.2609  0.0000  0.4080  0.0000  0.1029</span>
<span class="go">[torch.FloatTensor of size 3x5]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]),</span> <span class="mf">1.23</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span>

<span class="go"> 0.0000  0.0000  1.2300  0.0000</span>
<span class="go"> 0.0000  0.0000  0.0000  1.2300</span>
<span class="go">[torch.FloatTensor of size 2x4]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.select">
<code class="descname">select</code><span class="sig-paren">(</span><em>dim</em>, <em>index</em><span class="sig-paren">)</span> &rarr; Tensor or number<a class="headerlink" href="#torch.Tensor.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Slices the tensor along the selected dimension at the given index. If this
tensor is one dimensional, this function returns a number. Otherwise, it
returns a tensor with the given dimension removed.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Dimension to slice</li>
<li><strong>index</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Index to select</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><a class="reference internal" href="#torch.Tensor.select" title="torch.Tensor.select"><code class="xref py py-meth docutils literal"><span class="pre">select()</span></code></a> is equivalent to slicing. For example, <code class="docutils literal"><span class="pre">tensor.select(0,</span> <span class="pre">index)</span></code>
is equivalent to <code class="docutils literal"><span class="pre">tensor[index]</span></code> and <code class="docutils literal"><span class="pre">tensor.select(2,</span> <span class="pre">index)</span></code> is equivalent
to <code class="docutils literal"><span class="pre">tensor[:,:,index]</span></code>.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.set_">
<code class="descname">set_</code><span class="sig-paren">(</span><em>source=None</em>, <em>storage_offset=0</em>, <em>size=None</em>, <em>stride=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.set_" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the underlying storage, size, and strides. If <code class="xref py py-attr docutils literal"><span class="pre">source</span></code> is a tensor,
this tensor will share the same storage and have the same size and strides
as the given tensor. Changes to elements in one tensor will be reflected in the
other.</p>
<p>If <code class="xref py py-attr docutils literal"><span class="pre">source</span></code> is a <code class="xref py py-class docutils literal"><span class="pre">Storage</span></code>, the method sets the underlying
storage, offset, size, and stride.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>source</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><em>Storage</em>) &#8211; The tensor or storage to use</li>
<li><strong>storage_offset</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The offset in the storage</li>
<li><strong>size</strong> (<em>torch.Size</em>) &#8211; The desired size. Defaults to the size of the source.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; The desired stride. Defaults to C-contiguous strides.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.share_memory_">
<code class="descname">share_memory_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.share_memory_" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves the underlying storage to shared memory.</p>
<p>This is a no-op if the underlying storage is already in shared memory
and for CUDA tensors. Tensors in shared memory cannot be resized.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.short">
<code class="descname">short</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.short" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this tensor to short type</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sigmoid">
<code class="descname">sigmoid</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.sigmoid" title="torch.sigmoid"><code class="xref py py-func docutils literal"><span class="pre">torch.sigmoid()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sigmoid_">
<code class="descname">sigmoid_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.sigmoid_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sigmoid" title="torch.Tensor.sigmoid"><code class="xref py py-meth docutils literal"><span class="pre">sigmoid()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sign">
<code class="descname">sign</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.sign" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.sign" title="torch.sign"><code class="xref py py-func docutils literal"><span class="pre">torch.sign()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sign_">
<code class="descname">sign_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.sign_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sign" title="torch.Tensor.sign"><code class="xref py py-meth docutils literal"><span class="pre">sign()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sin">
<code class="descname">sin</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.sin" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.sin" title="torch.sin"><code class="xref py py-func docutils literal"><span class="pre">torch.sin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sin_">
<code class="descname">sin_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.sin_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sin" title="torch.Tensor.sin"><code class="xref py py-meth docutils literal"><span class="pre">sin()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sinh">
<code class="descname">sinh</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.sinh" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.sinh" title="torch.sinh"><code class="xref py py-func docutils literal"><span class="pre">torch.sinh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sinh_">
<code class="descname">sinh_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.sinh_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sinh" title="torch.Tensor.sinh"><code class="xref py py-meth docutils literal"><span class="pre">sinh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.size">
<code class="descname">size</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; torch.Size<a class="headerlink" href="#torch.Tensor.size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the size of the tensor. The returned value is a subclass of
<code class="xref py py-class docutils literal"><span class="pre">tuple</span></code>.</p>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([3, 4, 5])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sort">
<code class="descname">sort</code><span class="sig-paren">(</span><em>dim=None</em>, <em>descending=False) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.sort" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.sort" title="torch.sort"><code class="xref py py-func docutils literal"><span class="pre">torch.sort()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.split">
<code class="descname">split</code><span class="sig-paren">(</span><em>split_size</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.split" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits this tensor into a tuple of tensors.</p>
<p>See <a class="reference internal" href="index.html#torch.split" title="torch.split"><code class="xref py py-func docutils literal"><span class="pre">torch.split()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sqrt">
<code class="descname">sqrt</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.sqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.sqrt" title="torch.sqrt"><code class="xref py py-func docutils literal"><span class="pre">torch.sqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sqrt_">
<code class="descname">sqrt_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.sqrt_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sqrt" title="torch.Tensor.sqrt"><code class="xref py py-meth docutils literal"><span class="pre">sqrt()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.squeeze">
<code class="descname">squeeze</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.squeeze" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.squeeze" title="torch.squeeze"><code class="xref py py-func docutils literal"><span class="pre">torch.squeeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.squeeze_">
<code class="descname">squeeze_</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.squeeze_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.squeeze" title="torch.Tensor.squeeze"><code class="xref py py-meth docutils literal"><span class="pre">squeeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.std">
<code class="descname">std</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; float<a class="headerlink" href="#torch.Tensor.std" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.std" title="torch.std"><code class="xref py py-func docutils literal"><span class="pre">torch.std()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.storage">
<code class="descname">storage</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; torch.Storage<a class="headerlink" href="#torch.Tensor.storage" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the underlying storage</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.storage_offset">
<code class="descname">storage_offset</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; int<a class="headerlink" href="#torch.Tensor.storage_offset" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns this tensor&#8217;s offset in the underlying storage in terms of number of
storage elements (not bytes).</p>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">:]</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
<span class="go">3</span>
</pre></div>
</div>
</dd></dl>

<dl class="classmethod">
<dt id="torch.Tensor.storage_type">
<em class="property">classmethod </em><code class="descname">storage_type</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.storage_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.Tensor.stride">
<code class="descname">stride</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; tuple<a class="headerlink" href="#torch.Tensor.stride" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the stride of the tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sub">
<code class="descname">sub</code><span class="sig-paren">(</span><em>value</em>, <em>other</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.sub" title="Permalink to this definition">¶</a></dt>
<dd><p>Subtracts a scalar or tensor from this tensor. If both <code class="xref py py-attr docutils literal"><span class="pre">value</span></code> and
<code class="xref py py-attr docutils literal"><span class="pre">other</span></code> are specified, each element of <code class="xref py py-attr docutils literal"><span class="pre">other</span></code> is scaled by
<code class="xref py py-attr docutils literal"><span class="pre">value</span></code> before being used.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sub_">
<code class="descname">sub_</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.sub_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.sub" title="torch.Tensor.sub"><code class="xref py py-meth docutils literal"><span class="pre">sub()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.sum">
<code class="descname">sum</code><span class="sig-paren">(</span><em>dim=None</em><span class="sig-paren">)</span> &rarr; float<a class="headerlink" href="#torch.Tensor.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.sum" title="torch.sum"><code class="xref py py-func docutils literal"><span class="pre">torch.sum()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.svd">
<code class="descname">svd</code><span class="sig-paren">(</span><em>some=True) -&gt; (Tensor</em>, <em>Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.svd" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.svd" title="torch.svd"><code class="xref py py-func docutils literal"><span class="pre">torch.svd()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.symeig">
<code class="descname">symeig</code><span class="sig-paren">(</span><em>eigenvectors=False</em>, <em>upper=True) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.symeig" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.symeig" title="torch.symeig"><code class="xref py py-func docutils literal"><span class="pre">torch.symeig()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.t">
<code class="descname">t</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.t" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.t" title="torch.t"><code class="xref py py-func docutils literal"><span class="pre">torch.t()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.t_">
<code class="descname">t_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.t_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.t" title="torch.Tensor.t"><code class="xref py py-meth docutils literal"><span class="pre">t()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tan">
<code class="descname">tan</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.tan" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.tan" title="torch.tan"><code class="xref py py-func docutils literal"><span class="pre">torch.tan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tan_">
<code class="descname">tan_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.tan_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.tan" title="torch.Tensor.tan"><code class="xref py py-meth docutils literal"><span class="pre">tan()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tanh">
<code class="descname">tanh</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.tanh" title="torch.tanh"><code class="xref py py-func docutils literal"><span class="pre">torch.tanh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tanh_">
<code class="descname">tanh_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.tanh_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.tanh" title="torch.Tensor.tanh"><code class="xref py py-meth docutils literal"><span class="pre">tanh()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tolist">
<code class="descname">tolist</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.tolist" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a nested list represenation of this tensor.</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.topk">
<code class="descname">topk</code><span class="sig-paren">(</span><em>k</em>, <em>dim=None</em>, <em>largest=True</em>, <em>sorted=True) -&gt; (Tensor</em>, <em>LongTensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.topk" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.topk" title="torch.topk"><code class="xref py py-func docutils literal"><span class="pre">torch.topk()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trace">
<code class="descname">trace</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; float<a class="headerlink" href="#torch.Tensor.trace" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.trace" title="torch.trace"><code class="xref py py-func docutils literal"><span class="pre">torch.trace()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.transpose">
<code class="descname">transpose</code><span class="sig-paren">(</span><em>dim0</em>, <em>dim1</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.transpose" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.transpose" title="torch.transpose"><code class="xref py py-func docutils literal"><span class="pre">torch.transpose()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.transpose_">
<code class="descname">transpose_</code><span class="sig-paren">(</span><em>dim0</em>, <em>dim1</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.transpose_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.transpose" title="torch.Tensor.transpose"><code class="xref py py-meth docutils literal"><span class="pre">transpose()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tril">
<code class="descname">tril</code><span class="sig-paren">(</span><em>k=0</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.tril" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.tril" title="torch.tril"><code class="xref py py-func docutils literal"><span class="pre">torch.tril()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.tril_">
<code class="descname">tril_</code><span class="sig-paren">(</span><em>k=0</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.tril_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.tril" title="torch.Tensor.tril"><code class="xref py py-meth docutils literal"><span class="pre">tril()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.triu">
<code class="descname">triu</code><span class="sig-paren">(</span><em>k=0</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.triu" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.triu" title="torch.triu"><code class="xref py py-func docutils literal"><span class="pre">torch.triu()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.triu_">
<code class="descname">triu_</code><span class="sig-paren">(</span><em>k=0</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.triu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.triu" title="torch.Tensor.triu"><code class="xref py py-meth docutils literal"><span class="pre">triu()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trtrs">
<code class="descname">trtrs</code><span class="sig-paren">(</span><em>A</em>, <em>upper=True</em>, <em>transpose=False</em>, <em>unitriangular=False) -&gt; (Tensor</em>, <em>Tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.trtrs" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.trtrs" title="torch.trtrs"><code class="xref py py-func docutils literal"><span class="pre">torch.trtrs()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trunc">
<code class="descname">trunc</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.trunc" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.trunc" title="torch.trunc"><code class="xref py py-func docutils literal"><span class="pre">torch.trunc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.trunc_">
<code class="descname">trunc_</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.trunc_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.trunc" title="torch.Tensor.trunc"><code class="xref py py-meth docutils literal"><span class="pre">trunc()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.type">
<code class="descname">type</code><span class="sig-paren">(</span><em>new_type=None</em>, <em>async=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this object to the specified type.</p>
<p>If this is already of the correct type, no copy is performed and the
original object is returned.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>new_type</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#type" title="(in Python v2.7)"><em>type</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/string.html#module-string" title="(in Python v2.7)"><em>string</em></a>) &#8211; The desired type</li>
<li><strong>async</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If True, and the source is in pinned memory and
destination is on the GPU or vice versa, the copy is
performed asynchronously with respect to the host.
Otherwise, the argument has no effect.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.type_as">
<code class="descname">type_as</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.type_as" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns this tensor cast to the type of the given tensor.</p>
<p>This is a no-op if the tensor is already of the correct type. This is
equivalent to:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">type</span><span class="p">())</span>
</pre></div>
</div>
<dl class="docutils">
<dt>Params:</dt>
<dd>tensor (Tensor): the tensor which has the desired type</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unfold">
<code class="descname">unfold</code><span class="sig-paren">(</span><em>dim</em>, <em>size</em>, <em>step</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.unfold" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor which contains all slices of size <a class="reference internal" href="#torch.Tensor.size" title="torch.Tensor.size"><code class="xref py py-attr docutils literal"><span class="pre">size</span></code></a> in
the dimension <a class="reference internal" href="#torch.Tensor.dim" title="torch.Tensor.dim"><code class="xref py py-attr docutils literal"><span class="pre">dim</span></code></a>.</p>
<p>Step between two slices is given by <code class="xref py py-attr docutils literal"><span class="pre">step</span></code>.</p>
<p>If <cite>sizedim</cite> is the original size of dimension dim, the size of dimension <cite>dim</cite>
in the returned tensor will be <cite>(sizedim - size) / step + 1</cite></p>
<p>An additional dimension of size size is appended in the returned tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; dimension in which unfolding happens</li>
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; size of each slice that is unfolded</li>
<li><strong>step</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the step between each slice</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>

<span class="go"> 1</span>
<span class="go"> 2</span>
<span class="go"> 3</span>
<span class="go"> 4</span>
<span class="go"> 5</span>
<span class="go"> 6</span>
<span class="go"> 7</span>
<span class="go">[torch.FloatTensor of size 7]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="go"> 1  2</span>
<span class="go"> 2  3</span>
<span class="go"> 3  4</span>
<span class="go"> 4  5</span>
<span class="go"> 5  6</span>
<span class="go"> 6  7</span>
<span class="go">[torch.FloatTensor of size 6x2]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="go"> 1  2</span>
<span class="go"> 3  4</span>
<span class="go"> 5  6</span>
<span class="go">[torch.FloatTensor of size 3x2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.uniform_">
<code class="descname">uniform_</code><span class="sig-paren">(</span><em>from=0</em>, <em>to=1</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.uniform_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills this tensor with numbers sampled from the uniform distribution:</p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unsqueeze">
<code class="descname">unsqueeze</code><span class="sig-paren">(</span><em>dim</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.unsqueeze" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.unsqueeze" title="torch.unsqueeze"><code class="xref py py-func docutils literal"><span class="pre">torch.unsqueeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.unsqueeze_">
<code class="descname">unsqueeze_</code><span class="sig-paren">(</span><em>dim</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.unsqueeze_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.Tensor.unsqueeze" title="torch.Tensor.unsqueeze"><code class="xref py py-meth docutils literal"><span class="pre">unsqueeze()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.var">
<code class="descname">var</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &rarr; float<a class="headerlink" href="#torch.Tensor.var" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="index.html#torch.var" title="torch.var"><code class="xref py py-func docutils literal"><span class="pre">torch.var()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.view">
<code class="descname">view</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span> &rarr; Tensor<a class="headerlink" href="#torch.Tensor.view" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new tensor with the same data but different size.</p>
<p>The returned tensor shares the same data and must have the same number
of elements, but may have a different size. A tensor must be
<a class="reference internal" href="#torch.Tensor.contiguous" title="torch.Tensor.contiguous"><code class="xref py py-func docutils literal"><span class="pre">contiguous()</span></code></a> to be viewed.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>args</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) &#8211; Desired size</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([4, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([16])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>  <span class="c1"># the size -1 is inferred from other dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([2, 8])</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.view_as">
<code class="descname">view_as</code><span class="sig-paren">(</span><em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.view_as" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns this tensor viewed as the size as the specified tensor.</p>
<p>This is equivalent to:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.Tensor.zero_">
<code class="descname">zero_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.Tensor.zero_" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills this tensor with zeros.</p>
</dd></dl>

</dd></dl>

</div>
<span id="document-storage"></span><div class="section" id="torch-storage">
<h2>torch.Storage<a class="headerlink" href="#torch-storage" title="Permalink to this headline">¶</a></h2>
<p>A <code class="xref py py-class docutils literal"><span class="pre">torch.Storage</span></code> is a contiguous, one-dimensional array of a single
data type.</p>
<p>Every <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal"><span class="pre">torch.Tensor</span></code></a> has a corresponding storage of the same data type.</p>
<dl class="class">
<dt id="torch.FloatStorage">
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">FloatStorage</code><a class="headerlink" href="#torch.FloatStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="torch.FloatStorage.byte">
<code class="descname">byte</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.byte" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to byte type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.char">
<code class="descname">char</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.char" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to char type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this storage</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.copy_">
<code class="descname">copy_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.copy_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.cpu">
<code class="descname">cpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a CPU copy of this storage if it&#8217;s not already on the CPU</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.cuda">
<code class="descname">cuda</code><span class="sig-paren">(</span><em>device=None</em>, <em>async=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this object in CUDA memory.</p>
<p>If this object is already in CUDA memory and on the correct device, then
no copy is performed and the original object is returned.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>device</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The destination GPU id. Defaults to the current device.</li>
<li><strong>async</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If True and the source is in pinned memory, the copy will
be asynchronous with respect to the host. Otherwise, the
argument has no effect.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.data_ptr">
<code class="descname">data_ptr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.data_ptr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.double">
<code class="descname">double</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to double type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.element_size">
<code class="descname">element_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.element_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.fill_">
<code class="descname">fill_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.fill_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.float">
<code class="descname">float</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to float type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.from_buffer">
<code class="descname">from_buffer</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.from_buffer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.half">
<code class="descname">half</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to half type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.int">
<code class="descname">int</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.int" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to int type</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.FloatStorage.is_cuda">
<code class="descname">is_cuda</code><em class="property"> = False</em><a class="headerlink" href="#torch.FloatStorage.is_cuda" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.is_pinned">
<code class="descname">is_pinned</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.is_pinned" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.is_shared">
<code class="descname">is_shared</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.is_shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="torch.FloatStorage.is_sparse">
<code class="descname">is_sparse</code><em class="property"> = False</em><a class="headerlink" href="#torch.FloatStorage.is_sparse" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.long">
<code class="descname">long</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.long" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to long type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.new">
<code class="descname">new</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.new" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.pin_memory">
<code class="descname">pin_memory</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.pin_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the storage to pinned memory, if it&#8217;s not already pinned.</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.resize_">
<code class="descname">resize_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.resize_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.share_memory_">
<code class="descname">share_memory_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.share_memory_" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves the storage to shared memory.</p>
<p>This is a no-op for storages already in shared memory and for CUDA
storages, which do not need to be moved for sharing across processes.
Storages in shared memory cannot be resized.</p>
<p>Returns: self</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.short">
<code class="descname">short</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.short" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to short type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.size">
<code class="descname">size</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.tolist">
<code class="descname">tolist</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.tolist" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list containing the elements of this storage</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.type">
<code class="descname">type</code><span class="sig-paren">(</span><em>new_type=None</em>, <em>async=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this object to the specified type.</p>
<p>If this is already of the correct type, no copy is performed and the
original object is returned.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>new_type</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#type" title="(in Python v2.7)"><em>type</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/string.html#module-string" title="(in Python v2.7)"><em>string</em></a>) &#8211; The desired type</li>
<li><strong>async</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If True, and the source is in pinned memory and
destination is on the GPU or vice versa, the copy is
performed asynchronously with respect to the host.
Otherwise, the argument has no effect.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<span id="document-nn"></span><div class="section" id="module-torch.nn">
<span id="torch-nn"></span><h2>torch.nn<a class="headerlink" href="#module-torch.nn" title="Permalink to this headline">¶</a></h2>
<div class="section" id="parameters">
<h3>Parameters<a class="headerlink" href="#parameters" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.nn.Parameter">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Parameter</code><a class="headerlink" href="#torch.nn.Parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>A kind of Variable that is to be considered a module parameter.</p>
<p>Parameters are <a class="reference internal" href="index.html#torch.autograd.Variable" title="torch.autograd.Variable"><code class="xref py py-class docutils literal"><span class="pre">Variable</span></code></a> subclasses, that have a
very special property when used with <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal"><span class="pre">Module</span></code></a> s - when they&#8217;re
assigned as Module attributes they are automatically added to the list of
its parameters, and will appear e.g. in <a class="reference internal" href="#torch.nn.Module.parameters" title="torch.nn.Module.parameters"><code class="xref py py-meth docutils literal"><span class="pre">parameters()</span></code></a> iterator.
Assigning a Variable doesn&#8217;t have such effect. This is because one might
want to cache some temporary state, like last hidden state of the RNN, in
the model. If there was no such class as <a class="reference internal" href="#torch.nn.Parameter" title="torch.nn.Parameter"><code class="xref py py-class docutils literal"><span class="pre">Parameter</span></code></a>, these
temporaries would get registered too.</p>
<p>Another difference is that parameters can&#8217;t be volatile and that they
require gradient by default.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>data</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; parameter tensor.</li>
<li><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; if the parameter requires gradient. See
<a class="reference internal" href="index.html#excluding-subgraphs"><span class="std std-ref">Excluding subgraphs from backward</span></a> for more details.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="containers">
<h3>Containers<a class="headerlink" href="#containers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="module">
<h4><span class="hidden-section">Module</span><a class="headerlink" href="#module" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Module">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Module</code><a class="headerlink" href="#torch.nn.Module" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
       <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
       <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call .cuda(), etc.</p>
<dl class="method">
<dt id="torch.nn.Module.add_module">
<code class="descname">add_module</code><span class="sig-paren">(</span><em>name</em>, <em>module</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.add_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.children">
<code class="descname">children</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.cpu">
<code class="descname">cpu</code><span class="sig-paren">(</span><em>device_id=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.cuda">
<code class="descname">cuda</code><span class="sig-paren">(</span><em>device_id=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device_id</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; if specified, all parameters will be
copied to that device</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.double">
<code class="descname">double</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to double datatype.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.eval">
<code class="descname">eval</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on modules such as Dropout or BatchNorm.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.float">
<code class="descname">float</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to float datatype.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>*input</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overriden by all subclasses.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.half">
<code class="descname">half</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to half datatype.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.load_state_dict">
<code class="descname">load_state_dict</code><span class="sig-paren">(</span><em>state_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-attr docutils literal"><span class="pre">state_dict</span></code></a> into
this module and its descendants. The keys of <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-attr docutils literal"><span class="pre">state_dict</span></code></a> must
exactly match the keys returned by this module&#8217;s <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code class="xref py py-func docutils literal"><span class="pre">state_dict()</span></code></a>
function.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict" title="(in Python v2.7)"><em>dict</em></a>) &#8211; A dict containing parameters and
persistent buffers.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.modules">
<code class="descname">modules</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal"><span class="pre">l</span></code> will be returned only once.</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
<span class="go">0 -&gt; Sequential (</span>
<span class="go">  (0): Linear (2 -&gt; 2)</span>
<span class="go">  (1): Linear (2 -&gt; 2)</span>
<span class="go">)</span>
<span class="go">1 -&gt; Linear (2 -&gt; 2)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_children">
<code class="descname">named_children</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.named_children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv4&#39;</span><span class="p">,</span> <span class="s1">&#39;conv5&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_modules">
<code class="descname">named_modules</code><span class="sig-paren">(</span><em>memo=None</em>, <em>prefix=''</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.named_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal"><span class="pre">l</span></code> will be returned only once.</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
<span class="go">0 -&gt; (&#39;&#39;, Sequential (</span>
<span class="go">  (0): Linear (2 -&gt; 2)</span>
<span class="go">  (1): Linear (2 -&gt; 2)</span>
<span class="go">))</span>
<span class="go">1 -&gt; (&#39;0&#39;, Linear (2 -&gt; 2))</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.named_parameters">
<code class="descname">named_parameters</code><span class="sig-paren">(</span><em>memo=None</em>, <em>prefix=''</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.named_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself</p>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.parameters">
<code class="descname">parameters</code><span class="sig-paren">(</span><em>memo=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.FloatTensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_backward_hook">
<code class="descname">register_backward_hook</code><span class="sig-paren">(</span><em>hook</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.register_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">grad_output</span></code> may be tuples if the
module has multiple inputs or outputs. The hook should not modify its
arguments, but it can optionally return a new gradient with respect to
input that will be used in place of <code class="xref py py-attr docutils literal"><span class="pre">grad_input</span></code> in subsequent
computations.</p>
<p>This function returns a handle with a method <code class="docutils literal"><span class="pre">handle.remove()</span></code>
that removes the hook from the module.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_buffer">
<code class="descname">register_buffer</code><span class="sig-paren">(</span><em>name</em>, <em>tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.register_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a persistent buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm&#8217;s <code class="docutils literal"><span class="pre">running_mean</span></code>
is not a parameter, but is part of the persistent state.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_forward_hook">
<code class="descname">register_forward_hook</code><span class="sig-paren">(</span><em>hook</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.register_forward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time <a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-func docutils literal"><span class="pre">forward()</span></code></a> computes an output.
It should have the following signature:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The hook should not modify the input or output.
This function returns a handle with a method <code class="docutils literal"><span class="pre">handle.remove()</span></code>
that removes the hook from the module.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.register_parameter">
<code class="descname">register_parameter</code><span class="sig-paren">(</span><em>name</em>, <em>param</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.register_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.state_dict">
<code class="descname">state_dict</code><span class="sig-paren">(</span><em>destination=None</em>, <em>prefix=''</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.</p>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.train">
<code class="descname">train</code><span class="sig-paren">(</span><em>mode=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in training mode.</p>
<p>This has any effect only on modules such as Dropout or BatchNorm.</p>
</dd></dl>

<dl class="method">
<dt id="torch.nn.Module.zero_grad">
<code class="descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Module.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets gradients of all model parameters to zero.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="sequential">
<h4><span class="hidden-section">Sequential</span><a class="headerlink" href="#sequential" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Sequential">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Sequential</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Sequential" title="Permalink to this definition">¶</a></dt>
<dd><p>A sequential container.
Modules will be added to it in the order they are passed in the constructor.
Alternatively, an ordered dict of modules can also be passed in.</p>
<p>To make it easier to understand, given is a small example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># Example of using Sequential</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>

<span class="c1"># Example of using Sequential with OrderedDict</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
          <span class="p">(</span><span class="s1">&#39;conv1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;relu1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
          <span class="p">(</span><span class="s1">&#39;conv2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
          <span class="p">(</span><span class="s1">&#39;relu2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="p">]))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="modulelist">
<h4><span class="hidden-section">ModuleList</span><a class="headerlink" href="#modulelist" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ModuleList">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ModuleList</code><span class="sig-paren">(</span><em>modules=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ModuleList" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds submodules in a list.</p>
<p>ModuleList can be indexed like a regular Python list, but modules it contains
are properly registered, and will be visible by all Module methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>modules</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#list" title="(in Python v2.7)"><em>list</em></a><em>, </em><em>optional</em>) &#8211; a list of modules to add</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># ModuleList can act as an iterable, or be indexed using ints</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.nn.ModuleList.append">
<code class="descname">append</code><span class="sig-paren">(</span><em>module</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ModuleList.append" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends a given module at the end of the list.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>module</strong> (<a class="reference internal" href="index.html#torch.nn.Module" title="torch.nn.Module"><em>nn.Module</em></a>) &#8211; module to append</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ModuleList.extend">
<code class="descname">extend</code><span class="sig-paren">(</span><em>modules</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ModuleList.extend" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends modules from a Python list at the end.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>modules</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#list" title="(in Python v2.7)"><em>list</em></a>) &#8211; list of modules to append</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="parameterlist">
<h4><span class="hidden-section">ParameterList</span><a class="headerlink" href="#parameterlist" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ParameterList">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ParameterList</code><span class="sig-paren">(</span><em>parameters=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ParameterList" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds submodules in a list.</p>
<p>ParameterList can be indexed like a regular Python list, but parameters it contains
are properly registered, and will be visible by all Module methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>modules</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#list" title="(in Python v2.7)"><em>list</em></a><em>, </em><em>optional</em>) &#8211; a list of <code class="xref py py-class docutils literal"><span class="pre">nn.Parameter`</span></code> to add</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># ModuleList can act as an iterable, or be indexed using ints</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<dl class="method">
<dt id="torch.nn.ParameterList.append">
<code class="descname">append</code><span class="sig-paren">(</span><em>parameter</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ParameterList.append" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends a given parameter at the end of the list.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>parameter</strong> (<a class="reference internal" href="index.html#torch.nn.Parameter" title="torch.nn.Parameter"><em>nn.Parameter</em></a>) &#8211; parameter to append</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.nn.ParameterList.extend">
<code class="descname">extend</code><span class="sig-paren">(</span><em>parameters</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ParameterList.extend" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends parameters from a Python list at the end.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>parameters</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#list" title="(in Python v2.7)"><em>list</em></a>) &#8211; list of parameters to append</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="convolution-layers">
<h3>Convolution Layers<a class="headerlink" href="#convolution-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="conv1d">
<h4><span class="hidden-section">Conv1d</span><a class="headerlink" href="#conv1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Conv1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Conv1d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Conv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C_{in}, L)\)</span>
and output <span class="math">\((N, C_{out}, L_{out})\)</span> can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_{out_j})  = bias(C_{out_j})
               + \sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k)  \star input(N_i, k)
\end{array}\]</div>
<p>where <span class="math">\(\star\)</span> is the valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator</p>
<div class="line-block">
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</div>
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> controls the spacing between the kernel points. It is harder to describe,
but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> does.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">groups</span></code> controls the connections between inputs and outputs.</div>
<div class="line-block">
<div class="line">At groups=1, all inputs are convolved to all outputs.</div>
<div class="line">At groups=2, the operation becomes equivalent to having two conv layers
side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently concatenated.</div>
</div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Stride of the convolution</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to both sides of the input</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Spacing between kernel elements</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Number of blocked connections from input channels to output channels</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; If True, adds a learnable bias to the output</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C_{in}, L_{in})\)</span></li>
<li>Output: <span class="math">\((N, C_{out}, L_{out})\)</span> where
<span class="math">\(L_{out} = floor((L_{in}  + 2 * padding - dilation * (kernel\_size - 1) - 1) / stride + 1)\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable weights of the module of shape (out_channels, in_channels, kernel_size)</li>
<li><strong>bias</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable bias of the module of shape (out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv2d">
<h4><span class="hidden-section">Conv2d</span><a class="headerlink" href="#conv2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Conv2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Conv2d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C_{in}, H, W)\)</span>
and output <span class="math">\((N, C_{out}, H_{out}, W_{out})\)</span> can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_{out_j})  = bias(C_{out_j})
               + \sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k)  \star input(N_i, k)
\end{array}\]</div>
<p>where <span class="math">\(\star\)</span> is the valid 2D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator</p>
<div class="line-block">
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</div>
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> controls the spacing between the kernel points. It is harder to describe,
but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> does.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">groups</span></code> controls the connections between inputs and outputs.</div>
<div class="line-block">
<div class="line">At groups=1, all inputs are convolved to all outputs.</div>
<div class="line">At groups=2, the operation becomes equivalent to having two conv layers
side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently concatenated.</div>
</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of two ints &#8211; in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Stride of the convolution</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to both sides of the input</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Spacing between kernel elements</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Number of blocked connections from input channels to output channels</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; If True, adds a learnable bias to the output</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = floor((H_{in}  + 2 * padding[0] - dilation[0] * (kernel\_size[0] - 1) - 1) / stride[0] + 1)\)</span>
<span class="math">\(W_{out} = floor((W_{in}  + 2 * padding[1] - dilation[1] * (kernel\_size[1] - 1) - 1) / stride[1] + 1)\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable weights of the module of shape
(out_channels, in_channels, kernel_size[0], kernel_size[1])</li>
<li><strong>bias</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable bias of the module of shape (out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding and dilation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv3d">
<h4><span class="hidden-section">Conv3d</span><a class="headerlink" href="#conv3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Conv3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Conv3d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Conv3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D convolution over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C_{in}, D, H, W)\)</span>
and output <span class="math">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_{out_j})  = bias(C_{out_j})
               + \sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k)  \star input(N_i, k)
\end{array}\]</div>
<p>where <span class="math">\(\star\)</span> is the valid 3D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator</p>
<div class="line-block">
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</div>
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> controls the spacing between the kernel points. It is harder to describe,
but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> does.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">groups</span></code> controls the connections between inputs and outputs.</div>
<div class="line-block">
<div class="line">At groups=1, all inputs are convolved to all outputs.</div>
<div class="line">At groups=2, the operation becomes equivalent to having two conv layers
side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently concatenated.</div>
</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of three ints &#8211; in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Stride of the convolution</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to both sides of the input</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Spacing between kernel elements</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Number of blocked connections from input channels to output channels</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; If True, adds a learnable bias to the output</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(D_{out} = floor((D_{in}  + 2 * padding[0] - dilation[0] * (kernel\_size[0] - 1) - 1) / stride[0] + 1)\)</span>
<span class="math">\(H_{out} = floor((H_{in}  + 2 * padding[1] - dilation[1] * (kernel\_size[1] - 1) - 1) / stride[1] + 1)\)</span>
<span class="math">\(W_{out} = floor((W_{in}  + 2 * padding[2] - dilation[2] * (kernel\_size[2] - 1) - 1) / stride[2] + 1)\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable weights of the module of shape
(out_channels, in_channels, kernel_size[0], kernel_size[1], kernel_size[2])</li>
<li><strong>bias</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable bias of the module of shape (out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="convtranspose1d">
<h4><span class="hidden-section">ConvTranspose1d</span><a class="headerlink" href="#convtranspose1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ConvTranspose1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConvTranspose1d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ConvTranspose1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D transposed convolution operator over an input image
composed of several input planes.</p>
<p>This module can be seen as the gradient of Conv1d with respect to its input.
It is sometimes (but incorrectly) refered to as a deconvolutional operation.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Stride of the convolution</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to both sides of the input</li>
<li><strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to one side of the output</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Number of blocked connections from input channels to output channels</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; If True, adds a learnable bias to the output</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C_{in}, L_{in})\)</span></li>
<li>Output: <span class="math">\((N, C_{out}, L_{out})\)</span> where
<span class="math">\(L_{out} = (L_{in} - 1) * stride - 2 * padding + kernel\_size + output\_padding\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable weights of the module of shape
(in_channels, out_channels, kernel_size[0], kernel_size[1])</li>
<li><strong>bias</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable bias of the module of shape (out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="convtranspose2d">
<h4><span class="hidden-section">ConvTranspose2d</span><a class="headerlink" href="#convtranspose2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ConvTranspose2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConvTranspose2d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ConvTranspose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D transposed convolution operator over an input image
composed of several input planes.</p>
<p>This module can be seen as the gradient of Conv2d with respect to its input.
It is sometimes (but incorrectly) refered to as a deconvolutional operation.</p>
<div class="line-block">
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</div>
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points</div>
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">output_padding</span></code> is non-zero, then the output is implicitly zero-padded on one side
for <code class="xref py py-attr docutils literal"><span class="pre">output_padding</span></code> number of points</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> controls the spacing between the kernel points. It is harder to describe,
but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> does.</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">groups</span></code> controls the connections between inputs and outputs.</div>
<div class="line-block">
<div class="line">At groups=1, all inputs are convolved to all outputs.</div>
<div class="line">At groups=2, the operation becomes equivalent to having two conv layers
side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently concatenated.</div>
</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">output_padding</span></code>
can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of two ints &#8211; in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Stride of the convolution</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to both sides of the input</li>
<li><strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to one side of the output</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Number of blocked connections from input channels to output channels</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; If True, adds a learnable bias to the output</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Spacing between kernel elements</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = (H_{in} - 1) * stride[0] - 2 * padding[0] + kernel\_size[0] + output\_padding[0]\)</span>
<span class="math">\(W_{out} = (W_{in} - 1) * stride[1] - 2 * padding[1] + kernel\_size[1] + output\_padding[1]\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable weights of the module of shape
(in_channels, out_channels, kernel_size[0], kernel_size[1])</li>
<li><strong>bias</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable bias of the module of shape (out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># exact output size can be also specified as an argument</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">upsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span> <span class="o">=</span> <span class="n">downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 16, 6, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">upsample</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([1, 16, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="convtranspose3d">
<h4><span class="hidden-section">ConvTranspose3d</span><a class="headerlink" href="#convtranspose3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ConvTranspose3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ConvTranspose3d</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>kernel_size</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>bias=True</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ConvTranspose3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D transposed convolution operator over an input image composed of several input
planes.
The transposed convolution operator multiplies each input value element-wise by a learnable kernel,
and sums over the outputs from all input feature planes.</p>
<p><strong>This module can be seen as the exact reverse of Conv3d</strong>.
It is sometimes (but incorrectly) refered to as a deconvolutional operation.</p>
<div class="line-block">
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</div>
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points</div>
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">output_padding</span></code> is non-zero, then the output is implicitly zero-padded on one side
for <code class="xref py py-attr docutils literal"><span class="pre">output_padding</span></code> number of points</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">groups</span></code> controls the connections between inputs and outputs.</div>
<div class="line-block">
<div class="line">At groups=1, all inputs are convolved to all outputs.</div>
<div class="line">At groups=2, the operation becomes equivalent to having two conv layers
side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently concatenated.</div>
</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">output_padding</span></code>
can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of three ints &#8211; in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the width dimension and the third <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>,
and not a full <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.
It is up to the user to add proper padding.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the convolving kernel</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Stride of the convolution</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to both sides of the input</li>
<li><strong>output_padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Zero-padding added to one side of the output</li>
<li><strong>groups</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; Number of blocked connections from input channels to output channels</li>
<li><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; If True, adds a learnable bias to the output</li>
<li><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; Spacing between kernel elements</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C_{in}, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C_{out}, D_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(D_{out} = (D_{in} - 1) * stride[0] - 2 * padding[0] + kernel\_size[0] + output\_padding[0]\)</span>
<span class="math">\(H_{out} = (H_{in} - 1) * stride[1] - 2 * padding[1] + kernel\_size[1] + output\_padding[1]\)</span>
<span class="math">\(W_{out} = (W_{in} - 1) * stride[2] - 2 * padding[2] + kernel\_size[2] + output\_padding[2]\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable weights of the module of shape
(in_channels, out_channels, kernel_size[0], kernel_size[1], kernel_size[2])</li>
<li><strong>bias</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable bias of the module of shape (out_channels)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># non-square kernels and unequal stride and with padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="pooling-layers">
<h3>Pooling Layers<a class="headerlink" href="#pooling-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="maxpool1d">
<h4><span class="hidden-section">MaxPool1d</span><a class="headerlink" href="#maxpool1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MaxPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxPool1d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>return_indices=False</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MaxPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D max pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, L)\)</span>
and output <span class="math">\((N, C, L_{out})\)</span> can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_j, k)  = \max_{{m}=0}^{{kernel\_size}-1} input(N_i, C_j, stride * k + m)
\end{array}\]</div>
<div class="line-block">
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> controls the spacing between the kernel points. It is harder to describe,
but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> does.</div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window to take a max over</li>
<li><strong>stride</strong> &#8211; the stride of the window. Default value is <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> &#8211; implicit zero padding to be added on both sides</li>
<li><strong>dilation</strong> &#8211; a parameter that controls the stride of elements in the window</li>
<li><strong>return_indices</strong> &#8211; if True, will return the max indices along with the outputs.
Useful when Unpooling later</li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, L_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, L_{out})\)</span> where
<span class="math">\(L_{out} = floor((L_{in}  + 2 * padding - dilation * (kernel\_size - 1) - 1) / stride + 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxpool2d">
<h4><span class="hidden-section">MaxPool2d</span><a class="headerlink" href="#maxpool2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MaxPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxPool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>return_indices=False</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D max pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, H, W)\)</span>,
output <span class="math">\((N, C, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code> <span class="math">\((kH, kW)\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_j, h, w)  = \max_{{m}=0}^{kH-1} \max_{{n}=0}^{kW-1}
                       input(N_i, C_j, stride[0] * h + m, stride[1] * w + n)
\end{array}\]</div>
<div class="line-block">
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> controls the spacing between the kernel points. It is harder to describe,
but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> does.</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of two ints &#8211; in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window to take a max over</li>
<li><strong>stride</strong> &#8211; the stride of the window. Default value is <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> &#8211; implicit zero padding to be added on both sides</li>
<li><strong>dilation</strong> &#8211; a parameter that controls the stride of elements in the window</li>
<li><strong>return_indices</strong> &#8211; if True, will return the max indices along with the outputs.
Useful when Unpooling later</li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = floor((H_{in}  + 2 * padding[0] - dilation[0] * (kernel\_size[0] - 1) - 1) / stride[0] + 1)\)</span>
<span class="math">\(W_{out} = floor((W_{in}  + 2 * padding[1] - dilation[1] * (kernel\_size[1] - 1) - 1) / stride[1] + 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxpool3d">
<h4><span class="hidden-section">MaxPool3d</span><a class="headerlink" href="#maxpool3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MaxPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxPool3d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>return_indices=False</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MaxPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D max pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, D, H, W)\)</span>,
output <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code> <span class="math">\((kD, kH, kW)\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_j, d, h, w)  = \max_{{k}=0}^{kD-1} \max_{{m}=0}^{kH-1} \max_{{n}=0}^{kW-1}
                 input(N_i, C_j, stride[0] * k + d, stride[1] * h + m, stride[2] * w + n)
\end{array}\]</div>
<div class="line-block">
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points</div>
<div class="line"><code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> controls the spacing between the kernel points. It is harder to describe,
but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> does.</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of three ints &#8211; in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the width dimension and the third <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window to take a max over</li>
<li><strong>stride</strong> &#8211; the stride of the window. Default value is <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> &#8211; implicit zero padding to be added on both sides</li>
<li><strong>dilation</strong> &#8211; a parameter that controls the stride of elements in the window</li>
<li><strong>return_indices</strong> &#8211; if True, will return the max indices along with the outputs.
Useful when Unpooling later</li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(D_{out} = floor((D_{in}  + 2 * padding[0] - dilation[0] * (kernel\_size[0] - 1) - 1) / stride[0] + 1)\)</span>
<span class="math">\(H_{out} = floor((H_{in}  + 2 * padding[1] - dilation[1] * (kernel\_size[1] - 1) - 1) / stride[1] + 1)\)</span>
<span class="math">\(W_{out} = floor((W_{in}  + 2 * padding[2] - dilation[2] * (kernel\_size[2] - 1) - 1) / stride[2] + 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span><span class="mi">44</span><span class="p">,</span> <span class="mi">31</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxunpool1d">
<h4><span class="hidden-section">MaxUnpool1d</span><a class="headerlink" href="#maxunpool1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MaxUnpool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxUnpool1d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MaxUnpool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool1d</span></code></a>.</p>
<p><a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool1d</span></code></a> is not fully invertible, since the non-maximal values are lost.</p>
<p><a class="reference internal" href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code class="xref py py-class docutils literal"><span class="pre">MaxUnpool1d</span></code></a> takes in as input the output of <a class="reference internal" href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool1d</span></code></a>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><cite>MaxPool1d</cite> can map several input sizes to the same output sizes.
Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <cite>output_size</cite> in the forward call.
See the Inputs and Example below.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the max pooling window.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Stride of the max pooling window.
It is set to <code class="docutils literal"><span class="pre">kernel_size</span></code> by default.</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Padding that was added to the input</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs:</dt>
<dd><ul class="first last simple">
<li><cite>input</cite>: the input Tensor to invert</li>
<li><cite>indices</cite>: the indices given out by <cite>MaxPool1d</cite></li>
<li><cite>output_size</cite> (optional) : a <cite>torch.Size</cite> that specifies the targeted output size</li>
</ul>
</dd>
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out})\)</span> where
<span class="math">\(H_{out} = (H_{in} - 1) * stride[0] - 2 * padding[0] + kernel\_size[0]\)</span>
or as given by <code class="xref py py-attr docutils literal"><span class="pre">output_size</span></code> in the call operator</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool1d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,.,.) =</span>
<span class="go">   0   2   0   4   0   6   0   8</span>
<span class="go">[torch.FloatTensor of size 1x1x8]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Example showcasing the use of output_size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,.,.) =</span>
<span class="go">   0   2   0   4   0   6   0   8   0</span>
<span class="go">[torch.FloatTensor of size 1x1x9]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,.,.) =</span>
<span class="go">   0   2   0   4   0   6   0   8</span>
<span class="go">[torch.FloatTensor of size 1x1x8]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxunpool2d">
<h4><span class="hidden-section">MaxUnpool2d</span><a class="headerlink" href="#maxunpool2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MaxUnpool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxUnpool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MaxUnpool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool2d</span></code></a>.</p>
<p><a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool2d</span></code></a> is not fully invertible, since the non-maximal values are lost.</p>
<p><a class="reference internal" href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code class="xref py py-class docutils literal"><span class="pre">MaxUnpool2d</span></code></a> takes in as input the output of <a class="reference internal" href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool2d</span></code></a>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><cite>MaxPool2d</cite> can map several input sizes to the same output sizes.
Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <cite>output_size</cite> in the forward call.
See the Inputs and Example below.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the max pooling window.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Stride of the max pooling window.
It is set to <code class="docutils literal"><span class="pre">kernel_size</span></code> by default.</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Padding that was added to the input</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs:</dt>
<dd><ul class="first last simple">
<li><cite>input</cite>: the input Tensor to invert</li>
<li><cite>indices</cite>: the indices given out by <cite>MaxPool2d</cite></li>
<li><cite>output_size</cite> (optional) : a <cite>torch.Size</cite> that specifies the targeted output size</li>
</ul>
</dd>
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = (H_{in} - 1) * stride[0] -2 * padding[0] + kernel\_size[0]\)</span>
<span class="math">\(W_{out} = (W_{in} - 1) * stride[1] -2 * padding[1] + kernel\_size[1]\)</span>
or as given by <code class="xref py py-attr docutils literal"><span class="pre">output_size</span></code> in the call operator</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[[[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">],</span>
<span class="gp">... </span>                                 <span class="p">[</span> <span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">8</span><span class="p">],</span>
<span class="gp">... </span>                                 <span class="p">[</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span>
<span class="gp">... </span>                                 <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">]]]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">   0   0   0   0</span>
<span class="go">   0   6   0   8</span>
<span class="go">   0   0   0   0</span>
<span class="go">   0  14   0  16</span>
<span class="go">[torch.FloatTensor of size 1x1x4x4]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># specify a different output size than input size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]))</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">   0   0   0   0   0</span>
<span class="go">   6   0   8   0   0</span>
<span class="go">   0   0   0  14   0</span>
<span class="go">  16   0   0   0   0</span>
<span class="go">   0   0   0   0   0</span>
<span class="go">[torch.FloatTensor of size 1x1x5x5]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="maxunpool3d">
<h4><span class="hidden-section">MaxUnpool3d</span><a class="headerlink" href="#maxunpool3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MaxUnpool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MaxUnpool3d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MaxUnpool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool3d</span></code></a>.</p>
<p><a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool3d</span></code></a> is not fully invertible, since the non-maximal values are lost.
<a class="reference internal" href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code class="xref py py-class docutils literal"><span class="pre">MaxUnpool3d</span></code></a> takes in as input the output of <a class="reference internal" href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal"><span class="pre">MaxPool3d</span></code></a>
including the indices of the maximal values and computes a partial inverse
in which all non-maximal values are set to zero.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><cite>MaxPool3d</cite> can map several input sizes to the same output sizes.
Hence, the inversion process can get ambiguous.
To accommodate this, you can provide the needed output size
as an additional argument <cite>output_size</cite> in the forward call.
See the Inputs section below.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Size of the max pooling window.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Stride of the max pooling window.
It is set to <code class="docutils literal"><span class="pre">kernel_size</span></code> by default.</li>
<li><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; Padding that was added to the input</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs:</dt>
<dd><ul class="first last simple">
<li><cite>input</cite>: the input Tensor to invert</li>
<li><cite>indices</cite>: the indices given out by <cite>MaxPool3d</cite></li>
<li><cite>output_size</cite> (optional) : a <cite>torch.Size</cite> that specifies the targeted output size</li>
</ul>
</dd>
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(D_{out} = (D_{in} - 1) * stride[0] - 2 * padding[0] + kernel\_size[0]\)</span>
<span class="math">\(H_{out} = (H_{in} - 1) * stride[1] - 2 * padding[1] + kernel\_size[1]\)</span>
<span class="math">\(W_{out} = (W_{in} - 1) * stride[2] - 2 * padding[2] + kernel\_size[2]\)</span>
or as given by <code class="xref py py-attr docutils literal"><span class="pre">output_size</span></code> in the call operator</li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">15</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpooled_output</span> <span class="o">=</span> <span class="n">unpool</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpooled_output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="go">torch.Size([20, 16, 51, 33, 15])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avgpool1d">
<h4><span class="hidden-section">AvgPool1d</span><a class="headerlink" href="#avgpool1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AvgPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AvgPool1d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AvgPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D average pooling over an input signal composed of several
input planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, L)\)</span>,
output <span class="math">\((N, C, L_{out})\)</span> and <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code> <span class="math">\(k\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_j, l)  = 1 / k * \sum_{{m}=0}^{k}
                       input(N_i, C_j, stride * l + m)
\end{array}\]</div>
<div class="line-block">
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> can each be
an <code class="docutils literal"><span class="pre">int</span></code> or a one-element tuple.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window</li>
<li><strong>stride</strong> &#8211; the stride of the window. Default value is <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> &#8211; implicit zero padding to be added on both sides</li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
<li><strong>count_include_pad</strong> &#8211; when True, will include the zero-padding in the averaging calculation</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, L_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, L_{out})\)</span> where
<span class="math">\(L_{out} = floor((L_{in}  + 2 * padding - kernel\_size) / stride + 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool with window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]]])))</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,.,.) =</span>
<span class="go">  2  4  6</span>
<span class="go">[torch.FloatTensor of size 1x1x3]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avgpool2d">
<h4><span class="hidden-section">AvgPool2d</span><a class="headerlink" href="#avgpool2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AvgPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AvgPool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D average pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, H, W)\)</span>,
output <span class="math">\((N, C, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code> <span class="math">\((kH, kW)\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_j, h, w)  = 1 / (kH * kW) * \sum_{{m}=0}^{kH-1} \sum_{{n}=0}^{kW-1}
                       input(N_i, C_j, stride[0] * h + m, stride[1] * w + n)
\end{array}\]</div>
<div class="line-block">
<div class="line">If <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> number of points</div>
</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">padding</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of two ints &#8211; in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window</li>
<li><strong>stride</strong> &#8211; the stride of the window. Default value is <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> &#8211; implicit zero padding to be added on both sides</li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
<li><strong>count_include_pad</strong> &#8211; when True, will include the zero-padding in the averaging calculation</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = floor((H_{in}  + 2 * padding[0] - kernel\_size[0]) / stride[0] + 1)\)</span>
<span class="math">\(W_{out} = floor((W_{in}  + 2 * padding[1] - kernel\_size[1]) / stride[1] + 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avgpool3d">
<h4><span class="hidden-section">AvgPool3d</span><a class="headerlink" href="#avgpool3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AvgPool3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AvgPool3d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>stride=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AvgPool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D average pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math">\((N, C, D, H, W)\)</span>,
output <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> and <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code> <span class="math">\((kD, kH, kW)\)</span>
can be precisely described as:</p>
<div class="math">
\[\begin{array}{ll}
out(N_i, C_j, d, h, w)  = 1 / (kD * kH * kW) * \sum_{{k}=0}^{kD-1} \sum_{{m}=0}^{kH-1} \sum_{{n}=0}^{kW-1}
                       input(N_i, C_j, stride[0] * d + k, stride[1] * h + m, stride[2] * w + n)
\end{array}\]</div>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of three ints &#8211; in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the width dimension and the third <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window</li>
<li><strong>stride</strong> &#8211; the stride of the window. Default value is <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D_{in}, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, D_{out}, H_{out}, W_{out})\)</span> where
<span class="math">\(D_{out} = floor((D_{in}  - kernel\_size[0]) / stride[0] + 1)\)</span>
<span class="math">\(H_{out} = floor((H_{in}  - kernel\_size[1]) / stride[1] + 1)\)</span>
<span class="math">\(W_{out} = floor((W_{in}  - kernel\_size[2]) / stride[2] + 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span><span class="mi">44</span><span class="p">,</span> <span class="mi">31</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="fractionalmaxpool2d">
<h4><span class="hidden-section">FractionalMaxPool2d</span><a class="headerlink" href="#fractionalmaxpool2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.FractionalMaxPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">FractionalMaxPool2d</code><span class="sig-paren">(</span><em>kernel_size</em>, <em>output_size=None</em>, <em>output_ratio=None</em>, <em>return_indices=False</em>, <em>_random_samples=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.FractionalMaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D fractional max pooling over an input signal composed of several input planes.</p>
<p>Fractiona MaxPooling is described in detail in the paper <a class="reference external" href="http://arxiv.org/abs/1412.6071">Fractional MaxPooling</a> by Ben Graham</p>
<p>The max-pooling operation is applied in kHxkW regions by a stochastic
step size determined by the target output size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window to take a max over.
Can be a single number k (for a square kernel of k x k) or a tuple (kh x kw)</li>
<li><strong>output_size</strong> &#8211; the target output size of the image of the form oH x oW.
Can be a tuple (oH, oW) or a single number oH for a square image oH x oH</li>
<li><strong>output_ratio</strong> &#8211; If one wants to have an output size as a ratio of the input size, this option can be given.
This has to be a number or tuple in the range (0, 1)</li>
<li><strong>return_indices</strong> &#8211; if True, will return the indices along with the outputs.
Useful to pass to nn.MaxUnpool2d . Default: False</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, and target output size 13x12</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FractionalMaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window and target output size being half of input image size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FractionalMaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_ratio</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="lppool2d">
<h4><span class="hidden-section">LPPool2d</span><a class="headerlink" href="#lppool2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.LPPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LPPool2d</code><span class="sig-paren">(</span><em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.LPPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D power-average pooling over an input signal composed of several input
planes.</p>
<p>On each window, the function computed is: <span class="math">\(f(X) = pow(sum(pow(X, p)), 1/p)\)</span></p>
<blockquote>
<div><ul class="simple">
<li>At p = infinity, one gets Max Pooling</li>
<li>At p = 1, one gets Average Pooling</li>
</ul>
</div></blockquote>
<p>The parameters <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal"><span class="pre">stride</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li>a single <code class="docutils literal"><span class="pre">int</span></code> &#8211; in which case the same value is used for the height and width dimension</li>
<li>a <code class="docutils literal"><span class="pre">tuple</span></code> of two ints &#8211; in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window</li>
<li><strong>stride</strong> &#8211; the stride of the window. Default value is <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code></li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = floor((H_{in}  + 2 * padding[0] - dilation[0] * (kernel\_size[0] - 1) - 1) / stride[0] + 1)\)</span>
<span class="math">\(W_{out} = floor((W_{in}  + 2 * padding[1] - dilation[1] * (kernel\_size[1] - 1) - 1) / stride[1] + 1)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># power-2 pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of non-square window of power 1.2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool2d</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivemaxpool1d">
<h4><span class="hidden-section">AdaptiveMaxPool1d</span><a class="headerlink" href="#adaptivemaxpool1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AdaptiveMaxPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveMaxPool1d</code><span class="sig-paren">(</span><em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AdaptiveMaxPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</p>
<p>The output size is H, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> &#8211; the target output size H</li>
<li><strong>return_indices</strong> &#8211; if True, will return the indices along with the outputs.
Useful to pass to nn.MaxUnpool2d . Default: False</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool1d</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptivemaxpool2d">
<h4><span class="hidden-section">AdaptiveMaxPool2d</span><a class="headerlink" href="#adaptivemaxpool2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AdaptiveMaxPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveMaxPool2d</code><span class="sig-paren">(</span><em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AdaptiveMaxPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</p>
<p>The output is of size H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> &#8211; the target output size of the image of the form H x W.
Can be a tuple (H, W) or a single number H for a square image H x H</li>
<li><strong>return_indices</strong> &#8211; if True, will return the indices along with the outputs.
Useful to pass to nn.MaxUnpool2d . Default: False</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7 (square)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptiveavgpool1d">
<h4><span class="hidden-section">AdaptiveAvgPool1d</span><a class="headerlink" href="#adaptiveavgpool1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AdaptiveAvgPool1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveAvgPool1d</code><span class="sig-paren">(</span><em>output_size</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AdaptiveAvgPool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</p>
<p>The output size is H, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> &#8211; the target output size H</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="adaptiveavgpool2d">
<h4><span class="hidden-section">AdaptiveAvgPool2d</span><a class="headerlink" href="#adaptiveavgpool2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.AdaptiveAvgPool2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">AdaptiveAvgPool2d</code><span class="sig-paren">(</span><em>output_size</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.AdaptiveAvgPool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</p>
<p>The output is of size H x W, for any input size.
The number of output features is equal to the number of input planes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> &#8211; the target output size of the image of the form H x W.
Can be a tuple (H, W) or a single number H for a square image H x H</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 5x7</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># target output size of 7x7 (square)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="non-linear-activations">
<h3>Non-linear Activations<a class="headerlink" href="#non-linear-activations" title="Permalink to this headline">¶</a></h3>
<div class="section" id="relu">
<h4><span class="hidden-section">ReLU</span><a class="headerlink" href="#relu" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReLU</code><span class="sig-paren">(</span><em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the rectified linear unit function element-wise <span class="math">\({ReLU}(x)= max(0, x)\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>inplace</strong> &#8211; can optionally do the operation in-place</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="relu6">
<h4><span class="hidden-section">ReLU6</span><a class="headerlink" href="#relu6" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ReLU6">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ReLU6</code><span class="sig-paren">(</span><em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ReLU6" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function <span class="math">\({ReLU6}(x) = min(max(0,x), 6)\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>inplace</strong> &#8211; can optionally do the operation in-place</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="elu">
<h4><span class="hidden-section">ELU</span><a class="headerlink" href="#elu" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.ELU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">ELU</code><span class="sig-paren">(</span><em>alpha=1.0</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.ELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, <span class="math">\(f(x) = max(0,x) + min(0, alpha * (exp(x) - 1))\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>alpha</strong> &#8211; the alpha value for the ELU formulation</li>
<li><strong>inplace</strong> &#8211; can optionally do the operation in-place</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="prelu">
<h4><span class="hidden-section">PReLU</span><a class="headerlink" href="#prelu" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.PReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PReLU</code><span class="sig-paren">(</span><em>num_parameters=1</em>, <em>init=0.25</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.PReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise the function <span class="math">\(PReLU(x) = max(0,x) + a * min(0,x)\)</span>
Here &#8220;a&#8221; is a learnable parameter.
When called without arguments, nn.PReLU() uses a single parameter &#8220;a&#8221;
across all input channels. If called with nn.PReLU(nChannels), a separate
&#8220;a&#8221; is used for each input channel.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">weight decay should not be used when learning &#8220;a&#8221; for good performance.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_parameters</strong> &#8211; number of &#8220;a&#8221; to learn. Default: 1</li>
<li><strong>init</strong> &#8211; the initial value of &#8220;a&#8221;. Default: 0.25</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="leakyrelu">
<h4><span class="hidden-section">LeakyReLU</span><a class="headerlink" href="#leakyrelu" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.LeakyReLU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LeakyReLU</code><span class="sig-paren">(</span><em>negative_slope=0.01</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.LeakyReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, <span class="math">\(f(x) = max(0, x) + {negative\_slope} * min(0, x)\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>negative_slope</strong> &#8211; Controls the angle of the negative slope. Default: 1e-2</li>
<li><strong>inplace</strong> &#8211; can optionally do the operation in-place</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="threshold">
<h4><span class="hidden-section">Threshold</span><a class="headerlink" href="#threshold" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Threshold">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Threshold</code><span class="sig-paren">(</span><em>threshold</em>, <em>value</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Thresholds each element of the input Tensor</p>
<p>Threshold is defined as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span>  <span class="n">x</span>        <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="n">threshold</span>
     <span class="n">value</span>    <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span>  <span class="n">threshold</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>threshold</strong> &#8211; The value to threshold at</li>
<li><strong>value</strong> &#8211; The value to replace with</li>
<li><strong>inplace</strong> &#8211; can optionally do the operation in-place</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Threshold</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="hardtanh">
<h4><span class="hidden-section">Hardtanh</span><a class="headerlink" href="#hardtanh" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Hardtanh">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Hardtanh</code><span class="sig-paren">(</span><em>min_value=-1</em>, <em>max_value=1</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Hardtanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardTanh function element-wise</p>
<p>HardTanh is defined as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="k">if</span> <span class="n">x</span>  <span class="o">&gt;</span>  <span class="mi">1</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="k">if</span> <span class="n">x</span>  <span class="o">&lt;</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span>  <span class="n">x</span><span class="p">,</span>  <span class="n">otherwise</span>
</pre></div>
</div>
<p>The range of the linear region <span class="math">\([-1, 1]\)</span> can be adjusted</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>min_value</strong> &#8211; minimum value of the linear region range</li>
<li><strong>max_value</strong> &#8211; maximum value of the linear region range</li>
<li><strong>inplace</strong> &#8211; can optionally do the operation in-place</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">HardTanh</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="sigmoid">
<h4><span class="hidden-section">Sigmoid</span><a class="headerlink" href="#sigmoid" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Sigmoid">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Sigmoid</code><a class="headerlink" href="#torch.nn.Sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function <span class="math">\(f(x) = 1 / ( 1 + exp(-x))\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="tanh">
<h4><span class="hidden-section">Tanh</span><a class="headerlink" href="#tanh" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Tanh">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Tanh</code><a class="headerlink" href="#torch.nn.Tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, <span class="math">\(f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="logsigmoid">
<h4><span class="hidden-section">LogSigmoid</span><a class="headerlink" href="#logsigmoid" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.LogSigmoid">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LogSigmoid</code><a class="headerlink" href="#torch.nn.LogSigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise <span class="math">\(LogSigmoid(x) = log( 1 / (1 + exp(-x_i)))\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softplus">
<h4><span class="hidden-section">Softplus</span><a class="headerlink" href="#softplus" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Softplus">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softplus</code><span class="sig-paren">(</span><em>beta=1</em>, <em>threshold=20</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Softplus" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise <span class="math">\(f(x) = 1/beta * log(1 + exp(beta * x_i))\)</span></p>
<p>SoftPlus is a smooth approximation to the ReLU function and can be used
to constrain the output of a machine to always be positive.</p>
<p>For numerical stability the implementation reverts to the linear function
for inputs above a certain value.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>beta</strong> &#8211; the beta value for the Softplus formulation. Default: 1</li>
<li><strong>threshold</strong> &#8211; values above this revert to a linear function. Default: 20</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softshrink">
<h4><span class="hidden-section">Softshrink</span><a class="headerlink" href="#softshrink" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Softshrink">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softshrink</code><span class="sig-paren">(</span><em>lambd=0.5</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Softshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the soft shrinkage function elementwise</p>
<p>SoftShrinkage operator is defined as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">x</span><span class="o">-</span><span class="k">lambda</span><span class="p">,</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="k">lambda</span> <span class="o">&gt;</span>  <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">x</span><span class="o">+</span><span class="k">lambda</span><span class="p">,</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="o">-</span><span class="k">lambda</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">otherwise</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>lambd</strong> &#8211; the lambda value for the Softshrink formulation. Default: 0.5</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softshrink</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softsign">
<h4><span class="hidden-section">Softsign</span><a class="headerlink" href="#softsign" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Softsign">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softsign</code><a class="headerlink" href="#torch.nn.Softsign" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, the function <span class="math">\(f(x) = x / (1 + |x|)\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="tanhshrink">
<h4><span class="hidden-section">Tanhshrink</span><a class="headerlink" href="#tanhshrink" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Tanhshrink">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Tanhshrink</code><a class="headerlink" href="#torch.nn.Tanhshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, <span class="math">\(Tanhshrink(x) = x - Tanh(x)\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, *)\)</span> where <cite>*</cite> means, any number of additional dimensions</li>
<li>Output: <span class="math">\((N, *)\)</span>, same shape as the input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanhshrink</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softmin">
<h4><span class="hidden-section">Softmin</span><a class="headerlink" href="#softmin" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Softmin">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softmin</code><a class="headerlink" href="#torch.nn.Softmin" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Softmin function to an n-dimensional input Tensor
rescaling them so that the elements of the n-dimensional output Tensor
lie in the range <cite>(0, 1)</cite> and sum to 1</p>
<p><span class="math">\(f(x) = exp(-x_i - {shift}) / sum_j exp(-x_j - {shift})\)</span></p>
<p>where <span class="math">\({shift} = max_i - x_i\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, L)\)</span></li>
<li>Output: <span class="math">\((N, L)\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a Tensor of the same dimension and shape as the input, with
values in the range [0, 1]</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmin</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="softmax">
<h4><span class="hidden-section">Softmax</span><a class="headerlink" href="#softmax" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Softmax">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Softmax</code><a class="headerlink" href="#torch.nn.Softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Softmax function to an n-dimensional input Tensor
rescaling them so that the elements of the n-dimensional output Tensor
lie in the range (0,1) and sum to 1</p>
<p>Softmax is defined as <span class="math">\(f_i(x) = exp(x_i - shift) / sum_j exp(x_j - shift)\)</span>
where <cite>shift = max_i x_i</cite></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, L)\)</span></li>
<li>Output: <span class="math">\((N, L)\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a Tensor of the same dimension and shape as the input with
values in the range [0, 1]</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This module doesn&#8217;t work directly with NLLLoss,
which expects the Log to be computed between the Softmax and itself.
Use Logsoftmax instead (it&#8217;s faster).</p>
</div>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="logsoftmax">
<h4><span class="hidden-section">LogSoftmax</span><a class="headerlink" href="#logsoftmax" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.LogSoftmax">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LogSoftmax</code><a class="headerlink" href="#torch.nn.LogSoftmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Log(Softmax(x)) function to an n-dimensional input Tensor.
The LogSoftmax formulation can be simplified as</p>
<p><span class="math">\(f_i(x) = log(1 / a * exp(x_i))\)</span> where <span class="math">\(a = sum_j exp(x_j)\)</span></p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, L)\)</span></li>
<li>Output: <span class="math">\((N, L)\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">a Tensor of the same dimension and shape as the input with
values in the range [-inf, 0)</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="normalization-layers">
<h3>Normalization layers<a class="headerlink" href="#normalization-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="batchnorm1d">
<h4><span class="hidden-section">BatchNorm1d</span><a class="headerlink" href="#batchnorm1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.BatchNorm1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BatchNorm1d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.BatchNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization over a 2d or 3d input that is seen as a mini-batch.</p>
<div class="math">
\[y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size C (where C is the input size).</p>
<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1.</p>
<p>During evaluation, this running mean/variance is used for normalization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> &#8211; num_features from an expected input of size <cite>batch_size x num_features [x width]</cite></li>
<li><strong>eps</strong> &#8211; a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> &#8211; the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> &#8211; a boolean value that when set to true, gives the layer learnable affine parameters.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C)\)</span> or <span class="math">\((N, C, L)\)</span></li>
<li>Output: <span class="math">\((N, C)\)</span> or <span class="math">\((N, C, L)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="batchnorm2d">
<h4><span class="hidden-section">BatchNorm2d</span><a class="headerlink" href="#batchnorm2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.BatchNorm2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BatchNorm2d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.BatchNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization over a 4d input that is seen as a mini-batch of 3d inputs</p>
<div class="math">
\[y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size C (where C is the input size).</p>
<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1.</p>
<p>During evaluation, this running mean/variance is used for normalization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> &#8211; num_features from an expected input of size batch_size x num_features x height x width</li>
<li><strong>eps</strong> &#8211; a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> &#8211; the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> &#8211; a boolean value that when set to true, gives the layer learnable affine parameters.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="batchnorm3d">
<h4><span class="hidden-section">BatchNorm3d</span><a class="headerlink" href="#batchnorm3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.BatchNorm3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BatchNorm3d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.BatchNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization over a 5d input that is seen as a mini-batch of 4d inputs</p>
<div class="math">
\[y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size C (where C is the input size).</p>
<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1.</p>
<p>During evaluation, this running mean/variance is used for normalization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> &#8211; num_features from an expected input of size batch_size x num_features x depth x height x width</li>
<li><strong>eps</strong> &#8211; a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> &#8211; the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> &#8211; a boolean value that when set to true, gives the layer learnable affine parameters.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, D, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="instancenorm1d">
<h4><span class="hidden-section">InstanceNorm1d</span><a class="headerlink" href="#instancenorm1d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.InstanceNorm1d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">InstanceNorm1d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.InstanceNorm1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization over a 2d or 3d input that is seen as a mini-batch.</p>
<div class="math">
\[y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. Gamma and beta are learnable parameter vectors
of size C (where C is the input size).</p>
<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1.</p>
<p>At evaluation time (<cite>.eval()</cite>), the default behaviour of the InstanceNorm module stays the same
i.e. running mean/variance is NOT used for normalization. One can force using stored
mean and variance with <cite>.train(False)</cite> method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> &#8211; num_features from an expected input of size <cite>batch_size x num_features x width</cite></li>
<li><strong>eps</strong> &#8211; a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> &#8211; the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> &#8211; a boolean value that when set to true, gives the layer learnable affine parameters.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, L)\)</span></li>
<li>Output: <span class="math">\((N, C, L)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="instancenorm2d">
<h4><span class="hidden-section">InstanceNorm2d</span><a class="headerlink" href="#instancenorm2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.InstanceNorm2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">InstanceNorm2d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.InstanceNorm2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization over a 4d input that is seen as a mini-batch of 3d inputs
.. math:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> \<span class="n">frac</span><span class="p">{</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">[</span><span class="n">x</span><span class="p">]}{</span> \<span class="n">sqrt</span><span class="p">{</span><span class="n">Var</span><span class="p">[</span><span class="n">x</span><span class="p">]}</span> <span class="o">+</span> \<span class="n">epsilon</span><span class="p">}</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">beta</span>
</pre></div>
</div>
<p>The mean and standard-deviation are calculated per-dimension separately
for each object in a mini-batch. Gamma and beta are learnable parameter vectors
of size C (where C is the input size).</p>
<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1.</p>
<p>At evaluation time (<cite>.eval()</cite>), the default behaviour of the InstanceNorm module stays the same
i.e. running mean/variance is NOT used for normalization. One can force using stored
mean and variance with <cite>.train(False)</cite> method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> &#8211; num_features from an expected input of size batch_size x num_features x height x width</li>
<li><strong>eps</strong> &#8211; a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> &#8211; the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> &#8211; a boolean value that when set to true, gives the layer learnable affine parameters.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="instancenorm3d">
<h4><span class="hidden-section">InstanceNorm3d</span><a class="headerlink" href="#instancenorm3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.InstanceNorm3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">InstanceNorm3d</code><span class="sig-paren">(</span><em>num_features</em>, <em>eps=1e-05</em>, <em>momentum=0.1</em>, <em>affine=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.InstanceNorm3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization over a 5d input that is seen as a mini-batch of 4d inputs</p>
<div class="math">
\[y = \frac{x - mean[x]}{ \sqrt{Var[x]} + \epsilon} * gamma + beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch.
Gamma and beta are learnable parameter vectors
of size C (where C is the input size).</p>
<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1.</p>
<p>At evaluation time (<cite>.eval()</cite>), the default behaviour of the InstanceNorm module stays the same
i.e. running mean/variance is NOT used for normalization. One can force using stored
mean and variance with <cite>.train(False)</cite> method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_features</strong> &#8211; num_features from an expected input of size batch_size x num_features x depth x height x width</li>
<li><strong>eps</strong> &#8211; a value added to the denominator for numerical stability. Default: 1e-5</li>
<li><strong>momentum</strong> &#8211; the value used for the running_mean and running_var computation. Default: 0.1</li>
<li><strong>affine</strong> &#8211; a boolean value that when set to true, gives the layer learnable affine parameters.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, D, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Without Learnable Parameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="recurrent-layers">
<h3>Recurrent layers<a class="headerlink" href="#recurrent-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="rnn">
<h4><span class="hidden-section">RNN</span><a class="headerlink" href="#rnn" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.RNN">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">RNN</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.RNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a multi-layer Elman RNN with tanh or ReLU non-linearity to an input sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math">
\[h_t = \tanh(w_{ih} * x_t + b_{ih}  +  w_{hh} * h_{(t-1)} + b_{hh})\]</div>
<p>where <span class="math">\(h_t\)</span> is the hidden state at time <cite>t</cite>, and <span class="math">\(x_t\)</span> is the hidden
state of the previous layer at time <cite>t</cite> or <span class="math">\(input_t\)</span> for the first layer.
If nonlinearity=&#8217;relu&#8217;, then <cite>ReLU</cite> is used instead of <cite>tanh</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> &#8211; The number of expected features in the input x</li>
<li><strong>hidden_size</strong> &#8211; The number of features in the hidden state h</li>
<li><strong>num_layers</strong> &#8211; Number of recurrent layers.</li>
<li><strong>nonlinearity</strong> &#8211; The non-linearity to use [&#8216;tanh&#8217;|&#8217;relu&#8217;]. Default: &#8216;tanh&#8217;</li>
<li><strong>bias</strong> &#8211; If False, then the layer does not use bias weights b_ih and b_hh. Default: True</li>
<li><strong>batch_first</strong> &#8211; If True, then the input and output tensors are provided as (batch, seq, feature)</li>
<li><strong>dropout</strong> &#8211; If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer</li>
<li><strong>bidirectional</strong> &#8211; If True, becomes a bidirectional RNN. Default: False</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, h_0</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> (seq_len, batch, input_size): tensor containing the features of the input sequence.
The input can also be a packed variable length sequence. See <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence()</span></code></a>
for details.</li>
<li><strong>h_0</strong> (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state
for each element in the batch.</li>
</ul>
</dd>
<dt>Outputs: output, h_n</dt>
<dd><ul class="first last simple">
<li><strong>output</strong> (seq_len, batch, hidden_size * num_directions): tensor containing the output features (h_k)
from the last layer of the RNN, for each k.  If a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal"><span class="pre">torch.nn.utils.rnn.PackedSequence</span></code></a> has been given
as the input, the output will also be a packed sequence.</li>
<li><strong>h_n</strong> (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for k=seq_len.</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih_l[k]</strong> &#8211; the learnable input-hidden weights of the k-th layer,
of shape <cite>(input_size x hidden_size)</cite></li>
<li><strong>weight_hh_l[k]</strong> &#8211; the learnable hidden-hidden weights of the k-th layer,
of shape <cite>(hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih_l[k]</strong> &#8211; the learnable input-hidden bias of the k-th layer, of shape <cite>(hidden_size)</cite></li>
<li><strong>bias_hh_l[k]</strong> &#8211; the learnable hidden-hidden bias of the k-th layer, of shape <cite>(hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="lstm">
<h4><span class="hidden-section">LSTM</span><a class="headerlink" href="#lstm" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.LSTM">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LSTM</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.LSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\
f_t = sigmoid(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\
g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\
o_t = sigmoid(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\
c_t = f_t * c_{(t-1)} + i_t * g_t \\
h_t = o_t * \tanh(c_t)
\end{array}\end{split}\]</div>
<p>where <span class="math">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math">\(c_t\)</span> is the cell state at time <cite>t</cite>,
<span class="math">\(x_t\)</span> is the hidden state of the previous layer at time <cite>t</cite> or <span class="math">\(input_t\)</span> for the first layer,
and <span class="math">\(i_t\)</span>, <span class="math">\(f_t\)</span>, <span class="math">\(g_t\)</span>, <span class="math">\(o_t\)</span> are the input, forget,
cell, and out gates, respectively.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> &#8211; The number of expected features in the input x</li>
<li><strong>hidden_size</strong> &#8211; The number of features in the hidden state h</li>
<li><strong>num_layers</strong> &#8211; Number of recurrent layers.</li>
<li><strong>bias</strong> &#8211; If False, then the layer does not use bias weights b_ih and b_hh. Default: True</li>
<li><strong>batch_first</strong> &#8211; If True, then the input and output tensors are provided as (batch, seq, feature)</li>
<li><strong>dropout</strong> &#8211; If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer</li>
<li><strong>bidirectional</strong> &#8211; If True, becomes a bidirectional RNN. Default: False</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, (h_0, c_0)</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> (seq_len, batch, input_size): tensor containing the features of the input sequence.
The input can also be a packed variable length sequence. See <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence()</span></code></a>
for details.</li>
<li><strong>h_0</strong> (num_layers * num_directions, batch, hidden_size): tensor containing
the initial hidden state for each element in the batch.</li>
<li><strong>c_0</strong> (num_layers * num_directions, batch, hidden_size): tensor containing
the initial cell state for each element in the batch.</li>
</ul>
</dd>
<dt>Outputs: output, (h_n, c_n)</dt>
<dd><ul class="first last simple">
<li><strong>output</strong> (seq_len, batch, hidden_size * num_directions): tensor containing
the output features <cite>(h_t)</cite> from the last layer of the RNN, for each t. If a
<a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal"><span class="pre">torch.nn.utils.rnn.PackedSequence</span></code></a> has been given as the input, the output will also be a
packed sequence.</li>
<li><strong>h_n</strong> (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t=seq_len</li>
<li><strong>c_n</strong> (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t=seq_len</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih_l[k]</strong> &#8211; the learnable input-hidden weights of the k-th layer <cite>(W_ii|W_if|W_ig|W_io)</cite>, of shape
<cite>(input_size x 4*hidden_size)</cite></li>
<li><strong>weight_hh_l[k]</strong> &#8211; the learnable hidden-hidden weights of the k-th layer <cite>(W_hi|W_hf|W_hg|W_ho)</cite>, of shape
<cite>(hidden_size x 4*hidden_size)</cite></li>
<li><strong>bias_ih_l[k]</strong> &#8211; the learnable input-hidden bias of the k-th layer <cite>(b_ii|b_if|b_ig|b_io)</cite>, of shape
<cite>(4*hidden_size)</cite></li>
<li><strong>bias_hh_l[k]</strong> &#8211; the learnable hidden-hidden bias of the k-th layer <cite>(W_hi|W_hf|W_hg|b_ho)</cite>, of shape
<cite>(4*hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="gru">
<h4><span class="hidden-section">GRU</span><a class="headerlink" href="#gru" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.GRU">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">GRU</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.GRU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p>
<p>For each element in the input sequence, each layer computes the following
function:</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
r_t = sigmoid(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\
i_t = sigmoid(W_{ii} x_t + b_{ii} + W_hi h_{(t-1)} + b_{hi}) \\
n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\
h_t = (1 - i_t) * n_t + i_t * h_{(t-1)} \\
\end{array}\end{split}\]</div>
<p>where <span class="math">\(h_t\)</span> is the hidden state at time <cite>t</cite>, <span class="math">\(x_t\)</span> is the hidden
state of the previous layer at time <cite>t</cite> or <span class="math">\(input_t\)</span> for the first layer,
and <span class="math">\(r_t\)</span>, <span class="math">\(i_t\)</span>, <span class="math">\(n_t\)</span> are the reset, input, and new gates, respectively.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> &#8211; The number of expected features in the input x</li>
<li><strong>hidden_size</strong> &#8211; The number of features in the hidden state h</li>
<li><strong>num_layers</strong> &#8211; Number of recurrent layers.</li>
<li><strong>bias</strong> &#8211; If False, then the layer does not use bias weights b_ih and b_hh. Default: True</li>
<li><strong>batch_first</strong> &#8211; If True, then the input and output tensors are provided as (batch, seq, feature)</li>
<li><strong>dropout</strong> &#8211; If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer</li>
<li><strong>bidirectional</strong> &#8211; If True, becomes a bidirectional RNN. Default: False</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, h_0</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> (seq_len, batch, input_size): tensor containing the features of the input sequence.
The input can also be a packed variable length sequence. See <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal"><span class="pre">torch.nn.utils.rnn.pack_padded_sequence()</span></code></a>
for details.</li>
<li><strong>h_0</strong> (num_layers * num_directions, batch, hidden_size): tensor containing the initial
hidden state for each element in the batch.</li>
</ul>
</dd>
<dt>Outputs: output, h_n</dt>
<dd><ul class="first last simple">
<li><strong>output</strong> (seq_len, batch, hidden_size * num_directions): tensor containing the output features h_t from
the last layer of the RNN, for each t. If a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal"><span class="pre">torch.nn.utils.rnn.PackedSequence</span></code></a> has been given as the
input, the output will also be a packed sequence.</li>
<li><strong>h_n</strong> (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t=seq_len</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih_l[k]</strong> &#8211; the learnable input-hidden weights of the k-th layer (W_ir|W_ii|W_in), of shape
<cite>(input_size x 3*hidden_size)</cite></li>
<li><strong>weight_hh_l[k]</strong> &#8211; the learnable hidden-hidden weights of the k-th layer (W_hr|W_hi|W_hn), of shape
<cite>(hidden_size x 3*hidden_size)</cite></li>
<li><strong>bias_ih_l[k]</strong> &#8211; the learnable input-hidden bias of the k-th layer (b_ir|b_ii|b_in), of shape
<cite>(3*hidden_size)</cite></li>
<li><strong>bias_hh_l[k]</strong> &#8211; the learnable hidden-hidden bias of the k-th layer (W_hr|W_hi|W_hn), of shape
<cite>(3*hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="rnncell">
<h4><span class="hidden-section">RNNCell</span><a class="headerlink" href="#rnncell" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.RNNCell">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">RNNCell</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>bias=True</em>, <em>nonlinearity='tanh'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.RNNCell" title="Permalink to this definition">¶</a></dt>
<dd><p>An Elman RNN cell with tanh or ReLU non-linearity.</p>
<div class="math">
\[h' = \tanh(w_{ih} * x + b_{ih}  +  w_{hh} * h + b_{hh})\]</div>
<p>If nonlinearity=&#8217;relu&#8217;, then ReLU is used in place of tanh.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> &#8211; The number of expected features in the input x</li>
<li><strong>hidden_size</strong> &#8211; The number of features in the hidden state h</li>
<li><strong>bias</strong> &#8211; If False, then the layer does not use bias weights b_ih and b_hh. Default: True</li>
<li><strong>nonlinearity</strong> &#8211; The non-linearity to use [&#8216;tanh&#8217;|&#8217;relu&#8217;]. Default: &#8216;tanh&#8217;</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, hidden</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> (batch, input_size): tensor containing input features</li>
<li><strong>hidden</strong> (batch, hidden_size): tensor containing the initial hidden state for each element in the batch.</li>
</ul>
</dd>
<dt>Outputs: h&#8217;</dt>
<dd><ul class="first last simple">
<li><strong>h&#8217;</strong> (batch, hidden_size): tensor containing the next hidden state for each element in the batch</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih</strong> &#8211; the learnable input-hidden weights, of shape <cite>(input_size x hidden_size)</cite></li>
<li><strong>weight_hh</strong> &#8211; the learnable hidden-hidden weights, of shape <cite>(hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih</strong> &#8211; the learnable input-hidden bias, of shape <cite>(hidden_size)</cite></li>
<li><strong>bias_hh</strong> &#8211; the learnable hidden-hidden bias, of shape <cite>(hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hx</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">hx</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hx</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hx</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="lstmcell">
<h4><span class="hidden-section">LSTMCell</span><a class="headerlink" href="#lstmcell" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.LSTMCell">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">LSTMCell</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.LSTMCell" title="Permalink to this definition">¶</a></dt>
<dd><p>A long short-term memory (LSTM) cell.</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
i = sigmoid(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\
f = sigmoid(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\
g = \tanh(W_{ig} x + b_{ig} + W_{hc} h + b_{hg}) \\
o = sigmoid(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\
c' = f * c + i * g \\
h' = o * \tanh(c_t) \\
\end{array}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> &#8211; The number of expected features in the input x</li>
<li><strong>hidden_size</strong> &#8211; The number of features in the hidden state h</li>
<li><strong>bias</strong> &#8211; If <cite>False</cite>, then the layer does not use bias weights <cite>b_ih</cite> and <cite>b_hh</cite>. Default: True</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, (h_0, c_0)</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> (batch, input_size): tensor containing input features</li>
<li><strong>h_0</strong> (batch, hidden_size): tensor containing the initial hidden state for each element in the batch.</li>
<li><strong>c_0</strong> (batch. hidden_size): tensor containing the initial cell state for each element in the batch.</li>
</ul>
</dd>
<dt>Outputs: h_1, c_1</dt>
<dd><ul class="first last simple">
<li><strong>h_1</strong> (batch, hidden_size): tensor containing the next hidden state for each element in the batch</li>
<li><strong>c_1</strong> (batch, hidden_size): tensor containing the next cell state for each element in the batch</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih</strong> &#8211; the learnable input-hidden weights, of shape <cite>(input_size x hidden_size)</cite></li>
<li><strong>weight_hh</strong> &#8211; the learnable hidden-hidden weights, of shape <cite>(hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih</strong> &#8211; the learnable input-hidden bias, of shape <cite>(hidden_size)</cite></li>
<li><strong>bias_hh</strong> &#8211; the learnable hidden-hidden bias, of shape <cite>(hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hx</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cx</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">hx</span><span class="p">,</span> <span class="n">cx</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">(</span><span class="n">hx</span><span class="p">,</span> <span class="n">cx</span><span class="p">))</span>
<span class="gp">... </span>    <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hx</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="grucell">
<h4><span class="hidden-section">GRUCell</span><a class="headerlink" href="#grucell" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.GRUCell">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">GRUCell</code><span class="sig-paren">(</span><em>input_size</em>, <em>hidden_size</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.GRUCell" title="Permalink to this definition">¶</a></dt>
<dd><p>A gated recurrent unit (GRU) cell</p>
<div class="math">
\[\begin{split}\begin{array}{ll}
r = sigmoid(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\
i = sigmoid(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\
n = \tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\
h' = (1 - i) * n + i * h
\end{array}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input_size</strong> &#8211; The number of expected features in the input x</li>
<li><strong>hidden_size</strong> &#8211; The number of features in the hidden state h</li>
<li><strong>bias</strong> &#8211; If <cite>False</cite>, then the layer does not use bias weights <cite>b_ih</cite> and <cite>b_hh</cite>. Default: <cite>True</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Inputs: input, hidden</dt>
<dd><ul class="first last simple">
<li><strong>input</strong> (batch, input_size): tensor containing input features</li>
<li><strong>hidden</strong> (batch, hidden_size): tensor containing the initial hidden state for each element in the batch.</li>
</ul>
</dd>
<dt>Outputs: h&#8217;</dt>
<dd><ul class="first last simple">
<li><strong>h&#8217;</strong>: (batch, hidden_size): tensor containing the next hidden state for each element in the batch</li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight_ih</strong> &#8211; the learnable input-hidden weights, of shape <cite>(input_size x hidden_size)</cite></li>
<li><strong>weight_hh</strong> &#8211; the learnable hidden-hidden weights, of shape <cite>(hidden_size x hidden_size)</cite></li>
<li><strong>bias_ih</strong> &#8211; the learnable input-hidden bias, of shape <cite>(hidden_size)</cite></li>
<li><strong>bias_hh</strong> &#8211; the learnable hidden-hidden bias, of shape <cite>(hidden_size)</cite></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRUCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hx</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">hx</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hx</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hx</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="linear-layers">
<h3>Linear layers<a class="headerlink" href="#linear-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="linear">
<h4><span class="hidden-section">Linear</span><a class="headerlink" href="#linear" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Linear">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Linear</code><span class="sig-paren">(</span><em>in_features</em>, <em>out_features</em>, <em>bias=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a linear transformation to the incoming data: <span class="math">\(y = Ax + b\)</span></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_features</strong> &#8211; size of each input sample</li>
<li><strong>out_features</strong> &#8211; size of each output sample</li>
<li><strong>bias</strong> &#8211; If set to False, the layer will not learn an additive bias. Default: True</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, in\_features)\)</span></li>
<li>Output: <span class="math">\((N, out\_features)\)</span></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> &#8211; the learnable weights of the module of shape (out_features x in_features)</li>
<li><strong>bias</strong> &#8211; the learnable bias of the module of shape (out_features)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="dropout-layers">
<h3>Dropout layers<a class="headerlink" href="#dropout-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="dropout">
<h4><span class="hidden-section">Dropout</span><a class="headerlink" href="#dropout" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Dropout">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Dropout</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zeroes some of the elements of the input tensor.
The elements to zero are randomized on every forward call.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> &#8211; probability of an element to be zeroed. Default: 0.5</li>
<li><strong>inplace</strong> &#8211; If set to True, will do this operation in-place. Default: false</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <cite>Any</cite>. Input can be of any shape</li>
<li>Output: <cite>Same</cite>. Output is of the same shape as input</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="dropout2d">
<h4><span class="hidden-section">Dropout2d</span><a class="headerlink" href="#dropout2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Dropout2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Dropout2d</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Dropout2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zeroes whole channels of the input tensor.
The channels to zero-out are randomized on every forward call.</p>
<p><em>Usually the input comes from Conv2d modules.</em></p>
<p>As described in the paper
<a class="reference external" href="http://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> ,
if adjacent pixels within feature maps are strongly correlated
(as is normally the case in early convolution layers) then iid dropout
will not regularize the activations and will otherwise just result
in an effective learning rate decrease.</p>
<p>In this case, <code class="xref py py-func docutils literal"><span class="pre">nn.Dropout2d()</span></code> will help promote independence between
feature maps and should be used instead.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; probability of an element to be zeroed.</li>
<li><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; If set to True, will do this operation in-place</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout2d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="dropout3d">
<h4><span class="hidden-section">Dropout3d</span><a class="headerlink" href="#dropout3d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Dropout3d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Dropout3d</code><span class="sig-paren">(</span><em>p=0.5</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Dropout3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zeroes whole channels of the input tensor.
The channels to zero are randomized on every forward call.</p>
<p><em>Usually the input comes from Conv3d modules.</em></p>
<p>As described in the paper
<a class="reference external" href="http://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a> ,
if adjacent pixels within feature maps are strongly correlated
(as is normally the case in early convolution layers) then iid dropout
will not regularize the activations and will otherwise just result
in an effective learning rate decrease.</p>
<p>In this case, <code class="xref py py-func docutils literal"><span class="pre">nn.Dropout3d()</span></code> will help promote independence between
feature maps and should be used instead.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>p</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; probability of an element to be zeroed.</li>
<li><strong>inplace</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; If set to True, will do this operation in-place</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, D, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, D, H, W)\)</span> (same shape as input)</li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout3d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="sparse-layers">
<h3>Sparse layers<a class="headerlink" href="#sparse-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="embedding">
<h4><span class="hidden-section">Embedding</span><a class="headerlink" href="#embedding" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.Embedding">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">Embedding</code><span class="sig-paren">(</span><em>num_embeddings</em>, <em>embedding_dim</em>, <em>padding_idx=None</em>, <em>max_norm=None</em>, <em>norm_type=2</em>, <em>scale_grad_by_freq=False</em>, <em>sparse=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.Embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p>
<p>This module is often used to store word embeddings and retrieve them using indices.
The input to the module is a list of indices, and the output is the corresponding
word embeddings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>num_embeddings</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; size of the dictionary of embeddings</li>
<li><strong>embedding_dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; the size of each embedding vector</li>
<li><strong>padding_idx</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; If given, pads the output with zeros whenever it encounters the index.</li>
<li><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; If given, will renormalize the embeddings to always have a norm lesser than this</li>
<li><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; The p of the p-norm to compute for the max_norm option</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) &#8211; if given, this will scale gradients by the frequency of
the words in the dictionary.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><p class="first last"><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; the learnable weights of the module of shape (num_embeddings, embedding_dim)</p>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: LongTensor <cite>(N, W)</cite>, N = mini-batch, W = number of indices to extract per mini-batch</li>
<li>Output: <cite>(N, W, embedding_dim)</cite></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># an Embedding module containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">Variable containing:</span>
<span class="go">(0 ,.,.) =</span>
<span class="go"> -1.0822  1.2522  0.2434</span>
<span class="go">  0.8393 -0.6062 -0.3348</span>
<span class="go">  0.6597  0.0350  0.0837</span>
<span class="go">  0.5521  0.9447  0.0498</span>

<span class="go">(1 ,.,.) =</span>
<span class="go">  0.6597  0.0350  0.0837</span>
<span class="go"> -0.1527  0.0877  0.4260</span>
<span class="go">  0.8393 -0.6062 -0.3348</span>
<span class="go"> -0.8738 -0.9054  0.4281</span>
<span class="go">[torch.FloatTensor of size 2x4x3]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># example with padding_idx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="go">Variable containing:</span>
<span class="go">(0 ,.,.) =</span>
<span class="go">  0.0000  0.0000  0.0000</span>
<span class="go">  0.3452  0.4937 -0.9361</span>
<span class="go">  0.0000  0.0000  0.0000</span>
<span class="go">  0.0706 -2.1962 -0.6276</span>
<span class="go">[torch.FloatTensor of size 1x4x3]</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="distance-functions">
<h3>Distance functions<a class="headerlink" href="#distance-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="pairwisedistance">
<h4><span class="hidden-section">PairwiseDistance</span><a class="headerlink" href="#pairwisedistance" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.PairwiseDistance">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PairwiseDistance</code><span class="sig-paren">(</span><em>p=2</em>, <em>eps=1e-06</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.PairwiseDistance" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the batchwise pairwise distance between vectors v1,v2:</p>
<blockquote>
<div><div class="math">
\[\Vert x \Vert _p := \left( \sum_{i=1}^n  \vert x_i \vert ^ p \right) ^ {1/p}\]</div>
<dl class="docutils">
<dt>Args:</dt>
<dd>x (Tensor): input tensor containing the two input batches
p (real): the norm degree. Default: 2</dd>
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, D)\)</span> where <cite>D = vector dimension</cite></li>
<li>Output: <span class="math">\((N, 1)\)</span></li>
</ul>
</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pdist</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PairwiseDistance</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">pdist</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</dd></dl>

</div>
</div>
<div class="section" id="loss-functions">
<h3>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="l1loss">
<h4><span class="hidden-section">L1Loss</span><a class="headerlink" href="#l1loss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.L1Loss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">L1Loss</code><span class="sig-paren">(</span><em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.L1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the mean absolute value of the
element-wise difference between input <cite>x</cite> and target <cite>y</cite>:</p>
<p><span class="math">\({loss}(x, y)  = 1/n \sum |x_i - y_i|\)</span></p>
<p><cite>x</cite> and <cite>y</cite> arbitrary shapes with a total of <cite>n</cite> elements each.</p>
<p>The sum operation still operates over all the elements, and divides by <cite>n</cite>.</p>
<p>The division by <cite>n</cite> can be avoided if one sets the constructor argument <cite>size_average=False</cite></p>
</dd></dl>

</div>
<div class="section" id="mseloss">
<h4><span class="hidden-section">MSELoss</span><a class="headerlink" href="#mseloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MSELoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MSELoss</code><span class="sig-paren">(</span><em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MSELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the mean squared error between
<cite>n</cite> elements in the input <cite>x</cite> and target <cite>y</cite>:</p>
<p><span class="math">\({loss}(x, y)  = 1/n \sum |x_i - y_i|^2\)</span></p>
<p><cite>x</cite> and <cite>y</cite> arbitrary shapes with a total of <cite>n</cite> elements each.</p>
<p>The sum operation still operates over all the elements, and divides by <cite>n</cite>.</p>
<p>The division by <cite>n</cite> can be avoided if one sets the internal variable
<cite>size_average</cite> to <cite>False</cite>.</p>
</dd></dl>

</div>
<div class="section" id="crossentropyloss">
<h4><span class="hidden-section">CrossEntropyLoss</span><a class="headerlink" href="#crossentropyloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.CrossEntropyLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CrossEntropyLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.CrossEntropyLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>This criterion combines <cite>LogSoftMax</cite> and <cite>NLLLoss</cite> in one single class.</p>
<p>It is useful when training a classification problem with <cite>n</cite> classes.
If provided, the optional argument <cite>weights</cite> should be a 1D <cite>Tensor</cite>
assigning weight to each of the classes.
This is particularly useful when you have an unbalanced training set.</p>
<p>The <cite>input</cite> is expected to contain scores for each class.</p>
<p><cite>input</cite> has to be a 2D <cite>Tensor</cite> of size <cite>batch x n</cite>.</p>
<p>This criterion expects a class index (0 to nClasses-1) as the
<cite>target</cite> for each value of a 1D tensor of size <cite>n</cite></p>
<p>The loss can be described as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">class</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">class</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span>\<span class="n">sum_j</span> <span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">])))</span>
               <span class="o">=</span> <span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">class</span><span class="p">]</span> <span class="o">+</span> <span class="n">log</span><span class="p">(</span>\<span class="n">sum_j</span> <span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
</pre></div>
</div>
<p>or in the case of the <cite>weights</cite> argument being specified:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">class</span><span class="p">)</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">class</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">class</span><span class="p">]</span> <span class="o">+</span> <span class="n">log</span><span class="p">(</span>\<span class="n">sum_j</span> <span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">])))</span>
</pre></div>
</div>
<p>The losses are averaged across observations for each minibatch.</p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite></li>
<li>Target: <span class="math">\((N)\)</span> where each value is <cite>0 &lt;= targets[i] &lt;= C-1</cite></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="nllloss">
<h4><span class="hidden-section">NLLLoss</span><a class="headerlink" href="#nllloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.NLLLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">NLLLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.NLLLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The negative log likelihood loss. It is useful to train a classification problem with n classes</p>
<p>If provided, the optional argument <cite>weights</cite> should be a 1D Tensor assigning
weight to each of the classes.</p>
<p>This is particularly useful when you have an unbalanced training set.</p>
<p>The input given through a forward call is expected to contain log-probabilities
of each class: input has to be a 2D Tensor of size <cite>(minibatch, n)</cite></p>
<p>Obtaining log-probabilities in a neural network is easily achieved by
adding a  <cite>LogSoftmax</cite>  layer in the last layer of your network.</p>
<p>You may use <cite>CrossEntropyLoss</cite>  instead, if you prefer not to add an extra layer.</p>
<p>The target that this loss expects is a class index <cite>(0 to N-1, where N = number of classes)</cite></p>
<p>The loss can be described as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">class</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">class</span><span class="p">]</span>
</pre></div>
</div>
<p>or in the case of the weights argument it is specified as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">class</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span><span class="n">weights</span><span class="p">[</span><span class="n">class</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">class</span><span class="p">]</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; a manual rescaling weight given to each class.
If given, has to be a Tensor of size &#8220;nclasses&#8221;</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged over observations for each minibatch.
However, if the field size_average is set to False,
the losses are instead summed for each minibatch.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite></li>
<li>Target: <span class="math">\((N)\)</span> where each value is <cite>0 &lt;= targets[i] &lt;= C-1</cite></li>
</ul>
</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><strong>weight</strong> &#8211; the class-weights given as input to the constructor</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size nBatch x nClasses = 3 x 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; nclasses</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="nllloss2d">
<h4><span class="hidden-section">NLLLoss2d</span><a class="headerlink" href="#nllloss2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.NLLLoss2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">NLLLoss2d</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.NLLLoss2d" title="Permalink to this definition">¶</a></dt>
<dd><p>This is negative log likehood loss, but for image inputs. It computes NLL loss per-pixel.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; a manual rescaling weight given to each class.
If given, has to be a 1D Tensor having as many elements, as there are classes.</li>
<li><strong>size_average</strong> &#8211; By default, the losses are averaged over observations for each minibatch.
However, if the field size_average is set to False, the losses
are instead summed for each minibatch. Default: True</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H, W)\)</span> where <cite>C = number of classes</cite></li>
<li>Target: <span class="math">\((N, H, W)\)</span> where each value is <cite>0 &lt;= targets[i] &lt;= C-1</cite></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss2d</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size nBatch x nClasses x height x width</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; nclasses</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="kldivloss">
<h4><span class="hidden-section">KLDivLoss</span><a class="headerlink" href="#kldivloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.KLDivLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">KLDivLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.KLDivLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> Loss</p>
<p>KL divergence is a useful distance measure for continuous distributions
and is often useful when performing direct regression over the space of
(discretely sampled) continuous output distributions.</p>
<p>As with <cite>NLLLoss</cite>, the <cite>input</cite> given is expected to contain
<em>log-probabilities</em>, however unlike <cite>ClassNLLLoss</cite>, <cite>input</cite> is not
restricted to a 2D Tensor, because the criterion is applied element-wise.</p>
<p>This criterion expects a <cite>target</cite> <cite>Tensor</cite> of the same size as the
<cite>input</cite> <cite>Tensor</cite>.</p>
<p>The loss can be described as:</p>
<div class="math">
\[loss(x, target) = 1/n \sum(target_i * (log(target_i) - x_i))\]</div>
<p>By default, the losses are averaged for each minibatch over observations
<strong>as well as</strong> over dimensions. However, if the field
<cite>size_average</cite> is set to <cite>False</cite>, the losses are instead summed.</p>
</dd></dl>

</div>
<div class="section" id="bceloss">
<h4><span class="hidden-section">BCELoss</span><a class="headerlink" href="#bceloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.BCELoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">BCELoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.BCELoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the Binary Cross Entropy
between the target and the output:</p>
<div class="math">
\[loss(o, t) = - 1/n \sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))\]</div>
<p>or in the case of the weights argument being specified:</p>
<div class="math">
\[loss(o, t) = - 1/n \sum_i weights[i] * (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))\]</div>
<p>This is used for measuring the error of a reconstruction in for example
an auto-encoder. Note that the targets <cite>t[i]</cite> should be numbers between 0 and 1.</p>
<p>By default, the losses are averaged for each minibatch over observations
<em>as well as</em> over dimensions. However, if the field <cite>size_average</cite> is set
to <cite>False</cite>, the losses are instead summed.</p>
</dd></dl>

</div>
<div class="section" id="marginrankingloss">
<h4><span class="hidden-section">MarginRankingLoss</span><a class="headerlink" href="#marginrankingloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MarginRankingLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MarginRankingLoss</code><span class="sig-paren">(</span><em>margin=0</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MarginRankingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the loss given
inputs <cite>x1</cite>, <cite>x2</cite>, two 1D mini-batch <cite>Tensor`s,
and a label 1D mini-batch tensor `y</cite> with values (<cite>1</cite> or <cite>-1</cite>).</p>
<p>If <cite>y == 1</cite> then it assumed the first input should be ranked higher
(have a larger value) than the second input, and vice-versa for <cite>y == -1</cite>.</p>
<p>The loss function for each sample in the mini-batch is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">y</span> <span class="o">*</span> <span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span> <span class="o">+</span> <span class="n">margin</span><span class="p">)</span>
</pre></div>
</div>
<p>if the internal variable <cite>size_average = True</cite>,
the loss function averages the loss over the batch samples;
if <cite>size_average = False</cite>, then the loss function sums over the batch samples.
By default, <cite>size_average</cite> equals to <cite>True</cite>.</p>
</dd></dl>

</div>
<div class="section" id="hingeembeddingloss">
<h4><span class="hidden-section">HingeEmbeddingLoss</span><a class="headerlink" href="#hingeembeddingloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.HingeEmbeddingLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">HingeEmbeddingLoss</code><span class="sig-paren">(</span><em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.HingeEmbeddingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Measures the loss given an input <cite>x</cite> which is a 2D mini-batch tensor
and a labels <cite>y</cite>, a 1D tensor containg values (<cite>1</cite> or <cite>-1</cite>).
This is usually used for measuring whether two inputs are similar or dissimilar,
e.g. using the L1 pairwise distance, and is typically used for learning
nonlinear embeddings or semi-supervised learning:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>                 <span class="p">{</span> <span class="n">x_i</span><span class="p">,</span>                  <span class="k">if</span> <span class="n">y_i</span> <span class="o">==</span>  <span class="mi">1</span>
<span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">n</span> <span class="p">{</span>
                 <span class="p">{</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">margin</span> <span class="o">-</span> <span class="n">x_i</span><span class="p">),</span> <span class="k">if</span> <span class="n">y_i</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span>
</pre></div>
</div>
<p><cite>x</cite> and <cite>y</cite> arbitrary shapes with a total of <cite>n</cite> elements each
the sum operation still operates over all the elements, and divides by <cite>n</cite>.</p>
<p>The division by <cite>n</cite> can be avoided if one sets the internal variable <cite>size_average=False</cite>.</p>
<p>The <cite>margin</cite> has a default value of <cite>1</cite>, or can be set in the constructor.</p>
</dd></dl>

</div>
<div class="section" id="multilabelmarginloss">
<h4><span class="hidden-section">MultiLabelMarginLoss</span><a class="headerlink" href="#multilabelmarginloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MultiLabelMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MultiLabelMarginLoss</code><span class="sig-paren">(</span><em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MultiLabelMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a multi-class multi-classification
hinge loss (margin-based loss) between input <cite>x</cite>  (a 2D mini-batch <cite>Tensor</cite>) and
output <cite>y</cite> (which is a 2D <cite>Tensor</cite> of target class indices).
For each sample in the mini-batch:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="n">sum_ij</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">j</span><span class="p">]]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>where <cite>i == 0</cite> to <cite>x.size(0)</cite>, <cite>j == 0</cite> to <cite>y.size(0)</cite>,
<cite>y[j] != 0</cite>, and <cite>i != y[j]</cite> for all <cite>i</cite> and <cite>j</cite>.</p>
<p><cite>y</cite> and <cite>x</cite> must have the same size.</p>
<p>The criterion only considers the first non zero <cite>y[j]</cite> targets.</p>
<p>This allows for different samples to have variable amounts of target classes</p>
</dd></dl>

</div>
<div class="section" id="smoothl1loss">
<h4><span class="hidden-section">SmoothL1Loss</span><a class="headerlink" href="#smoothl1loss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.SmoothL1Loss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">SmoothL1Loss</code><span class="sig-paren">(</span><em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.SmoothL1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that uses a squared term if the absolute
element-wise error falls below 1 and an L1 term otherwise.
It is less sensitive to outliers than the <cite>MSELoss</cite> and in some cases
prevents exploding gradients (e.g. see &#8220;Fast R-CNN&#8221; paper by Ross Girshick).
Also known as the Huber loss:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>                      <span class="p">{</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_i</span> <span class="o">-</span> <span class="n">y_i</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span><span class="p">,</span> <span class="k">if</span> <span class="o">|</span><span class="n">x_i</span> <span class="o">-</span> <span class="n">y_i</span><span class="o">|</span> <span class="o">&lt;</span> <span class="mi">1</span>
<span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">n</span> \<span class="nb">sum</span> <span class="p">{</span>
                      <span class="p">{</span> <span class="o">|</span><span class="n">x_i</span> <span class="o">-</span> <span class="n">y_i</span><span class="o">|</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span>   <span class="n">otherwise</span>
</pre></div>
</div>
<p><cite>x</cite> and <cite>y</cite> arbitrary shapes with a total of <cite>n</cite> elements each
the sum operation still operates over all the elements, and divides by <cite>n</cite>.</p>
<p>The division by <cite>n</cite> can be avoided if one sets the internal variable
<cite>size_average</cite> to <cite>False</cite></p>
</dd></dl>

</div>
<div class="section" id="softmarginloss">
<h4><span class="hidden-section">SoftMarginLoss</span><a class="headerlink" href="#softmarginloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.SoftMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">SoftMarginLoss</code><span class="sig-paren">(</span><em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.SoftMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a two-class classification
logistic loss between input <cite>x</cite> (a 2D mini-batch Tensor) and
target <cite>y</cite> (which is a tensor containing either <cite>1</cite> or <cite>-1</cite>).</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="n">sum_i</span> <span class="p">(</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span>
</pre></div>
</div>
<p>The normalization by the number of elements in the input can be disabled by
setting <cite>self.size_average</cite> to <cite>False</cite>.</p>
</dd></dl>

</div>
<div class="section" id="multilabelsoftmarginloss">
<h4><span class="hidden-section">MultiLabelSoftMarginLoss</span><a class="headerlink" href="#multilabelsoftmarginloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MultiLabelSoftMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MultiLabelSoftMarginLoss</code><span class="sig-paren">(</span><em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MultiLabelSoftMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a multi-label one-versus-all
loss based on max-entropy, between input <cite>x</cite>  (a 2D mini-batch <cite>Tensor</cite>) and
target <cite>y</cite> (a binary 2D <cite>Tensor</cite>). For each sample in the minibatch:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span> <span class="n">sum_i</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="n">log</span><span class="p">(</span> <span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
                      <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]))))</span> <span class="o">/</span> <span class="n">x</span><span class="p">:</span><span class="n">nElement</span><span class="p">()</span>
</pre></div>
</div>
<p>where <cite>i == 0</cite> to <cite>x.nElement()-1</cite>, <cite>y[i]  in {0,1}</cite>.
<cite>y</cite> and <cite>x</cite> must have the same size.</p>
</dd></dl>

</div>
<div class="section" id="cosineembeddingloss">
<h4><span class="hidden-section">CosineEmbeddingLoss</span><a class="headerlink" href="#cosineembeddingloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.CosineEmbeddingLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">CosineEmbeddingLoss</code><span class="sig-paren">(</span><em>margin=0</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.CosineEmbeddingLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that measures the loss given  an input tensors x1, x2
and a <cite>Tensor</cite> label <cite>y</cite> with values 1 or -1.
This is used for measuring whether two inputs are similar or dissimilar,
using the cosine distance, and is typically used for learning nonlinear
embeddings or semi-supervised learning.</p>
<p><cite>margin</cite> should be a number from <cite>-1</cite> to <cite>1</cite>, <cite>0</cite> to <cite>0.5</cite> is suggested.
If <cite>margin</cite> is missing, the default value is <cite>0</cite>.</p>
<p>The loss function for each sample is:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>             <span class="p">{</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">cos</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">),</span>              <span class="k">if</span> <span class="n">y</span> <span class="o">==</span>  <span class="mi">1</span>
<span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="p">{</span>
             <span class="p">{</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">cos</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="o">-</span> <span class="n">margin</span><span class="p">),</span> <span class="k">if</span> <span class="n">y</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span>
</pre></div>
</div>
<p>If the internal variable <cite>size_average</cite> is equal to <cite>True</cite>,
the loss function averages the loss over the batch samples;
if <cite>size_average</cite> is <cite>False</cite>, then the loss function sums over the
batch samples. By default, <cite>size_average = True</cite>.</p>
</dd></dl>

</div>
<div class="section" id="multimarginloss">
<h4><span class="hidden-section">MultiMarginLoss</span><a class="headerlink" href="#multimarginloss" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.MultiMarginLoss">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">MultiMarginLoss</code><span class="sig-paren">(</span><em>p=1</em>, <em>margin=1</em>, <em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.MultiMarginLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a criterion that optimizes a multi-class classification hinge loss
(margin-based loss) between input <cite>x</cite> (a 2D mini-batch <cite>Tensor</cite>) and
output <cite>y</cite> (which is a 1D tensor of target class indices, <cite>0</cite> &lt;= <cite>y</cite> &lt;= <cite>x.size(1)</cite>):</p>
<p>For each mini-batch sample:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>loss(x, y) = sum_i(max(0, (margin - x[y] + x[i]))^p) / x.size(0)
             where `i == 0` to `x.size(0)` and `i != y`.
</pre></div>
</div>
<p>Optionally, you can give non-equal weighting on the classes by passing
a 1D <cite>weights</cite> tensor into the constructor.</p>
<p>The loss function then becomes:</p>
<blockquote>
<div>loss(x, y) = sum_i(max(0, w[y] * (margin - x[y] - x[i]))^p) / x.size(0)</div></blockquote>
<p>By default, the losses are averaged over observations for each minibatch.
However, if the field <cite>size_average</cite> is set to <cite>False</cite>,
the losses are instead summed.</p>
</dd></dl>

</div>
</div>
<div class="section" id="vision-layers">
<h3>Vision layers<a class="headerlink" href="#vision-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="pixelshuffle">
<h4><span class="hidden-section">PixelShuffle</span><a class="headerlink" href="#pixelshuffle" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.PixelShuffle">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">PixelShuffle</code><span class="sig-paren">(</span><em>upscale_factor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.PixelShuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>Rearranges elements in a Tensor of shape <span class="math">\((*, C * r^2, H, W]\)</span> to a
tensor of shape <span class="math">\((C, H * r, W * r)\)</span>.</p>
<p>This is useful for implementing efficient sub-pixel convolution
with a stride of <span class="math">\(1/r\)</span>.</p>
<p>Look at the paper:
<a class="reference external" href="https://arxiv.org/abs/1609.05158">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</a>
by Shi et. al (2016) for more details</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>upscale_factor</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; factor to increase spatial resolution by</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C * {upscale\_factor}^2, H, W)\)</span></li>
<li>Output: <span class="math">\((N, C, H * {upscale\_factor}, W * {upscale\_factor})\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ps</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PixelShuffle</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">ps</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([1, 1, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="upsamplingnearest2d">
<h4><span class="hidden-section">UpsamplingNearest2d</span><a class="headerlink" href="#upsamplingnearest2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.UpsamplingNearest2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">UpsamplingNearest2d</code><span class="sig-paren">(</span><em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.UpsamplingNearest2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D nearest neighbor upsampling to an input signal composed of several input
channels.</p>
<p>To specify the scale, it takes either the <code class="xref py py-attr docutils literal"><span class="pre">size</span></code> or the <code class="xref py py-attr docutils literal"><span class="pre">scale_factor</span></code>
as it&#8217;s constructor argument.</p>
<p>When <cite>size</cite> is given, it is the output size of the image (h, w).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; a tuple of ints (H_out, W_out) output sizes</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; the multiplier for the image height / width</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = floor(H_{in} * scale\_factor)\)</span>
<span class="math">\(W_{out} = floor(W_{in}  * scale\_factor)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inp</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  1  2</span>
<span class="go">  3  4</span>
<span class="go">[torch.FloatTensor of size 1x1x2x2]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingNearest2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  1  1  2  2</span>
<span class="go">  1  1  2  2</span>
<span class="go">  3  3  4  4</span>
<span class="go">  3  3  4  4</span>
<span class="go">[torch.FloatTensor of size 1x1x4x4]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="upsamplingbilinear2d">
<h4><span class="hidden-section">UpsamplingBilinear2d</span><a class="headerlink" href="#upsamplingbilinear2d" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.UpsamplingBilinear2d">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">UpsamplingBilinear2d</code><span class="sig-paren">(</span><em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.UpsamplingBilinear2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D bilinear upsampling to an input signal composed of several input
channels.</p>
<p>To specify the scale, it takes either the <code class="xref py py-attr docutils literal"><span class="pre">size</span></code> or the <code class="xref py py-attr docutils literal"><span class="pre">scale_factor</span></code>
as it&#8217;s constructor argument.</p>
<p>When <cite>size</cite> is given, it is the output size of the image (h, w).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a><em>, </em><em>optional</em>) &#8211; a tuple of ints (H_out, W_out) output sizes</li>
<li><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; the multiplier for the image height / width</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, C, H_{in}, W_{in})\)</span></li>
<li>Output: <span class="math">\((N, C, H_{out}, W_{out})\)</span> where
<span class="math">\(H_{out} = floor(H_{in} * scale\_factor)\)</span>
<span class="math">\(W_{out} = floor(W_{in}  * scale\_factor)\)</span></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inp</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  1  2</span>
<span class="go">  3  4</span>
<span class="go">[torch.FloatTensor of size 1x1x2x2]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">UpsamplingBilinear2d</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,0 ,.,.) =</span>
<span class="go">  1.0000  1.3333  1.6667  2.0000</span>
<span class="go">  1.6667  2.0000  2.3333  2.6667</span>
<span class="go">  2.3333  2.6667  3.0000  3.3333</span>
<span class="go">  3.0000  3.3333  3.6667  4.0000</span>
<span class="go">[torch.FloatTensor of size 1x1x4x4]</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="multi-gpu-layers">
<h3>Multi-GPU layers<a class="headerlink" href="#multi-gpu-layers" title="Permalink to this headline">¶</a></h3>
<div class="section" id="dataparallel">
<h4><span class="hidden-section">DataParallel</span><a class="headerlink" href="#dataparallel" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="torch.nn.DataParallel">
<em class="property">class </em><code class="descclassname">torch.nn.</code><code class="descname">DataParallel</code><span class="sig-paren">(</span><em>module</em>, <em>device_ids=None</em>, <em>output_device=None</em>, <em>dim=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.DataParallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements data parallelism at the module level.</p>
<p>This container parallelizes the application of the given module by
splitting the input across the specified devices by chunking in the batch
dimension. In the forward pass, the module is replicated on each device,
and each replica handles a portion of the input. During the backwards
pass, gradients from each replica are summed into the original module.</p>
<p>The batch size should be larger than the number of GPUs used. It should
also be an integer multiple of the number of GPUs so that each chunk is the
same size (so that each GPU processes the same number of samples).</p>
<p>See also: <a class="reference internal" href="index.html#cuda-nn-dataparallel-instead"><span class="std std-ref">Use nn.DataParallel instead of multiprocessing</span></a></p>
<p>Arbitrary positional and keyword inputs are allowed to be passed into
DataParallel EXCEPT Tensors. All variables will be scattered on dim
specified (default 0). Primitive types will be broadcasted, but all
other types will be a shallow copy and can be corrupted if written to in
the model&#8217;s forward pass.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>module</strong> &#8211; module to be parallelized</li>
<li><strong>device_ids</strong> &#8211; CUDA devices (default: all devices)</li>
<li><strong>output_device</strong> &#8211; device location of output (default: device_ids[0])</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_var</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="utilities">
<h3>Utilities<a class="headerlink" href="#utilities" title="Permalink to this headline">¶</a></h3>
<div class="section" id="clip-grad-norm">
<h4><span class="hidden-section">clip_grad_norm</span><a class="headerlink" href="#clip-grad-norm" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.clip_grad_norm">
<code class="descclassname">torch.nn.utils.</code><code class="descname">clip_grad_norm</code><span class="sig-paren">(</span><em>parameters</em>, <em>max_norm</em>, <em>norm_type=2</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.clip_grad_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Clips gradient norm of an iterable of parameters.</p>
<p>The norm is computed over all gradients together, as if they were
concatenated into a single vector. Gradients are modified in-place.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>parameters</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="index.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a><em>]</em><em></em>) &#8211; an iterable of Variables that will have
gradients normalized</li>
<li><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; max norm of the gradients</li>
<li><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; type of the used p-norm. Can be <code class="docutils literal"><span class="pre">'inf'</span></code> for infinity norm.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Total norm of the parameters (viewed as a single vector).</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="packedsequence">
<h4><span class="hidden-section">PackedSequence</span><a class="headerlink" href="#packedsequence" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.rnn.PackedSequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">PackedSequence</code><span class="sig-paren">(</span><em>_cls</em>, <em>data</em>, <em>batch_sizes</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.rnn.PackedSequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds the data and list of batch_sizes of a packed sequence.</p>
<p>All RNN modules accept packed sequences as inputs.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Instances of this class should never be created manually. They are meant
to be instantiated by functions like <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal"><span class="pre">pack_padded_sequence()</span></code></a>.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>data</strong> (<a class="reference internal" href="index.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; Variable containing packed sequence</li>
<li><strong>batch_sizes</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#list" title="(in Python v2.7)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>]</em><em></em>) &#8211; list of integers holding information about
the batch size at each sequence step</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pack-padded-sequence">
<h4><span class="hidden-section">pack_padded_sequence</span><a class="headerlink" href="#pack-padded-sequence" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.rnn.pack_padded_sequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">pack_padded_sequence</code><span class="sig-paren">(</span><em>input</em>, <em>lengths</em>, <em>batch_first=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.rnn.pack_padded_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Packs a Variable containing padded sequences of variable length.</p>
<p>Input can be of size <code class="docutils literal"><span class="pre">TxBx*</span></code> where T is the length of the longest sequence
(equal to <code class="docutils literal"><span class="pre">lengths[0]</span></code>), B is the batch size, and * is any number of
dimensions (including 0). If <code class="docutils literal"><span class="pre">batch_first</span></code> is True <code class="docutils literal"><span class="pre">BxTx*</span></code> inputs are expected.</p>
<p>The sequences should be sorted by length in a decreasing order, i.e.
<code class="docutils literal"><span class="pre">input[:,0]</span></code> should be the longest sequence, and <code class="docutils literal"><span class="pre">input[:,B-1]</span></code> the
shortest one.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function accept any input that has at least two dimensions. You
can apply it to pack the labels, and use the output of the RNN with
them to compute the loss directly. A Variable can be retrieved from
a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal"><span class="pre">PackedSequence</span></code></a> object by accessing its <code class="docutils literal"><span class="pre">.data</span></code> attribute.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; padded batch of variable length sequences.</li>
<li><strong>lengths</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#list" title="(in Python v2.7)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>]</em><em></em>) &#8211; list of sequences lengths of each batch element.</li>
<li><strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; if True, the input is expected in BxTx*
format.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">a <a class="reference internal" href="#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><code class="xref py py-class docutils literal"><span class="pre">PackedSequence</span></code></a> object</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="pad-packed-sequence">
<h4><span class="hidden-section">pad_packed_sequence</span><a class="headerlink" href="#pad-packed-sequence" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.utils.rnn.pad_packed_sequence">
<code class="descclassname">torch.nn.utils.rnn.</code><code class="descname">pad_packed_sequence</code><span class="sig-paren">(</span><em>sequence</em>, <em>batch_first=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.utils.rnn.pad_packed_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads a packed batch of variable length sequences.</p>
<p>It is an inverse operation to <a class="reference internal" href="#torch.nn.utils.rnn.pack_padded_sequence" title="torch.nn.utils.rnn.pack_padded_sequence"><code class="xref py py-func docutils literal"><span class="pre">pack_padded_sequence()</span></code></a>.</p>
<p>The returned Variable&#8217;s data will be of size TxBx*, where T is the length
of the longest sequence and B is the batch size. If <code class="docutils literal"><span class="pre">batch_first</span></code> is True,
the data will be transposed into BxTx* format.</p>
<p>Batch elements will be ordered decreasingly by their length.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>sequence</strong> (<a class="reference internal" href="index.html#torch.nn.utils.rnn.PackedSequence" title="torch.nn.utils.rnn.PackedSequence"><em>PackedSequence</em></a>) &#8211; batch to pad</li>
<li><strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; if True, the output will be in BxTx* format.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">Tuple of Variable containing the padded sequence, and a list of lengths
of each sequence in the batch.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
<div class="section" id="torch-nn-functional">
<h2>torch.nn.functional<a class="headerlink" href="#torch-nn-functional" title="Permalink to this headline">¶</a></h2>
<div class="section" id="convolution-functions">
<h3>Convolution functions<a class="headerlink" href="#convolution-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id13">
<h4><span class="hidden-section">conv1d</span><a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.conv1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv1d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.conv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D convolution over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code class="xref py py-class docutils literal"><span class="pre">Conv1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; input tensor of shape (minibatch x in_channels x iW)</li>
<li><strong>weight</strong> &#8211; filters of shape (out_channels, in_channels, kW)</li>
<li><strong>bias</strong> &#8211; optional bias of shape (out_channels)</li>
<li><strong>stride</strong> &#8211; the stride of the convolving kernel, default 1</li>
<li><strong>padding</strong> &#8211; implicit zero padding on the input. Can be a single number or
a tuple. Default: 0</li>
<li><strong>dilation</strong> &#8211; the spacing between kernel elements. Default: 1</li>
<li><strong>groups</strong> &#8211; split input into groups, in_channels should be divisible by
the number of groups</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">33</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="id14">
<h4><span class="hidden-section">conv2d</span><a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.conv2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv2d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D convolution over an input image composed of several input
planes.</p>
<p>See <a class="reference internal" href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code class="xref py py-class docutils literal"><span class="pre">Conv2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; input tensor (minibatch x in_channels x iH x iW)</li>
<li><strong>weight</strong> &#8211; filters tensor (out_channels, in_channels/groups, kH, kW)</li>
<li><strong>bias</strong> &#8211; optional bias tensor (out_channels)</li>
<li><strong>stride</strong> &#8211; the stride of the convolving kernel. Can be a single number or
a tuple (sh x sw). Default: 1</li>
<li><strong>padding</strong> &#8211; implicit zero padding on the input. Can be a single number or
a tuple. Default: 0</li>
<li><strong>dilation</strong> &#8211; the spacing between kernel elements. Default: 1</li>
<li><strong>groups</strong> &#8211; split input into groups, in_channels should be divisible by
the number of groups</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="id15">
<h4><span class="hidden-section">conv3d</span><a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.conv3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv3d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.conv3d" title="Permalink to this definition">¶</a></dt>
<dd><dl class="docutils">
<dt>Applies a 3D convolution over an input image composed of several input</dt>
<dd>planes.</dd>
</dl>
<p>See <a class="reference internal" href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code class="xref py py-class docutils literal"><span class="pre">Conv3d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; input tensor of shape (minibatch x in_channels x iT x iH x iW)</li>
<li><strong>weight</strong> &#8211; filters tensor of shape (out_channels, in_channels, kT, kH, kW)</li>
<li><strong>bias</strong> &#8211; optional bias tensor of shape (out_channels)</li>
<li><strong>stride</strong> &#8211; the stride of the convolving kernel. Can be a single number or
a tuple (st x sh x sw). Default: 1</li>
<li><strong>padding</strong> &#8211; implicit zero padding on the input. Can be a single number or
a tuple. Default: 0</li>
<li><strong>dilation</strong> &#8211; the spacing between kernel elements. Default: 1</li>
<li><strong>groups</strong> &#8211; split input into groups, in_channels should be divisible by
the number of groups</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">33</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv3d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv-transpose1d">
<h4><span class="hidden-section">conv_transpose1d</span><a class="headerlink" href="#conv-transpose1d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose1d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.conv_transpose1d" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="conv-transpose2d">
<h4><span class="hidden-section">conv_transpose2d</span><a class="headerlink" href="#conv-transpose2d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose2d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.conv_transpose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D transposed convolution operator over an input image
composed of several input planes, sometimes also called &#8220;deconvolution&#8221;.</p>
<p>See <a class="reference internal" href="#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code class="xref py py-class docutils literal"><span class="pre">ConvTranspose2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; input tensor of shape (minibatch x in_channels x iH x iW)</li>
<li><strong>weight</strong> &#8211; filters of shape (in_channels x out_channels x kH x kW)</li>
<li><strong>bias</strong> &#8211; optional bias of shape (out_channels)</li>
<li><strong>stride</strong> &#8211; the stride of the convolving kernel, a single number or a
tuple (sh x sw). Default: 1</li>
<li><strong>padding</strong> &#8211; implicit zero padding on the input, a single number or a
tuple (padh x padw). Default: 0</li>
<li><strong>groups</strong> &#8211; split input into groups, in_channels should be divisible by
the number of groups</li>
<li><strong>output_padding</strong> &#8211; A zero-padding of 0 &lt;= padding &lt; stride that should be
added to the output. Can be a single number or a tuple. Default: 0</li>
<li><strong>dilation</strong> &#8211; the spacing between kernel elements. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="conv-transpose3d">
<h4><span class="hidden-section">conv_transpose3d</span><a class="headerlink" href="#conv-transpose3d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose3d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.conv_transpose3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D transposed convolution operator over an input image
composed of several input planes, sometimes also called &#8220;deconvolution&#8221;</p>
<p>See <a class="reference internal" href="#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code class="xref py py-class docutils literal"><span class="pre">ConvTranspose3d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; input tensor of shape (minibatch x in_channels x iT x iH x iW)</li>
<li><strong>weight</strong> &#8211; filters of shape (in_channels x out_channels x kH x kW)</li>
<li><strong>bias</strong> &#8211; optional bias of shape (out_channels)</li>
<li><strong>stride</strong> &#8211; the stride of the convolving kernel, a single number or a
tuple (sh x sw). Default: 1</li>
<li><strong>padding</strong> &#8211; implicit zero padding on the input, a single number or a
tuple (padh x padw). Default: 0</li>
<li><strong>output_padding</strong> &#8211; A zero-padding of 0 &lt;= padding &lt; stride that should be
added to the output. Can be a single number or a tuple. Default: 0</li>
<li><strong>groups</strong> &#8211; split input into groups, in_channels should be divisible by
the number of groups</li>
<li><strong>dilation</strong> &#8211; the spacing between kernel elements. Default: 1</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="pooling-functions">
<h3>Pooling functions<a class="headerlink" href="#pooling-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="avg-pool1d">
<h4><span class="hidden-section">avg_pool1d</span><a class="headerlink" href="#avg-pool1d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.avg_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.avg_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D average pooling over an input signal composed of several
input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AvgPool1d" title="torch.nn.AvgPool1d"><code class="xref py py-class docutils literal"><span class="pre">AvgPool1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>kernel_size</strong> &#8211; the size of the window</li>
<li><strong>stride</strong> &#8211; the stride of the window. Default value is <code class="xref py py-attr docutils literal"><span class="pre">kernel_size</span></code></li>
<li><strong>padding</strong> &#8211; implicit zero padding to be added on both sides</li>
<li><strong>ceil_mode</strong> &#8211; when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</li>
<li><strong>count_include_pad</strong> &#8211; when True, will include the zero-padding in the averaging calculation</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">Variable containing:</span>
<span class="go">(0 ,.,.) =</span>
<span class="go">  2  4  6</span>
<span class="go">[torch.FloatTensor of size 1x1x3]</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avg-pool2d">
<h4><span class="hidden-section">avg_pool2d</span><a class="headerlink" href="#avg-pool2d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.avg_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.avg_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies 2D average-pooling operation in kh x kw regions by step size
dh x dw steps. The number of output features is equal to the number of
input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AvgPool2d" title="torch.nn.AvgPool2d"><code class="xref py py-class docutils literal"><span class="pre">AvgPool2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; input tensor (minibatch x in_channels x iH x iW)</li>
<li><strong>kernel_size</strong> &#8211; size of the pooling region, a single number or a
tuple (kh x kw)</li>
<li><strong>stride</strong> &#8211; stride of the pooling operation, a single number or a
tuple (sh x sw). Default is equal to kernel size</li>
<li><strong>padding</strong> &#8211; implicit zero padding on the input, a single number or
a tuple (padh x padw), Default: 0</li>
<li><strong>ceil_mode</strong> &#8211; operation that defines spatial output shape</li>
<li><strong>count_include_pad</strong> &#8211; divide by the number of elements inside the
original non-padded image or kh * kw</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="avg-pool3d">
<h4><span class="hidden-section">avg_pool3d</span><a class="headerlink" href="#avg-pool3d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.avg_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool3d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.avg_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies 3D average-pooling operation in kt x kh x kw regions by step
size kt x dh x dw steps. The number of output features is equal to the
number of input planes / dt.</p>
</dd></dl>

</div>
<div class="section" id="max-pool1d">
<h4><span class="hidden-section">max_pool1d</span><a class="headerlink" href="#max-pool1d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.max_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>ceil_mode=False</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_pool1d" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="max-pool2d">
<h4><span class="hidden-section">max_pool2d</span><a class="headerlink" href="#max-pool2d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.max_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>ceil_mode=False</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_pool2d" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="max-pool3d">
<h4><span class="hidden-section">max_pool3d</span><a class="headerlink" href="#max-pool3d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.max_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool3d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>ceil_mode=False</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_pool3d" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="max-unpool1d">
<h4><span class="hidden-section">max_unpool1d</span><a class="headerlink" href="#max-unpool1d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.max_unpool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool1d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_unpool1d" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="max-unpool2d">
<h4><span class="hidden-section">max_unpool2d</span><a class="headerlink" href="#max-unpool2d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.max_unpool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool2d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_unpool2d" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="max-unpool3d">
<h4><span class="hidden-section">max_unpool3d</span><a class="headerlink" href="#max-unpool3d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.max_unpool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool3d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_unpool3d" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="lp-pool2d">
<h4><span class="hidden-section">lp_pool2d</span><a class="headerlink" href="#lp-pool2d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.lp_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">lp_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.lp_pool2d" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="adaptive-max-pool1d">
<h4><span class="hidden-section">adaptive_max_pool1d</span><a class="headerlink" href="#adaptive-max-pool1d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveMaxPool1d" title="torch.nn.AdaptiveMaxPool1d"><code class="xref py py-class docutils literal"><span class="pre">AdaptiveMaxPool1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> &#8211; the target output size (single integer)</li>
<li><strong>return_indices</strong> &#8211; whether to return pooling indices</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-max-pool2d">
<h4><span class="hidden-section">adaptive_max_pool2d</span><a class="headerlink" href="#adaptive-max-pool2d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em>, <em>return_indices=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveMaxPool2d" title="torch.nn.AdaptiveMaxPool2d"><code class="xref py py-class docutils literal"><span class="pre">AdaptiveMaxPool2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>output_size</strong> &#8211; the target output size (single integer or double-integer tuple)</li>
<li><strong>return_indices</strong> &#8211; whether to return pooling indices</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool1d">
<h4><span class="hidden-section">adaptive_avg_pool1d</span><a class="headerlink" href="#adaptive-avg-pool1d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveAvgPool1d" title="torch.nn.AdaptiveAvgPool1d"><code class="xref py py-class docutils literal"><span class="pre">AdaptiveAvgPool1d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> &#8211; the target output size (single integer)</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool2d">
<h4><span class="hidden-section">adaptive_avg_pool2d</span><a class="headerlink" href="#adaptive-avg-pool2d" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="#torch.nn.AdaptiveAvgPool2d" title="torch.nn.AdaptiveAvgPool2d"><code class="xref py py-class docutils literal"><span class="pre">AdaptiveAvgPool2d</span></code></a> for details and output shape.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>output_size</strong> &#8211; the target output size (single integer or double-integer tuple)</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="non-linear-activation-functions">
<h3>Non-linear activation functions<a class="headerlink" href="#non-linear-activation-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id16">
<h4><span class="hidden-section">threshold</span><a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.threshold">
<code class="descclassname">torch.nn.functional.</code><code class="descname">threshold</code><span class="sig-paren">(</span><em>input</em>, <em>threshold</em>, <em>value</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.threshold" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id17">
<h4><span class="hidden-section">relu</span><a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.relu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">relu</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.relu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id18">
<h4><span class="hidden-section">hardtanh</span><a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.hardtanh">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hardtanh</code><span class="sig-paren">(</span><em>input</em>, <em>min_val=-1.0</em>, <em>max_val=1.0</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.hardtanh" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id19">
<h4><span class="hidden-section">relu6</span><a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.relu6">
<code class="descclassname">torch.nn.functional.</code><code class="descname">relu6</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.relu6" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id20">
<h4><span class="hidden-section">elu</span><a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.elu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">elu</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.0</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.elu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="leaky-relu">
<h4><span class="hidden-section">leaky_relu</span><a class="headerlink" href="#leaky-relu" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.leaky_relu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">leaky_relu</code><span class="sig-paren">(</span><em>input</em>, <em>negative_slope=0.01</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.leaky_relu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id21">
<h4><span class="hidden-section">prelu</span><a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.prelu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">prelu</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.prelu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="rrelu">
<h4><span class="hidden-section">rrelu</span><a class="headerlink" href="#rrelu" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.rrelu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">rrelu</code><span class="sig-paren">(</span><em>input</em>, <em>lower=0.125</em>, <em>upper=0.3333333333333333</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.rrelu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id22">
<h4><span class="hidden-section">logsigmoid</span><a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.logsigmoid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">logsigmoid</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.logsigmoid" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="hardshrink">
<h4><span class="hidden-section">hardshrink</span><a class="headerlink" href="#hardshrink" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.hardshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hardshrink</code><span class="sig-paren">(</span><em>input</em>, <em>lambd=0.5</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.hardshrink" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id23">
<h4><span class="hidden-section">tanhshrink</span><a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.tanhshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">tanhshrink</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.tanhshrink" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id24">
<h4><span class="hidden-section">softsign</span><a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.softsign">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softsign</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.softsign" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id25">
<h4><span class="hidden-section">softplus</span><a class="headerlink" href="#id25" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.softplus">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softplus</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1</em>, <em>threshold=20</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.softplus" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id26">
<h4><span class="hidden-section">softmin</span><a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.softmin">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softmin</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.softmin" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id27">
<h4><span class="hidden-section">softmax</span><a class="headerlink" href="#id27" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.softmax">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softmax</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.softmax" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id28">
<h4><span class="hidden-section">softshrink</span><a class="headerlink" href="#id28" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.softshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softshrink</code><span class="sig-paren">(</span><em>input</em>, <em>lambd=0.5</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.softshrink" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="log-softmax">
<h4><span class="hidden-section">log_softmax</span><a class="headerlink" href="#log-softmax" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.log_softmax">
<code class="descclassname">torch.nn.functional.</code><code class="descname">log_softmax</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.log_softmax" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id29">
<h4><span class="hidden-section">tanh</span><a class="headerlink" href="#id29" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.tanh">
<code class="descclassname">torch.nn.functional.</code><code class="descname">tanh</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.tanh" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="id30">
<h4><span class="hidden-section">sigmoid</span><a class="headerlink" href="#id30" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.sigmoid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">sigmoid</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>
<div class="section" id="normalization-functions">
<h3>Normalization functions<a class="headerlink" href="#normalization-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="batch-norm">
<h4><span class="hidden-section">batch_norm</span><a class="headerlink" href="#batch-norm" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.batch_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">batch_norm</code><span class="sig-paren">(</span><em>input</em>, <em>running_mean</em>, <em>running_var</em>, <em>weight=None</em>, <em>bias=None</em>, <em>training=False</em>, <em>momentum=0.1</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.batch_norm" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>
<div class="section" id="linear-functions">
<h3>Linear functions<a class="headerlink" href="#linear-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id31">
<h4><span class="hidden-section">linear</span><a class="headerlink" href="#id31" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.linear">
<code class="descclassname">torch.nn.functional.</code><code class="descname">linear</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.linear" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>
<div class="section" id="dropout-functions">
<h3>Dropout functions<a class="headerlink" href="#dropout-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id32">
<h4><span class="hidden-section">dropout</span><a class="headerlink" href="#id32" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.dropout">
<code class="descclassname">torch.nn.functional.</code><code class="descname">dropout</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.dropout" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>
<div class="section" id="id33">
<h3>Distance functions<a class="headerlink" href="#id33" title="Permalink to this headline">¶</a></h3>
<div class="section" id="pairwise-distance">
<h4><span class="hidden-section">pairwise_distance</span><a class="headerlink" href="#pairwise-distance" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.pairwise_distance">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pairwise_distance</code><span class="sig-paren">(</span><em>x1</em>, <em>x2</em>, <em>p=2</em>, <em>eps=1e-06</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.pairwise_distance" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the batchwise pairwise distance between vectors v1,v2:</p>
<blockquote>
<div><div class="math">
\[\Vert x \Vert _p := \left( \sum_{i=1}^n  \vert x_i \vert ^ p \right) ^ {1/p}\]</div>
<dl class="docutils">
<dt>Args:</dt>
<dd>x1: first input tensor
x2: second input tensor
p: the norm degree. Default: 2</dd>
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: <span class="math">\((N, D)\)</span> where <cite>D = vector dimension</cite></li>
<li>Output: <span class="math">\((N, 1)\)</span></li>
</ul>
</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pairwise_distance</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div></blockquote>
</dd></dl>

</div>
</div>
<div class="section" id="id34">
<h3>Loss functions<a class="headerlink" href="#id34" title="Permalink to this headline">¶</a></h3>
<div class="section" id="nll-loss">
<h4><span class="hidden-section">nll_loss</span><a class="headerlink" href="#nll-loss" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.nll_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">nll_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.nll_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>The negative log likelihood loss.</p>
<p>See <a class="reference internal" href="#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code class="xref py py-class docutils literal"><span class="pre">NLLLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input</strong> &#8211; <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite> or <cite>(N, C, H, W)</cite> in case of 2D - Loss</li>
<li><strong>target</strong> &#8211; <span class="math">\((N)\)</span> where each value is <cite>0 &lt;= targets[i] &lt;= C-1</cite></li>
<li><strong>weight</strong> (<a class="reference internal" href="index.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) &#8211; a manual rescaling weight given to each
class. If given, has to be a Variable of size &#8220;nclasses&#8221;</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over observations for each minibatch. However, if the field
sizeAverage is set to False, the losses are instead summed
for each minibatch.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><p class="first last"><strong>weight</strong> &#8211; the class-weights given as input to the constructor</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size nBatch x nClasses = 3 x 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; nclasses</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="kl-div">
<h4><span class="hidden-section">kl_div</span><a class="headerlink" href="#kl-div" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.kl_div">
<code class="descclassname">torch.nn.functional.</code><code class="descname">kl_div</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.kl_div" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> Loss.</p>
<p>See <a class="reference internal" href="#torch.nn.KLDivLoss" title="torch.nn.KLDivLoss"><code class="xref py py-class docutils literal"><span class="pre">KLDivLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; Variable of arbitrary shape</li>
<li><strong>target</strong> &#8211; Variable of the same shape as input</li>
<li><strong>size_average</strong> &#8211; if True the output is divided by the number of elements
in input tensor</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="cross-entropy">
<h4><span class="hidden-section">cross_entropy</span><a class="headerlink" href="#cross-entropy" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.cross_entropy">
<code class="descclassname">torch.nn.functional.</code><code class="descname">cross_entropy</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.cross_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>This criterion combines <cite>log_softmax</cite> and <cite>nll_loss</cite> in one single class.</p>
<p>See <a class="reference internal" href="#torch.nn.CrossEntropyLoss" title="torch.nn.CrossEntropyLoss"><code class="xref py py-class docutils literal"><span class="pre">torch.nn.CrossEntropyLoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; Variable <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite></li>
<li><strong>target</strong> &#8211; Variable <span class="math">\((N)\)</span> where each value is <cite>0 &lt;= targets[i] &lt;= C-1</cite></li>
<li><strong>weight</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) &#8211; a manual rescaling weight given to each
class. If given, has to be a Tensor of size &#8220;nclasses&#8221;</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over observations for each minibatch. However, if the field
sizeAverage is set to False, the losses are instead summed
for each minibatch.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="binary-cross-entropy">
<h4><span class="hidden-section">binary_cross_entropy</span><a class="headerlink" href="#binary-cross-entropy" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.binary_cross_entropy">
<code class="descclassname">torch.nn.functional.</code><code class="descname">binary_cross_entropy</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.binary_cross_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that measures the Binary Cross Entropy
between the target and the output:</p>
<p>See <a class="reference internal" href="#torch.nn.BCELoss" title="torch.nn.BCELoss"><code class="xref py py-class docutils literal"><span class="pre">BCELoss</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> &#8211; Variable of arbitrary shape</li>
<li><strong>target</strong> &#8211; Variable of the same shape as input</li>
<li><strong>weight</strong> (<a class="reference internal" href="index.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a><em>, </em><em>optional</em>) &#8211; a manual rescaling weight
if provided it&#8217;s repeated to match input tensor shape</li>
<li><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; By default, the losses are averaged
over observations for each minibatch. However, if the field
sizeAverage is set to False, the losses are instead summed
for each minibatch.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="smooth-l1-loss">
<h4><span class="hidden-section">smooth_l1_loss</span><a class="headerlink" href="#smooth-l1-loss" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.smooth_l1_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">smooth_l1_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.smooth_l1_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>
<div class="section" id="vision-functions">
<h3>Vision functions<a class="headerlink" href="#vision-functions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="pixel-shuffle">
<h4><span class="hidden-section">pixel_shuffle</span><a class="headerlink" href="#pixel-shuffle" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.pixel_shuffle">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pixel_shuffle</code><span class="sig-paren">(</span><em>input</em>, <em>upscale_factor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.pixel_shuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>Rearranges elements in a tensor of shape <code class="docutils literal"><span class="pre">[*,</span> <span class="pre">C*r^2,</span> <span class="pre">H,</span> <span class="pre">W]</span></code> to a
tensor of shape <code class="docutils literal"><span class="pre">[C,</span> <span class="pre">H*r,</span> <span class="pre">W*r]</span></code>.</p>
<p>See <a class="reference internal" href="#torch.nn.PixelShuffle" title="torch.nn.PixelShuffle"><code class="xref py py-class docutils literal"><span class="pre">PixelShuffle</span></code></a> for details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; Input</li>
<li><strong>upscale_factor</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; factor to increase spatial resolution by</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ps</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PixelShuffle</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">ps</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([1, 1, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pad">
<h4><span class="hidden-section">pad</span><a class="headerlink" href="#pad" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="torch.nn.functional.pad">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pad</code><span class="sig-paren">(</span><em>input</em>, <em>pad</em>, <em>mode='constant'</em>, <em>value=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.pad" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads tensor.</p>
<p>Currently only 2D and 3D padding supported.
In case of 4D input tensor pad should be in form (pad_l, pad_r, pad_t, pad_b )
In case of 5D pad should be (pleft, pright, ptop, pbottom, pfront, pback)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>input</strong> (<a class="reference internal" href="index.html#torch.autograd.Variable" title="torch.autograd.Variable"><em>Variable</em></a>) &#8211; 4D or 5D tensor</li>
<li><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#tuple" title="(in Python v2.7)"><em>tuple</em></a>) &#8211; 4-elem or 6-elem tuple</li>
<li><strong>mode</strong> &#8211; &#8216;constant&#8217;, &#8216;reflect&#8217; or &#8216;replicate&#8217;</li>
<li><strong>value</strong> &#8211; fill value for &#8216;constant&#8217; padding</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
<div class="section" id="torch-nn-init">
<h2>torch.nn.init<a class="headerlink" href="#torch-nn-init" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torch.nn.init.uniform">
<code class="descclassname">torch.nn.init.</code><code class="descname">uniform</code><span class="sig-paren">(</span><em>tensor</em>, <em>a=0</em>, <em>b=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.uniform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor or Variable with values drawn from a uniform U(a,b)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; a n-dimension torch.Tensor</li>
<li><strong>a</strong> &#8211; the lower bound of the uniform distribution</li>
<li><strong>b</strong> &#8211; the upper bound of the uniform distribution</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.normal">
<code class="descclassname">torch.nn.init.</code><code class="descname">normal</code><span class="sig-paren">(</span><em>tensor</em>, <em>mean=0</em>, <em>std=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.normal" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor or Variable with values drawn from a normal distribution with the given mean and std</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; a n-dimension torch.Tensor</li>
<li><strong>mean</strong> &#8211; the mean of the normal distribution</li>
<li><strong>std</strong> &#8211; the standard deviation of the normal distribution</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.constant">
<code class="descclassname">torch.nn.init.</code><code class="descname">constant</code><span class="sig-paren">(</span><em>tensor</em>, <em>val</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.constant" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor or Variable with the value <cite>val</cite></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; a n-dimension torch.Tensor</li>
<li><strong>val</strong> &#8211; the value to fill the tensor with</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.xavier_uniform">
<code class="descclassname">torch.nn.init.</code><code class="descname">xavier_uniform</code><span class="sig-paren">(</span><em>tensor</em>, <em>gain=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.xavier_uniform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor or Variable with values according to the method described in &#8220;Understanding the
difficulty of training deep feedforward neural networks&#8221; - Glorot, X. and Bengio, Y., using a uniform
distribution. The resulting tensor will have values sampled from U(-a, a) where a = gain * sqrt(2/(fan_in +
fan_out)) * sqrt(3)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; a n-dimension torch.Tensor</li>
<li><strong>gain</strong> &#8211; an optional scaling factor to be applied</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.xavier_normal">
<code class="descclassname">torch.nn.init.</code><code class="descname">xavier_normal</code><span class="sig-paren">(</span><em>tensor</em>, <em>gain=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.xavier_normal" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor or Variable with values according to the method described in &#8220;Understanding the
difficulty of training deep feedforward neural networks&#8221; - Glorot, X. and Bengio, Y., using a normal
distribution. The resulting tensor will have values sampled from normal distribution with mean=0 and std = gain *
sqrt(2/(fan_in + fan_out))</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; a n-dimension torch.Tensor</li>
<li><strong>gain</strong> &#8211; an optional scaling factor to be applied</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.kaiming_uniform">
<code class="descclassname">torch.nn.init.</code><code class="descname">kaiming_uniform</code><span class="sig-paren">(</span><em>tensor</em>, <em>a=0</em>, <em>mode='fan_in'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.kaiming_uniform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor or Variable with values according to the method described in &#8220;Delving deep into
rectifiers: Surpassing human-level performance on ImageNet classification&#8221; - He, K. et al using a uniform
distribution. The resulting tensor will have values sampled from U(-bound, bound) where bound = sqrt(2/((1 + a^2)
* fan_in)) * sqrt(3)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; a n-dimension torch.Tensor</li>
<li><strong>a</strong> &#8211; the coefficient of the slope of the rectifier used after this layer (0 for ReLU by default)</li>
<li><strong>mode</strong> &#8211; either &#8216;fan_in&#8217; (default) or &#8216;fan_out&#8217;. Choosing <cite>fan_in</cite> preserves the magnitude of the variance of the
weights in the forward pass. Choosing <cite>fan_out</cite> preserves the magnitudes in the backwards pass.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_in&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.kaiming_normal">
<code class="descclassname">torch.nn.init.</code><code class="descname">kaiming_normal</code><span class="sig-paren">(</span><em>tensor</em>, <em>a=0</em>, <em>mode='fan_in'</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.kaiming_normal" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor or Variable with values according to the method described in &#8220;Delving deep into
rectifiers: Surpassing human-level performance on ImageNet classification&#8221; - He, K. et al using a normal
distribution. The resulting tensor will have values sampled from normal distribution with mean=0 and std = sqrt(
2/((1 + a^2) * fan_in))</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; a n-dimension torch.Tensor</li>
<li><strong>a</strong> &#8211; the coefficient of the slope of the rectifier used after this layer (0 for ReLU by default)</li>
<li><strong>mode</strong> &#8211; either &#8216;fan_in&#8217; (default) or &#8216;fan_out&#8217;. Choosing <cite>fan_in</cite> preserves the magnitude of the variance of the
weights in the forward pass. Choosing <cite>fan_out</cite> preserves the magnitudes in the backwards pass.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_out&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.orthogonal">
<code class="descclassname">torch.nn.init.</code><code class="descname">orthogonal</code><span class="sig-paren">(</span><em>tensor</em>, <em>gain=1</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.orthogonal" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the input Tensor or Variable with a (semi) orthogonal matrix. The input tensor must have at least 2
dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened. viewed as 2D
representation with rows equal to the first dimension and columns equal to the product of  as a sparse matrix,
where the non-zero elements will be drawn from a normal distribution with mean=0 and std=`std`. Reference: &#8220;Exact
solutions to the nonlinear dynamics of learning in deep linear neural networks&#8221;-Saxe, A. et al.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; a n-dimension torch.Tensor, where n &gt;= 2</li>
<li><strong>gain</strong> &#8211; optional gain to be applied</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">orthogonal</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.nn.init.sparse">
<code class="descclassname">torch.nn.init.</code><code class="descname">sparse</code><span class="sig-paren">(</span><em>tensor</em>, <em>sparsity</em>, <em>std=0.01</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.init.sparse" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills the 2D input Tensor or Variable as a sparse matrix, where the non-zero elements will be drawn from a
normal distribution with mean=0 and std=`std`.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>tensor</strong> &#8211; a n-dimension torch.Tensor</li>
<li><strong>sparsity</strong> &#8211; The fraction of elements in each column to be set to zero</li>
<li><strong>std</strong> &#8211; the standard deviation of the normal distribution used to generate the non-zero values</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">sparse</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">sparsity</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<span id="document-optim"></span><div class="section" id="module-torch.optim">
<span id="torch-optim"></span><h2>torch.optim<a class="headerlink" href="#module-torch.optim" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="#module-torch.optim" title="torch.optim"><code class="xref py py-mod docutils literal"><span class="pre">torch.optim</span></code></a> is a package implementing various optimization algorithms.
Most commonly used methods are already supported, and the interface is general
enough, so that more sophisticated ones can be also easily integrated in the
future.</p>
<div class="section" id="how-to-use-an-optimizer">
<h3>How to use an optimizer<a class="headerlink" href="#how-to-use-an-optimizer" title="Permalink to this headline">¶</a></h3>
<p>To use <a class="reference internal" href="#module-torch.optim" title="torch.optim"><code class="xref py py-mod docutils literal"><span class="pre">torch.optim</span></code></a> you have to construct an optimizer object, that will hold
the current state and will update the parameters based on the computed gradients.</p>
<div class="section" id="constructing-it">
<h4>Constructing it<a class="headerlink" href="#constructing-it" title="Permalink to this headline">¶</a></h4>
<p>To construct an <a class="reference internal" href="#torch.optim.Optimizer" title="torch.optim.Optimizer"><code class="xref py py-class docutils literal"><span class="pre">Optimizer</span></code></a> you have to give it an iterable containing the
parameters (all should be <a class="reference internal" href="index.html#torch.autograd.Variable" title="torch.autograd.Variable"><code class="xref py py-class docutils literal"><span class="pre">Variable</span></code></a> s) to optimize. Then,
you can specify optimizer-specific options such as the learning rate, weight decay, etc.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">var1</span><span class="p">,</span> <span class="n">var2</span><span class="p">],</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="per-parameter-options">
<h4>Per-parameter options<a class="headerlink" href="#per-parameter-options" title="Permalink to this headline">¶</a></h4>
<p><a class="reference internal" href="#torch.optim.Optimizer" title="torch.optim.Optimizer"><code class="xref py py-class docutils literal"><span class="pre">Optimizer</span></code></a> s also support specifying per-parameter options. To do this, instead
of passing an iterable of <a class="reference internal" href="index.html#torch.autograd.Variable" title="torch.autograd.Variable"><code class="xref py py-class docutils literal"><span class="pre">Variable</span></code></a> s, pass in an iterable of
<a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict" title="(in Python v2.7)"><code class="xref py py-class docutils literal"><span class="pre">dict</span></code></a> s. Each of them will define a separate parameter group, and should contain
a <code class="docutils literal"><span class="pre">params</span></code> key, containing a list of parameters belonging to it. Other keys
should match the keyword arguments accepted by the optimizers, and will be used
as optimization options for this group.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can still pass options as keyword arguments. They will be used as
defaults, in the groups that didn&#8217;t override them. This is useful when you
only want to vary a single option, while keeping all others consistent
between parameter groups.</p>
</div>
<p>For example, this is very useful when one wants to specify per-layer learning rates:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span>
                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">parameters</span><span class="p">()},</span>
                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">3</span><span class="p">}</span>
            <span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
<p>This means that <code class="docutils literal"><span class="pre">model.base</span></code>&#8216;s parameters will use the default learning rate of <code class="docutils literal"><span class="pre">1e-2</span></code>,
<code class="docutils literal"><span class="pre">model.classifier</span></code>&#8216;s parameters will use a learning rate of <code class="docutils literal"><span class="pre">1e-3</span></code>, and a momentum of
<code class="docutils literal"><span class="pre">0.9</span></code> will be used for all parameters</p>
</div>
<div class="section" id="taking-an-optimization-step">
<h4>Taking an optimization step<a class="headerlink" href="#taking-an-optimization-step" title="Permalink to this headline">¶</a></h4>
<p>All optimizers implement a <a class="reference internal" href="#torch.optim.Optimizer.step" title="torch.optim.Optimizer.step"><code class="xref py py-func docutils literal"><span class="pre">step()</span></code></a> method, that updates the
parameters. It can be used in two ways:</p>
<div class="section" id="optimizer-step">
<h5><code class="docutils literal"><span class="pre">optimizer.step()</span></code><a class="headerlink" href="#optimizer-step" title="Permalink to this headline">¶</a></h5>
<p>This is a simplified version supported by most optimizers. The function can be
called once the gradients are computed using e.g.
<a class="reference internal" href="index.html#torch.autograd.Variable.backward" title="torch.autograd.Variable.backward"><code class="xref py py-func docutils literal"><span class="pre">backward()</span></code></a>.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="optimizer-step-closure">
<h5><code class="docutils literal"><span class="pre">optimizer.step(closure)</span></code><a class="headerlink" href="#optimizer-step-closure" title="Permalink to this headline">¶</a></h5>
<p>Some optimization algorithms such as Conjugate Gradient and LBFGS need to
reevaluate the function multiple times, so you have to pass in a closure that
allows them to recompute your model. The closure should clear the gradients,
compute the loss, and return it.</p>
<p>Example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">closure</span><span class="p">():</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="algorithms">
<h3>Algorithms<a class="headerlink" href="#algorithms" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.optim.Optimizer">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">Optimizer</code><span class="sig-paren">(</span><em>params</em>, <em>defaults</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for all optimizers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>params</strong> (<em>iterable</em>) &#8211; an iterable of <code class="xref py py-class docutils literal"><span class="pre">Variable</span></code> s or
<a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict" title="(in Python v2.7)"><code class="xref py py-class docutils literal"><span class="pre">dict</span></code></a> s. Specifies what Variables should be optimized.</li>
<li><strong>defaults</strong> &#8211; (dict): a dict containing default values of optimization
options (used when a parameter group doesn&#8217;t specify them).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torch.optim.Optimizer.load_state_dict">
<code class="descname">load_state_dict</code><span class="sig-paren">(</span><em>state_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Optimizer.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads the optimizer state.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict" title="(in Python v2.7)"><em>dict</em></a>) &#8211; optimizer state. Should be an object returned
from a call to <a class="reference internal" href="#torch.optim.Optimizer.state_dict" title="torch.optim.Optimizer.state_dict"><code class="xref py py-meth docutils literal"><span class="pre">state_dict()</span></code></a>.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.optim.Optimizer.state_dict">
<code class="descname">state_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Optimizer.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the state of the optimizer as a <a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict" title="(in Python v2.7)"><code class="xref py py-class docutils literal"><span class="pre">dict</span></code></a>.</p>
<p>It contains two entries:</p>
<ul class="simple">
<li><dl class="first docutils">
<dt>state - a dict holding current optimization state. Its content</dt>
<dd>differs between optimizer classes.</dd>
</dl>
</li>
<li>param_groups - a dict containig all parameter groups</li>
</ul>
</dd></dl>

<dl class="method">
<dt id="torch.optim.Optimizer.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Optimizer.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step (parameter update).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>closure</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#callable" title="(in Python v2.7)"><em>callable</em></a>) &#8211; A closure that reevaluates the model and
returns the loss. Optional for most optimizers.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.optim.Optimizer.zero_grad">
<code class="descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Optimizer.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Clears the gradients of all optimized <code class="xref py py-class docutils literal"><span class="pre">Variable</span></code> s.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.Adadelta">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">Adadelta</code><span class="sig-paren">(</span><em>params</em>, <em>lr=1.0</em>, <em>rho=0.9</em>, <em>eps=1e-06</em>, <em>weight_decay=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Adadelta" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements Adadelta algorithm.</p>
<p>It has been proposed in <a class="reference external" href="https://arxiv.org/abs/1212.5701">ADADELTA: An Adaptive Learning Rate Method</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>params</strong> (<em>iterable</em>) &#8211; iterable of parameters to optimize or dicts defining
parameter groups</li>
<li><strong>rho</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; coefficient used for computing a running average
of squared gradients (default: 0.9)</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; term added to the denominator to improve
numerical stability (default: 1e-6)</li>
<li><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; coefficient that scale delta before it is applied to the
parameters (default: 1.0)</li>
<li><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; weight decay (L2 penalty) (default: 0)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torch.optim.Adadelta.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Adadelta.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>closure</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#callable" title="(in Python v2.7)"><em>callable</em></a><em>, </em><em>optional</em>) &#8211; A closure that reevaluates the model
and returns the loss.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.Adagrad">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">Adagrad</code><span class="sig-paren">(</span><em>params</em>, <em>lr=0.01</em>, <em>lr_decay=0</em>, <em>weight_decay=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Adagrad" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements Adagrad algorithm.</p>
<p>It has been proposed in <a class="reference external" href="http://jmlr.org/papers/v12/duchi11a.html">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>params</strong> (<em>iterable</em>) &#8211; iterable of parameters to optimize or dicts defining
parameter groups</li>
<li><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; learning rate (default: 1e-2)</li>
<li><strong>lr_decay</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; learning rate decay (default: 0)</li>
<li><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; weight decay (L2 penalty) (default: 0)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torch.optim.Adagrad.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Adagrad.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>closure</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#callable" title="(in Python v2.7)"><em>callable</em></a><em>, </em><em>optional</em>) &#8211; A closure that reevaluates the model
and returns the loss.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.Adam">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">Adam</code><span class="sig-paren">(</span><em>params</em>, <em>lr=0.001</em>, <em>betas=(0.9</em>, <em>0.999)</em>, <em>eps=1e-08</em>, <em>weight_decay=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Adam" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements Adam algorithm.</p>
<p>It has been proposed in <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>params</strong> (<em>iterable</em>) &#8211; iterable of parameters to optimize or dicts defining
parameter groups</li>
<li><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; learning rate (default: 1e-3)</li>
<li><strong>betas</strong> (<em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>]</em><em></em><em>, </em><em>optional</em>) &#8211; coefficients used for computing
running averages of gradient and its square (default: (0.9, 0.999))</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; term added to the denominator to improve
numerical stability (default: 1e-8)</li>
<li><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; weight decay (L2 penalty) (default: 0)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torch.optim.Adam.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Adam.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>closure</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#callable" title="(in Python v2.7)"><em>callable</em></a><em>, </em><em>optional</em>) &#8211; A closure that reevaluates the model
and returns the loss.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.Adamax">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">Adamax</code><span class="sig-paren">(</span><em>params</em>, <em>lr=0.002</em>, <em>betas=(0.9</em>, <em>0.999)</em>, <em>eps=1e-08</em>, <em>weight_decay=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Adamax" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements Adamax algorithm (a variant of Adam based on infinity norm).</p>
<p>It has been proposed in <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>params</strong> (<em>iterable</em>) &#8211; iterable of parameters to optimize or dicts defining
parameter groups</li>
<li><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; learning rate (default: 2e-3)</li>
<li><strong>betas</strong> (<em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>]</em><em></em><em>, </em><em>optional</em>) &#8211; coefficients used for computing
running averages of gradient and its square</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; term added to the denominator to improve
numerical stability (default: 1e-8)</li>
<li><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; weight decay (L2 penalty) (default: 0)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torch.optim.Adamax.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Adamax.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>closure</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#callable" title="(in Python v2.7)"><em>callable</em></a><em>, </em><em>optional</em>) &#8211; A closure that reevaluates the model
and returns the loss.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.ASGD">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">ASGD</code><span class="sig-paren">(</span><em>params</em>, <em>lr=0.01</em>, <em>lambd=0.0001</em>, <em>alpha=0.75</em>, <em>t0=1000000.0</em>, <em>weight_decay=0</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.ASGD" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements Averaged Stochastic Gradient Descent.</p>
<p>It has been proposed in <a class="reference external" href="http://dl.acm.org/citation.cfm?id=131098">Acceleration of stochastic approximation by averaging</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>params</strong> (<em>iterable</em>) &#8211; iterable of parameters to optimize or dicts defining
parameter groups</li>
<li><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; learning rate (default: 1e-2)</li>
<li><strong>lambd</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; decay term (default: 1e-4)</li>
<li><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; power for eta update (default: 0.75)</li>
<li><strong>t0</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; point at which to start averaging (default: 1e6)</li>
<li><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; weight decay (L2 penalty) (default: 0)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torch.optim.ASGD.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.ASGD.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>closure</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#callable" title="(in Python v2.7)"><em>callable</em></a><em>, </em><em>optional</em>) &#8211; A closure that reevaluates the model
and returns the loss.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.LBFGS">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">LBFGS</code><span class="sig-paren">(</span><em>params</em>, <em>lr=1</em>, <em>max_iter=20</em>, <em>max_eval=None</em>, <em>tolerance_grad=1e-05</em>, <em>tolerance_change=1e-09</em>, <em>history_size=100</em>, <em>line_search_fn=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.LBFGS" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements L-BFGS algorithm.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This optimizer doesn&#8217;t support per-parameter options and parameter
groups (there can be only one).</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Right now all parameters have to be on a single device. This will be
improved in the future.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is a very memory intensive optimizer (it requires additional
<code class="docutils literal"><span class="pre">param_bytes</span> <span class="pre">*</span> <span class="pre">(history_size</span> <span class="pre">+</span> <span class="pre">1)</span></code> bytes). If it doesn&#8217;t fit in memory
try reducing the history size, or use a different algorithm.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; learning rate (default: 1)</li>
<li><strong>max_iter</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; maximal number of iterations per optimization step
(default: 20)</li>
<li><strong>max_eval</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; maximal number of function evaluations per optimization
step (default: max_iter * 1.25).</li>
<li><strong>tolerance_grad</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; termination tolerance on first order optimality
(default: 1e-5).</li>
<li><strong>tolerance_change</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; termination tolerance on function value/parameter
changes (default: 1e-9).</li>
<li><strong>history_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; update history size (default: 100).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torch.optim.LBFGS.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.LBFGS.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>closure</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#callable" title="(in Python v2.7)"><em>callable</em></a>) &#8211; A closure that reevaluates the model
and returns the loss.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.RMSprop">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">RMSprop</code><span class="sig-paren">(</span><em>params</em>, <em>lr=0.01</em>, <em>alpha=0.99</em>, <em>eps=1e-08</em>, <em>weight_decay=0</em>, <em>momentum=0</em>, <em>centered=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.RMSprop" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements RMSprop algorithm.</p>
<p>Proposed by G. Hinton in his <a class="reference external" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">course</a>.</p>
<p>The centered version first appears in <a class="reference external" href="https://arxiv.org/pdf/1308.0850v5.pdf">Generating Sequences
With Recurrent Neural Networks</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>params</strong> (<em>iterable</em>) &#8211; iterable of parameters to optimize or dicts defining
parameter groups</li>
<li><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; learning rate (default: 1e-2)</li>
<li><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; momentum factor (default: 0)</li>
<li><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; smoothing constant (default: 0.99)</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; term added to the denominator to improve
numerical stability (default: 1e-8)</li>
<li><strong>centered</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; if True, compute the centered RMSProp,
the gradient is normalized by an estimation of its variance</li>
<li><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; weight decay (L2 penalty) (default: 0)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torch.optim.RMSprop.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.RMSprop.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>closure</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#callable" title="(in Python v2.7)"><em>callable</em></a><em>, </em><em>optional</em>) &#8211; A closure that reevaluates the model
and returns the loss.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.Rprop">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">Rprop</code><span class="sig-paren">(</span><em>params</em>, <em>lr=0.01</em>, <em>etas=(0.5</em>, <em>1.2)</em>, <em>step_sizes=(1e-06</em>, <em>50)</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Rprop" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the resilient backpropagation algorithm.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>params</strong> (<em>iterable</em>) &#8211; iterable of parameters to optimize or dicts defining
parameter groups</li>
<li><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; learning rate (default: 1e-2)</li>
<li><strong>etas</strong> (<em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>]</em><em></em><em>, </em><em>optional</em>) &#8211; pair of (etaminus, etaplis), that
are multiplicative increase and decrease factors (default: (0.5, 1.2))</li>
<li><strong>step_sizes</strong> (<em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>]</em><em></em><em>, </em><em>optional</em>) &#8211; a pair of minimal and
maximal allowed step sizes (default: (1e-6, 50))</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torch.optim.Rprop.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.Rprop.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>closure</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#callable" title="(in Python v2.7)"><em>callable</em></a><em>, </em><em>optional</em>) &#8211; A closure that reevaluates the model
and returns the loss.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.optim.SGD">
<em class="property">class </em><code class="descclassname">torch.optim.</code><code class="descname">SGD</code><span class="sig-paren">(</span><em>params</em>, <em>lr=&lt;object object&gt;</em>, <em>momentum=0</em>, <em>dampening=0</em>, <em>weight_decay=0</em>, <em>nesterov=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.SGD" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements stochastic gradient descent (optionally with momentum).</p>
<p>Nesterov momentum is based on the formula from
<a class="reference external" href="http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf">On the importance of initialization and momentum in deep learning</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>params</strong> (<em>iterable</em>) &#8211; iterable of parameters to optimize or dicts defining
parameter groups</li>
<li><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a>) &#8211; learning rate</li>
<li><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; momentum factor (default: 0)</li>
<li><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; weight decay (L2 penalty) (default: 0)</li>
<li><strong>dampening</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#float" title="(in Python v2.7)"><em>float</em></a><em>, </em><em>optional</em>) &#8211; dampening for momentum (default: 0)</li>
<li><strong>nesterov</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; enables Nesterov momentum (default: False)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The implementation of SGD with Momentum/Nesterov subtly differs from
Sutskever et. al. and implementations in some other frameworks.</p>
<p>Considering the specific case of Momentum, the update can be written as</p>
<div class="math">
\[\begin{split}v = \rho * v + g \\
p = p - lr * v\end{split}\]</div>
<p>where p, g, v and <span class="math">\(\rho\)</span> denote the parameters, gradient, velocity, and
momentum respectively.</p>
<p>This is in constrast to Sutskever et. al. and
other frameworks which employ an update of the form</p>
<div class="math">
\[\begin{split}v = \rho * v + lr * g \\
p = p - v\end{split}\]</div>
<p class="last">The Nesterov version is analogously modified.</p>
</div>
<dl class="method">
<dt id="torch.optim.SGD.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>closure=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.optim.SGD.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>closure</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#callable" title="(in Python v2.7)"><em>callable</em></a><em>, </em><em>optional</em>) &#8211; A closure that reevaluates the model
and returns the loss.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
</div>
<span id="document-autograd"></span><div class="section" id="module-torch.autograd">
<span id="automatic-differentiation-package-torch-autograd"></span><h2>Automatic differentiation package - torch.autograd<a class="headerlink" href="#module-torch.autograd" title="Permalink to this headline">¶</a></h2>
<p>torch.autograd provides classes and functions implementing automatic
differentiation of arbitrary scalar valued functions. It requires minimal
changes to the existing code - you only need to wrap all tensors in
<a class="reference internal" href="#torch.autograd.Variable" title="torch.autograd.Variable"><code class="xref py py-class docutils literal"><span class="pre">Variable</span></code></a> objects.</p>
<dl class="function">
<dt id="torch.autograd.backward">
<code class="descclassname">torch.autograd.</code><code class="descname">backward</code><span class="sig-paren">(</span><em>variables</em>, <em>grad_variables</em>, <em>retain_variables=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the sum of gradients of given variables w.r.t. graph leaves.</p>
<p>The graph is differentiated using the chain rule. If any of <code class="docutils literal"><span class="pre">variables</span></code>
are non-scalar (i.e. their data has more than one element) and require
gradient, the function additionaly requires specifying <code class="docutils literal"><span class="pre">grad_variables</span></code>.
It should be a sequence of matching length, that containins gradient of
the differentiated function w.r.t. corresponding variables (<code class="docutils literal"><span class="pre">None</span></code> is an
acceptable value for all variables that don&#8217;t need gradient tensors).</p>
<p>This function accumulates gradients in the leaves - you might need to zero
them before calling it.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>variables</strong> (<em>sequence of Variable</em>) &#8211; Variables of which the derivative will be
computed.</li>
<li><strong>grad_variables</strong> (<em>sequence of Tensor</em>) &#8211; Gradients w.r.t. each element of
corresponding variables. Required only for non-scalar variables that
require gradient.</li>
<li><strong>retain_variables</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If <code class="docutils literal"><span class="pre">True</span></code>, buffers necessary for computing
gradients won&#8217;t be freed after use. It is only necessary to
specify <code class="docutils literal"><span class="pre">True</span></code> if you want to differentiate some subgraph multiple
times.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<div class="section" id="variable">
<h3>Variable<a class="headerlink" href="#variable" title="Permalink to this headline">¶</a></h3>
<div class="section" id="api-compatibility">
<h4>API compatibility<a class="headerlink" href="#api-compatibility" title="Permalink to this headline">¶</a></h4>
<p>Variable API is nearly the same as regular Tensor API (with the exception
of a couple in-place methods, that would overwrite inputs required for
gradient computation). In most cases Tensors can be safely replaced with
Variables and the code will remain to work just fine. Because of this,
we&#8217;re not documenting all the operations on variables, and you should
refer to <a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal"><span class="pre">torch.Tensor</span></code></a> docs for this purpose.</p>
</div>
<div class="section" id="in-place-operations-on-variables">
<h4>In-place operations on Variables<a class="headerlink" href="#in-place-operations-on-variables" title="Permalink to this headline">¶</a></h4>
<p>Supporting in-place operations in autograd is a hard matter, and we discourage
their use in most cases. Autograd&#8217;s aggressive buffer freeing and reuse makes
it very efficient and there are very few occasions when in-place operations
actually lower memory usage by any significant amount. Unless you&#8217;re operating
under heavy memory pressure, you might never need to use them.</p>
</div>
<div class="section" id="in-place-correctness-checks">
<h4>In-place correctness checks<a class="headerlink" href="#in-place-correctness-checks" title="Permalink to this headline">¶</a></h4>
<p>All <a class="reference internal" href="#torch.autograd.Variable" title="torch.autograd.Variable"><code class="xref py py-class docutils literal"><span class="pre">Variable</span></code></a> s keep track of in-place operations applied to them, and
if the implementation detects that a variable was saved for backward in one of
the functions, but it was modified in-place afterwards, an error will be raised
once backward pass is started. This ensures that if you&#8217;re using in-place
functions and not seing any errors, you can be sure that the computed gradients
are correct.</p>
<dl class="class">
<dt id="torch.autograd.Variable">
<em class="property">class </em><code class="descclassname">torch.autograd.</code><code class="descname">Variable</code><a class="headerlink" href="#torch.autograd.Variable" title="Permalink to this definition">¶</a></dt>
<dd><p>Wraps a tensor and records the operations applied to it.</p>
<p>Variable is a thin wrapper around a Tensor object, that also holds
the gradient w.r.t. to it, and a reference to a function that created it.
This reference allows retracing the whole chain of operations that
created the data. If the Variable has been created by the user, its creator
will be <code class="docutils literal"><span class="pre">None</span></code> and we call such objects <em>leaf</em> Variables.</p>
<p>Since autograd only supports scalar valued function differentiation, grad
size always matches the data size. Also, grad is normally only allocated
for leaf variables, and will be always zero otherwise.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first simple">
<li><strong>data</strong> &#8211; Wrapped tensor of any type.</li>
<li><strong>grad</strong> &#8211; Variable holding the gradient of type and location matching
the <code class="docutils literal"><span class="pre">.data</span></code>.  This attribute is lazily allocated and can&#8217;t
be reassigned.</li>
<li><strong>requires_grad</strong> &#8211; Boolean indicating whether the Variable has been
created by a subgraph containing any Variable, that requires it.
See <a class="reference internal" href="index.html#excluding-subgraphs"><span class="std std-ref">Excluding subgraphs from backward</span></a> for more details.
Can be changed only on leaf Variables.</li>
<li><strong>volatile</strong> &#8211; Boolean indicating that the Variable should be used in
inference mode, i.e. don&#8217;t save the history. See
<a class="reference internal" href="index.html#excluding-subgraphs"><span class="std std-ref">Excluding subgraphs from backward</span></a> for more details.
Can be changed only on leaf Variables.</li>
<li><strong>creator</strong> &#8211; Function of which the variable was an output. For leaf
(user created) variables it&#8217;s <code class="docutils literal"><span class="pre">None</span></code>. Read-only attribute.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>data</strong> (<em>any tensor class</em>) &#8211; Tensor to wrap.</li>
<li><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; Value of the requires_grad flag. <strong>Keyword only.</strong></li>
<li><strong>volatile</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; Value of the volatile flag. <strong>Keyword only.</strong></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torch.autograd.Variable.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>gradient=None</em>, <em>retain_variables=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.Variable.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the gradient of current variable w.r.t. graph leaves.</p>
<p>The graph is differentiated using the chain rule. If the variable is
non-scalar (i.e. its data has more than one element) and requires
gradient, the function additionaly requires specifying <code class="docutils literal"><span class="pre">gradient</span></code>.
It should be a tensor of matching type and location, that contains
the gradient of the differentiated function w.r.t. <code class="docutils literal"><span class="pre">self</span></code>.</p>
<p>This function accumulates gradients in the leaves - you might need to zero
them before calling it.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>gradient</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Gradient of the differentiated function
w.r.t. the data. Required only if the data has more than one
element. Type and location should match these of <code class="docutils literal"><span class="pre">self.data</span></code>.</li>
<li><strong>retain_variables</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If <code class="docutils literal"><span class="pre">True</span></code>, buffers necessary for computing
gradients won&#8217;t be freed after use. It is only necessary to
specify <code class="docutils literal"><span class="pre">True</span></code> if you want to differentiate some subgraph multiple
times (in some cases it will be much more efficient to use
<cite>autograd.backward</cite>).</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.autograd.Variable.detach">
<code class="descname">detach</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.Variable.detach" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new Variable, detached from the current graph.</p>
<p>Result will never require gradient. If the input is volatile, the output
will be volatile too.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Returned Variable uses the same data tensor, as the original one, and
in-place modifications on either of them will be seen, and may trigger
errors in correctness checks.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.autograd.Variable.detach_">
<code class="descname">detach_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.Variable.detach_" title="Permalink to this definition">¶</a></dt>
<dd><p>Detaches the Variable from the graph that created it, making it a leaf.</p>
</dd></dl>

<dl class="method">
<dt id="torch.autograd.Variable.register_hook">
<code class="descname">register_hook</code><span class="sig-paren">(</span><em>hook</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.Variable.register_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook.</p>
<p>The hook will be called every time a gradient with respect to the
variable is computed. The hook should have the following signature:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Variable</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The hook should not modify its argument, but it can optionally return
a new gradient which will be used in place of <code class="xref py py-attr docutils literal"><span class="pre">grad</span></code>.</p>
<p>This function returns a handle with a method <code class="docutils literal"><span class="pre">handle.remove()</span></code>
that removes the hook from the module.</p>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="k">lambda</span> <span class="n">grad</span><span class="p">:</span> <span class="n">grad</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># double the gradient</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
<span class="go"> 2</span>
<span class="go"> 2</span>
<span class="go"> 2</span>
<span class="go">[torch.FloatTensor of size 3]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>  <span class="c1"># removes the hook</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.autograd.Variable.reinforce">
<code class="descname">reinforce</code><span class="sig-paren">(</span><em>reward</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.Variable.reinforce" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a reward obtained as a result of a stochastic process.</p>
<p>Differentiating stochastic nodes requires providing them with reward
value. If your graph contains any stochastic operations, you should
call this function on their outputs. Otherwise an error will be raised.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>reward</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; Tensor with per-element rewards. It has to match
the device location and shape of Variable&#8217;s data.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="function">
<h3><span class="hidden-section">Function</span><a class="headerlink" href="#function" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.autograd.Function">
<em class="property">class </em><code class="descclassname">torch.autograd.</code><code class="descname">Function</code><a class="headerlink" href="#torch.autograd.Function" title="Permalink to this definition">¶</a></dt>
<dd><p>Records operation history and defines formulas for differentiating ops.</p>
<p>Every operation performed on <a class="reference internal" href="#torch.autograd.Variable" title="torch.autograd.Variable"><code class="xref py py-class docutils literal"><span class="pre">Variable</span></code></a> s creates a new function
object, that performs the computation, and records that it happened.
The history is retained in the form of a DAG of functions, with edges
denoting data dependencies (<code class="docutils literal"><span class="pre">input</span> <span class="pre">&lt;-</span> <span class="pre">output</span></code>). Then, when backward is
called, the graph is processed in the topological ordering, by calling
<a class="reference internal" href="#torch.autograd.backward" title="torch.autograd.backward"><code class="xref py py-func docutils literal"><span class="pre">backward()</span></code></a> methods of each <a class="reference internal" href="#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal"><span class="pre">Function</span></code></a> object, and passing
returned gradients on to next <a class="reference internal" href="#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal"><span class="pre">Function</span></code></a> s.</p>
<p>Normally, the only way users interact with functions is by creating
subclasses and defining new operations. This is a recommended way of
extending torch.autograd.</p>
<p>Since Function logic is a hotspot in most scripts, almost all of it
was moved to our C backend, to ensure that the framework overhead is
minimal.</p>
<p>Each function is meant to be used only once (in the forward pass).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>saved_tensors</strong> &#8211; Tuple of Tensors that were saved in the call to
<a class="reference internal" href="#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-func docutils literal"><span class="pre">forward()</span></code></a>.</li>
<li><strong>needs_input_grad</strong> &#8211; Tuple of booleans of length <code class="xref py py-attr docutils literal"><span class="pre">num_inputs</span></code>,
indicating whether a given input requires gradient. This can be
used to optimize buffers saved for backward, and ignoring gradient
computation in <a class="reference internal" href="#torch.autograd.Function.backward" title="torch.autograd.Function.backward"><code class="xref py py-func docutils literal"><span class="pre">backward()</span></code></a>.</li>
<li><strong>num_inputs</strong> &#8211; Number of inputs given to <a class="reference internal" href="#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-func docutils literal"><span class="pre">forward()</span></code></a>.</li>
<li><strong>num_outputs</strong> &#8211; Number of tensors returned by <a class="reference internal" href="#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-func docutils literal"><span class="pre">forward()</span></code></a>.</li>
<li><strong>requires_grad</strong> &#8211; Boolean indicating whether the <a class="reference internal" href="#torch.autograd.backward" title="torch.autograd.backward"><code class="xref py py-func docutils literal"><span class="pre">backward()</span></code></a> will
ever need to be called.</li>
<li><strong>previous_functions</strong> &#8211; Tuple of (int, Function) pairs of length
<code class="xref py py-attr docutils literal"><span class="pre">num_inputs</span></code>. Each entry contains a reference to a
<a class="reference internal" href="#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal"><span class="pre">Function</span></code></a> that created corresponding input, and an index
of the previous function output that&#8217;s been used.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torch.autograd.Function.backward">
<code class="descname">backward</code><span class="sig-paren">(</span><em>*grad_output</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.Function.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines a formula for differentiating the operation.</p>
<p>This function is to be overriden by all subclasses.</p>
<p>All arguments are tensors. It has to accept exactly as many arguments,
as many outputs did <a class="reference internal" href="#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-func docutils literal"><span class="pre">forward()</span></code></a> return, and it should return as
many tensors, as there were inputs to <a class="reference internal" href="#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-func docutils literal"><span class="pre">forward()</span></code></a>. Each argument
is the gradient w.r.t the given output, and each returned value should
be the gradient w.r.t. the corresponding input.</p>
</dd></dl>

<dl class="method">
<dt id="torch.autograd.Function.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>*input</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.Function.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs the operation.</p>
<p>This function is to be overriden by all subclasses.</p>
<p>It can take and return an arbitrary number of tensors.</p>
</dd></dl>

<dl class="method">
<dt id="torch.autograd.Function.mark_dirty">
<code class="descname">mark_dirty</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.Function.mark_dirty" title="Permalink to this definition">¶</a></dt>
<dd><p>Marks given tensors as modified in an in-place operation.</p>
<p><strong>This should be called at most once, only from inside the</strong>
<a class="reference internal" href="#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-func docutils literal"><span class="pre">forward()</span></code></a> <strong>method, and all arguments should be inputs.</strong></p>
<p>Every tensor that&#8217;s been modified in-place in a call to <a class="reference internal" href="#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-func docutils literal"><span class="pre">forward()</span></code></a>
should be given to this function, to ensure correcness of our checks.
It doesn&#8217;t matter wheter the function is called before or after
modification.</p>
</dd></dl>

<dl class="method">
<dt id="torch.autograd.Function.mark_non_differentiable">
<code class="descname">mark_non_differentiable</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.Function.mark_non_differentiable" title="Permalink to this definition">¶</a></dt>
<dd><p>Marks outputs as non-differentiable.</p>
<p><strong>This should be called at most once, only from inside the</strong>
<a class="reference internal" href="#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-func docutils literal"><span class="pre">forward()</span></code></a> <strong>method, and all arguments should be outputs.</strong></p>
<p>This will mark outputs as not requiring gradients, increasing the
efficiency of backward computation. You still need to accept a gradient
for each output in <a class="reference internal" href="#torch.autograd.Function.backward" title="torch.autograd.Function.backward"><code class="xref py py-meth docutils literal"><span class="pre">backward()</span></code></a>, but it&#8217;s always going to
be <code class="docutils literal"><span class="pre">None</span></code>.</p>
<p>This is used e.g. for indices returned from a max <a class="reference internal" href="#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal"><span class="pre">Function</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="torch.autograd.Function.mark_shared_storage">
<code class="descname">mark_shared_storage</code><span class="sig-paren">(</span><em>*pairs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.Function.mark_shared_storage" title="Permalink to this definition">¶</a></dt>
<dd><p>Marks that given pairs of distinct tensors are sharing storage.</p>
<p><strong>This should be called at most once, only from inside the</strong>
<a class="reference internal" href="#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-func docutils literal"><span class="pre">forward()</span></code></a> <strong>method, and all arguments should be pairs of
(input, output).</strong></p>
<p>If some of the outputs are going to be tensors sharing storage with
some of the inputs, all pairs of (input_arg, output_arg) should be
given to this function, to ensure correctness checking of in-place
modification. The only exception is when an output is exactly the same
tensor as input (e.g. in-place ops). In such case it&#8217;s easy to conclude
that they&#8217;re sharing data, so we don&#8217;t require specifying such
dependencies.</p>
<p>This function is not needed in most functions. It&#8217;s primarily used in
indexing and transpose ops.</p>
</dd></dl>

<dl class="method">
<dt id="torch.autograd.Function.save_for_backward">
<code class="descname">save_for_backward</code><span class="sig-paren">(</span><em>*tensors</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.autograd.Function.save_for_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves given tensors for a future call to <a class="reference internal" href="#torch.autograd.Function.backward" title="torch.autograd.Function.backward"><code class="xref py py-func docutils literal"><span class="pre">backward()</span></code></a>.</p>
<p><strong>This should be called at most once, and only from inside the</strong>
<a class="reference internal" href="#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-func docutils literal"><span class="pre">forward()</span></code></a> <strong>method.</strong></p>
<p>Later, saved tensors can be accessed through the <code class="xref py py-attr docutils literal"><span class="pre">saved_tensors</span></code>
attribute. Before returning them to the user, a check is made, to
ensure they weren&#8217;t used in any in-place operation that modified
their content.</p>
<p>Arguments can also be <code class="docutils literal"><span class="pre">None</span></code>.</p>
</dd></dl>

</dd></dl>

</div>
</div>
<span id="document-multiprocessing"></span><div class="section" id="module-torch.multiprocessing">
<span id="multiprocessing-package-torch-multiprocessing"></span><h2>Multiprocessing package - torch.multiprocessing<a class="headerlink" href="#module-torch.multiprocessing" title="Permalink to this headline">¶</a></h2>
<p>torch.multiprocessing is a wrapper around the native <a class="reference external" href="https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing" title="(in Python v2.7)"><code class="xref py py-mod docutils literal"><span class="pre">multiprocessing</span></code></a>
module. It registers custom reducers, that use shared memory to provide shared
views on the same data in different processes. Once the tensor/storage is moved
to shared_memory (see <a class="reference internal" href="index.html#torch.Tensor.share_memory_" title="torch.Tensor.share_memory_"><code class="xref py py-func docutils literal"><span class="pre">share_memory_()</span></code></a>), it will be possible
to send it to other processes without making any copies.</p>
<p>The API is 100% compatible with the original module - it&#8217;s enough to change
<code class="docutils literal"><span class="pre">import</span> <span class="pre">multiprocessing</span></code> to <code class="docutils literal"><span class="pre">import</span> <span class="pre">torch.multiprocessing</span></code> to have all the
tensors sent through the queues or shared via other mechanisms, moved to shared
memory.</p>
<p>Because of the similarity of APIs we do not document most of this package
contents, and we recommend referring to very good docs of the original module.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">If the main process exits abruptly (e.g. because of an incoming signal),
Python&#8217;s <code class="docutils literal"><span class="pre">multiprocessing</span></code> sometimes fails to clean up its children.
It&#8217;s a known caveat, so if you&#8217;re seeing any resource leaks after
interrupting the interpreter, it probably means that this has just happened
to you.</p>
</div>
<div class="section" id="strategy-management">
<h3>Strategy management<a class="headerlink" href="#strategy-management" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.multiprocessing.get_all_sharing_strategies">
<code class="descclassname">torch.multiprocessing.</code><code class="descname">get_all_sharing_strategies</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.multiprocessing.get_all_sharing_strategies" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a set of sharing strategies supported on a current system.</p>
</dd></dl>

<dl class="function">
<dt id="torch.multiprocessing.get_sharing_strategy">
<code class="descclassname">torch.multiprocessing.</code><code class="descname">get_sharing_strategy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.multiprocessing.get_sharing_strategy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current strategy for sharing CPU tensors.</p>
</dd></dl>

<dl class="function">
<dt id="torch.multiprocessing.set_sharing_strategy">
<code class="descclassname">torch.multiprocessing.</code><code class="descname">set_sharing_strategy</code><span class="sig-paren">(</span><em>new_strategy</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.multiprocessing.set_sharing_strategy" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the strategy for sharing CPU tensors.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>new_strategy</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; Name of the selected strategy. Should be one of
the values returned by <a class="reference internal" href="#torch.multiprocessing.get_all_sharing_strategies" title="torch.multiprocessing.get_all_sharing_strategies"><code class="xref py py-func docutils literal"><span class="pre">get_all_sharing_strategies()</span></code></a>.</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="sharing-cuda-tensors">
<h3>Sharing CUDA tensors<a class="headerlink" href="#sharing-cuda-tensors" title="Permalink to this headline">¶</a></h3>
<p>Sharing CUDA tensors between processes is supported only in Python 3, using
a <code class="docutils literal"><span class="pre">spawn</span></code> or <code class="docutils literal"><span class="pre">forkserver</span></code> start methods. <a class="reference external" href="https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing" title="(in Python v2.7)"><code class="docutils literal"><span class="pre">multiprocessing</span></code></a> in
Python 2 can only create subprocesses using <code class="docutils literal"><span class="pre">fork</span></code>, and it&#8217;s not supported
by the CUDA runtime.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">CUDA API requires that the allocation exported to other processes remains
valid as long as it&#8217;s used by them. You should be careful and ensure that
CUDA tensors you shared don&#8217;t go out of scope as long as it&#8217;s necessary.
This shouldn&#8217;t be a problem for sharing model parameters, but passing other
kinds of data should be done with care. Note that this restriction doesn&#8217;t
apply to shared CPU memory.</p>
</div>
</div>
<div class="section" id="sharing-strategies">
<h3>Sharing strategies<a class="headerlink" href="#sharing-strategies" title="Permalink to this headline">¶</a></h3>
<p>This section provides a brief overview into how different sharing strategies
work. Note that it applies only to CPU tensor - CUDA tensors will always use
the CUDA API, as that&#8217;s the only way they can be shared.</p>
<div class="section" id="file-descriptor-file-descriptor">
<h4>File descriptor - <code class="docutils literal"><span class="pre">file_descriptor</span></code><a class="headerlink" href="#file-descriptor-file-descriptor" title="Permalink to this headline">¶</a></h4>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is the default strategy (except for macOS and OS X where it&#8217;s not
supported).</p>
</div>
<p>This strategy will use file descriptors as shared memory handles. Whenever a
storage is moved to shared memory, a file descriptor obtained from <code class="docutils literal"><span class="pre">shm_open</span></code>
is cached with the object, and when it&#8217;s going to be sent to other processes,
the file descriptor will be transferred (e.g. via UNIX sockets) to it. The
receiver will also cache the file descriptor and <code class="docutils literal"><span class="pre">mmap</span></code> it, to obtain a shared
view onto the storage data.</p>
<p>Note that if there will be a lot of tensors shared, this strategy will keep a
large number of file descriptors open most of the time. If your system has low
limits for the number of open file descriptors, and you can&#8217;t rise them, you
should use the <code class="docutils literal"><span class="pre">file_system</span></code> strategy.</p>
</div>
<div class="section" id="file-system-file-system">
<h4>File system - <code class="docutils literal"><span class="pre">file_system</span></code><a class="headerlink" href="#file-system-file-system" title="Permalink to this headline">¶</a></h4>
<p>This strategy will use file names given to <code class="docutils literal"><span class="pre">shm_open</span></code> to identify the shared
memory regions. This has a benefit of not requiring the implementation to cache
the file descriptors obtained from it, but at the same time is prone to shared
memory leaks. The file can&#8217;t be deleted right after its creation, because other
processes need to access it to open their views. If the processes fatally
crash, or are killed, and don&#8217;t call the storage destructors, the files will
remain in the system. This is very serious, because they keep using up the
memory until the system is restarted, or they&#8217;re freed manually.</p>
<p>To counter the problem of shared memory file leaks, <a class="reference internal" href="#module-torch.multiprocessing" title="torch.multiprocessing"><code class="xref py py-mod docutils literal"><span class="pre">torch.multiprocessing</span></code></a>
will spawn a daemon named <code class="docutils literal"><span class="pre">torch_shm_manager</span></code> that will isolate itself from
the current process group, and will keep track of all shared memory allocations.
Once all processes connected to it exit, it will wait a moment to ensure there
will be no new connections, and will iterate over all shared memory files
allocated by the group. If it finds that any of them still exist, they will be
deallocated. We&#8217;ve tested this method and it prooved to be robust to various
failures. Still, if your system has high enough limits, and <code class="docutils literal"><span class="pre">file_descriptor</span></code>
is a supported strategy, we do not recommend switching to this one.</p>
</div>
</div>
</div>
<span id="document-legacy"></span><div class="section" id="module-torch.legacy">
<span id="legacy-package-torch-legacy"></span><h2>Legacy package - torch.legacy<a class="headerlink" href="#module-torch.legacy" title="Permalink to this headline">¶</a></h2>
<p>Package containing code ported from Lua torch.</p>
<p>To make it possible to work with existing models and ease the transition
for current Lua torch users, we&#8217;ve created this package. You can find the
<code class="docutils literal"><span class="pre">nn</span></code> code in <code class="docutils literal"><span class="pre">torch.legacy.nn</span></code>, and <code class="docutils literal"><span class="pre">optim</span></code> in <code class="docutils literal"><span class="pre">torch.legacy.optim</span></code>.
The APIs should exactly match Lua torch.</p>
</div>
<span id="document-cuda"></span><div class="section" id="module-torch.cuda">
<span id="torch-cuda"></span><h2>torch.cuda<a class="headerlink" href="#module-torch.cuda" title="Permalink to this headline">¶</a></h2>
<p>This package adds support for CUDA tensor types, that implement the same
function as CPU tensors, but they utilize GPUs for computation.</p>
<p>It is lazily initialized, so you can always import it, and use
<a class="reference internal" href="#torch.cuda.is_available" title="torch.cuda.is_available"><code class="xref py py-func docutils literal"><span class="pre">is_available()</span></code></a> to determine if your system supports CUDA.</p>
<p><a class="reference internal" href="index.html#cuda-semantics"><span class="std std-ref">CUDA semantics</span></a> has more details about working with CUDA.</p>
<dl class="function">
<dt id="torch.cuda.current_blas_handle">
<code class="descclassname">torch.cuda.</code><code class="descname">current_blas_handle</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.current_blas_handle" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns cublasHandle_t pointer to current cuBLAS handle</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.current_device">
<code class="descclassname">torch.cuda.</code><code class="descname">current_device</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.current_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the index of a currently selected device.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.current_stream">
<code class="descclassname">torch.cuda.</code><code class="descname">current_stream</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.current_stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a currently selected <a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><code class="xref py py-class docutils literal"><span class="pre">Stream</span></code></a>.</p>
</dd></dl>

<dl class="class">
<dt id="torch.cuda.device">
<em class="property">class </em><code class="descclassname">torch.cuda.</code><code class="descname">device</code><span class="sig-paren">(</span><em>idx</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.device" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that changes the selected device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>idx</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; device index to select. It&#8217;s a no-op if this argument
is negative.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.device_count">
<code class="descclassname">torch.cuda.</code><code class="descname">device_count</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.device_count" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of GPUs available.</p>
</dd></dl>

<dl class="class">
<dt id="torch.cuda.device_of">
<em class="property">class </em><code class="descclassname">torch.cuda.</code><code class="descname">device_of</code><span class="sig-paren">(</span><em>obj</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.device_of" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that changes the current device to that of given object.</p>
<p>You can use both tensors and storages as arguments. If a given object is
not allocated on a GPU, this is a no-op.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>obj</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><em>Storage</em>) &#8211; object allocated on the selected device.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.is_available">
<code class="descclassname">torch.cuda.</code><code class="descname">is_available</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.is_available" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a bool indicating if CUDA is currently available.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.set_device">
<code class="descclassname">torch.cuda.</code><code class="descname">set_device</code><span class="sig-paren">(</span><em>device</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.set_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the current device.</p>
<p>Usage of this function is discouraged in favor of <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref any py py-class docutils literal"><span class="pre">device</span></code></a>. In most
cases it&#8217;s better to use <code class="docutils literal"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> environmental variable.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>device</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; selected device. This function is a no-op if this
argument is negative.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.stream">
<code class="descclassname">torch.cuda.</code><code class="descname">stream</code><span class="sig-paren">(</span><em>stream</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that selects a given stream.</p>
<p>All CUDA kernels queued within its context will be enqueued on a selected
stream.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>stream</strong> (<a class="reference internal" href="index.html#torch.cuda.Stream" title="torch.cuda.Stream"><em>Stream</em></a>) &#8211; selected stream. This manager is a no-op if it&#8217;s
<code class="docutils literal"><span class="pre">None</span></code>.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.synchronize">
<code class="descclassname">torch.cuda.</code><code class="descname">synchronize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.synchronize" title="Permalink to this definition">¶</a></dt>
<dd><p>Waits for all kernels in all streams on current device to complete.</p>
</dd></dl>

<div class="section" id="communication-collectives">
<h3>Communication collectives<a class="headerlink" href="#communication-collectives" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.cuda.comm.broadcast">
<code class="descclassname">torch.cuda.comm.</code><code class="descname">broadcast</code><span class="sig-paren">(</span><em>tensor</em>, <em>devices</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.comm.broadcast" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcasts a tensor to a number of GPUs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; tensor to broadcast.</li>
<li><strong>devices</strong> (<em>Iterable</em>) &#8211; an iterable of devices among which to broadcast.
Note that it should be like (src, dst1, dst2, ...), the first element
of which is the source device to broadcast from.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A tuple containing copies of the <code class="docutils literal"><span class="pre">tensor</span></code>, placed on devices
corresponding to indices from <code class="docutils literal"><span class="pre">devices</span></code>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.comm.reduce_add">
<code class="descclassname">torch.cuda.comm.</code><code class="descname">reduce_add</code><span class="sig-paren">(</span><em>inputs</em>, <em>destination=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.comm.reduce_add" title="Permalink to this definition">¶</a></dt>
<dd><p>Sums tensors from multiple GPUs.</p>
<p>All inputs should have matching shapes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>inputs</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em><em></em>) &#8211; an iterable of tensors to add.</li>
<li><strong>destination</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; a device on which the output will be
placed (default: current device).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A tensor containing an elementwise sum of all inputs, placed on the
<code class="docutils literal"><span class="pre">destination</span></code> device.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.comm.scatter">
<code class="descclassname">torch.cuda.comm.</code><code class="descname">scatter</code><span class="sig-paren">(</span><em>tensor</em>, <em>devices</em>, <em>chunk_sizes=None</em>, <em>dim=0</em>, <em>streams=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.comm.scatter" title="Permalink to this definition">¶</a></dt>
<dd><p>Scatters tensor across multiple GPUs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; tensor to scatter.</li>
<li><strong>devices</strong> (<em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>]</em><em></em>) &#8211; iterable of ints, specifying among which
devices the tensor should be scattered.</li>
<li><strong>chunk_sizes</strong> (<em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>]</em><em></em><em>, </em><em>optional</em>) &#8211; sizes of chunks to be placed on
each device. It should match <code class="docutils literal"><span class="pre">devices</span></code> in length and sum to
<code class="docutils literal"><span class="pre">tensor.size(dim)</span></code>. If not specified, the tensor will be divided
into equal chunks.</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; A dimension along which to chunk the tensor.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A tuple containing chunks of the <code class="docutils literal"><span class="pre">tensor</span></code>, spread accross given
<code class="docutils literal"><span class="pre">devices</span></code>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.comm.gather">
<code class="descclassname">torch.cuda.comm.</code><code class="descname">gather</code><span class="sig-paren">(</span><em>tensors</em>, <em>dim=0</em>, <em>destination=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.comm.gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Gathers tensors from multiple GPUs.</p>
<p>Tensor sizes in all dimension different than <code class="docutils literal"><span class="pre">dim</span></code> have to match.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensors</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em><em></em>) &#8211; iterable of tensors to gather.</li>
<li><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; a dimension along which the tensors will be concatenated.</li>
<li><strong>destination</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; output device (-1 means CPU, default:
current device)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">A tensor located on <code class="docutils literal"><span class="pre">destination</span></code> device, that is a result of
concatenating <code class="docutils literal"><span class="pre">tensors</span></code> along <code class="docutils literal"><span class="pre">dim</span></code>.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="streams-and-events">
<h3>Streams and events<a class="headerlink" href="#streams-and-events" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="torch.cuda.Stream">
<em class="property">class </em><code class="descclassname">torch.cuda.</code><code class="descname">Stream</code><a class="headerlink" href="#torch.cuda.Stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper around a CUDA stream.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>device</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; a device on which to allocate the Stream.</li>
<li><strong>priority</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; priority of the stream. Lower numbers
represent higher priorities.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torch.cuda.Stream.query">
<code class="descname">query</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Stream.query" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks if all the work submitted has been completed.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">A boolean indicating if all kernels in this stream are completed.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Stream.record_event">
<code class="descname">record_event</code><span class="sig-paren">(</span><em>event=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Stream.record_event" title="Permalink to this definition">¶</a></dt>
<dd><p>Records an event.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>event</strong> (<a class="reference internal" href="index.html#torch.cuda.Event" title="torch.cuda.Event"><em>Event</em></a><em>, </em><em>optional</em>) &#8211; event to record. If not given, a new one
will be allocated.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Recorded event.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Stream.synchronize">
<code class="descname">synchronize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Stream.synchronize" title="Permalink to this definition">¶</a></dt>
<dd><p>Wait for all the kernels in this stream to complete.</p>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Stream.wait_event">
<code class="descname">wait_event</code><span class="sig-paren">(</span><em>event</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Stream.wait_event" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes all future work submitted to the stream wait for an event.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>event</strong> (<a class="reference internal" href="index.html#torch.cuda.Event" title="torch.cuda.Event"><em>Event</em></a>) &#8211; an event to wait for.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Stream.wait_stream">
<code class="descname">wait_stream</code><span class="sig-paren">(</span><em>stream</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Stream.wait_stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Synchronizes with another stream.</p>
<p>All future work submitted to this stream will wait until all kernels
submitted to a given stream at the time of call complete.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>stream</strong> (<a class="reference internal" href="index.html#torch.cuda.Stream" title="torch.cuda.Stream"><em>Stream</em></a>) &#8211; a stream to synchronize.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.cuda.Event">
<em class="property">class </em><code class="descclassname">torch.cuda.</code><code class="descname">Event</code><span class="sig-paren">(</span><em>enable_timing=False</em>, <em>blocking=False</em>, <em>interprocess=False</em>, <em>_handle=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Event" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper around CUDA event.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>enable_timing</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; indicates if the event should measure time
(default: False)</li>
<li><strong>blocking</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if true, <a class="reference internal" href="#torch.cuda.Event.wait" title="torch.cuda.Event.wait"><code class="xref py py-meth docutils literal"><span class="pre">wait()</span></code></a> will be blocking (default: False)</li>
<li><strong>interprocess</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; if true, the event can be shared between processes
(default: False)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="torch.cuda.Event.elapsed_time">
<code class="descname">elapsed_time</code><span class="sig-paren">(</span><em>end_event</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Event.elapsed_time" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the time elapsed before the event was recorded.</p>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.ipc_handle">
<code class="descname">ipc_handle</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Event.ipc_handle" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an IPC handle of this event.</p>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.query">
<code class="descname">query</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Event.query" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks if the event has been recorded.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">A boolean indicating if the event has been recorded.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.record">
<code class="descname">record</code><span class="sig-paren">(</span><em>stream=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Event.record" title="Permalink to this definition">¶</a></dt>
<dd><p>Records the event in a given stream.</p>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.synchronize">
<code class="descname">synchronize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Event.synchronize" title="Permalink to this definition">¶</a></dt>
<dd><p>Synchronizes with the event.</p>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.wait">
<code class="descname">wait</code><span class="sig-paren">(</span><em>stream=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.cuda.Event.wait" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes a given stream wait for the event.</p>
</dd></dl>

</dd></dl>

</div>
</div>
<span id="document-ffi"></span><div class="section" id="torch-utils-ffi">
<h2>torch.utils.ffi<a class="headerlink" href="#torch-utils-ffi" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torch.utils.ffi.create_extension">
<code class="descclassname">torch.utils.ffi.</code><code class="descname">create_extension</code><span class="sig-paren">(</span><em>name</em>, <em>headers</em>, <em>sources</em>, <em>verbose=True</em>, <em>with_cuda=False</em>, <em>package=False</em>, <em>relative_to='.'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.ffi.create_extension" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates and configures a cffi.FFI object, that builds PyTorch extension.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a>) &#8211; package name. Can be a nested module e.g. <code class="docutils literal"><span class="pre">.ext.my_lib</span></code>.</li>
<li><strong>headers</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a><em> or </em><em>List</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a><em>]</em><em></em>) &#8211; list of headers, that contain only exported
functions</li>
<li><strong>sources</strong> (<em>List</em><em>[</em><a class="reference external" href="https://docs.python.org/2/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a><em>]</em><em></em>) &#8211; list of sources to compile.</li>
<li><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; if set to <code class="docutils literal"><span class="pre">False</span></code>, no output will be printed
(default: True).</li>
<li><strong>with_cuda</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; set to <code class="docutils literal"><span class="pre">True</span></code> to compile with CUDA headers
(default: False)</li>
<li><strong>package</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; set to <code class="docutils literal"><span class="pre">True</span></code> to build in package mode (for modules
meant to be installed as pip packages) (default: False).</li>
<li><strong>relative_to</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#str" title="(in Python v2.7)"><em>str</em></a><em>, </em><em>optional</em>) &#8211; path of the build file. Required when
<code class="docutils literal"><span class="pre">package</span> <span class="pre">is</span> <span class="pre">True</span></code>. It&#8217;s best to use <code class="docutils literal"><span class="pre">__file__</span></code> for this argument.</li>
<li><strong>kwargs</strong> &#8211; additional arguments that are passed to ffi to declare the
extension. See <a class="reference external" href="https://docs.python.org/3/distutils/apiref.html#distutils.core.Extension">Extension API reference</a> for details.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<span id="document-data"></span><div class="section" id="module-torch.utils.data">
<span id="torch-utils-data"></span><h2>torch.utils.data<a class="headerlink" href="#module-torch.utils.data" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torch.utils.data.Dataset">
<em class="property">class </em><code class="descclassname">torch.utils.data.</code><code class="descname">Dataset</code><a class="headerlink" href="#torch.utils.data.Dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>An abstract class representing a Dataset.</p>
<p>All other datasets should subclass it. All subclasses should override
<code class="docutils literal"><span class="pre">__len__</span></code>, that provides the size of the dataset, and <code class="docutils literal"><span class="pre">__getitem__</span></code>,
supporting integer indexing in range from 0 to len(self) exclusive.</p>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.TensorDataset">
<em class="property">class </em><code class="descclassname">torch.utils.data.</code><code class="descname">TensorDataset</code><span class="sig-paren">(</span><em>data_tensor</em>, <em>target_tensor</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.TensorDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Dataset wrapping data and target tensors.</p>
<p>Each sample will be retrieved by indexing both tensors along the first
dimension.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>data_tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; contains sample data.</li>
<li><strong>target_tensor</strong> (<a class="reference internal" href="index.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) &#8211; contains sample targets (labels).</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.DataLoader">
<em class="property">class </em><code class="descclassname">torch.utils.data.</code><code class="descname">DataLoader</code><span class="sig-paren">(</span><em>dataset</em>, <em>batch_size=1</em>, <em>shuffle=False</em>, <em>sampler=None</em>, <em>num_workers=0</em>, <em>collate_fn=&lt;function default_collate&gt;</em>, <em>pin_memory=False</em>, <em>drop_last=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.DataLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>Data loader. Combines a dataset and a sampler, and provides
single- or multi-process iterators over the dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>dataset</strong> (<a class="reference internal" href="index.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><em>Dataset</em></a>) &#8211; dataset from which to load the data.</li>
<li><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; how many samples per batch to load
(default: 1).</li>
<li><strong>shuffle</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; set to <code class="docutils literal"><span class="pre">True</span></code> to have the data reshuffled
at every epoch (default: False).</li>
<li><strong>sampler</strong> (<a class="reference internal" href="index.html#torch.utils.data.sampler.Sampler" title="torch.utils.data.sampler.Sampler"><em>Sampler</em></a><em>, </em><em>optional</em>) &#8211; defines the strategy to draw samples from
the dataset. If specified, the <code class="docutils literal"><span class="pre">shuffle</span></code> argument is ignored.</li>
<li><strong>num_workers</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a><em>, </em><em>optional</em>) &#8211; how many subprocesses to use for data
loading. 0 means that the data will be loaded in the main process
(default: 0)</li>
<li><strong>collate_fn</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#callable" title="(in Python v2.7)"><em>callable</em></a><em>, </em><em>optional</em>) &#8211; </li>
<li><strong>pin_memory</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; </li>
<li><strong>drop_last</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a><em>, </em><em>optional</em>) &#8211; set to <code class="docutils literal"><span class="pre">True</span></code> to drop the last incomplete batch,
if the dataset size is not divisible by the batch size. If False and
the size of dataset is not divisible by the batch size, then the last batch
will be smaller. (default: False)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.sampler.Sampler">
<em class="property">class </em><code class="descclassname">torch.utils.data.sampler.</code><code class="descname">Sampler</code><span class="sig-paren">(</span><em>data_source</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.sampler.Sampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for all Samplers.</p>
<p>Every Sampler subclass has to provide an __iter__ method, providing a way
to iterate over indices of dataset elements, and a __len__ method that
returns the length of the returned iterators.</p>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.sampler.SequentialSampler">
<em class="property">class </em><code class="descclassname">torch.utils.data.sampler.</code><code class="descname">SequentialSampler</code><span class="sig-paren">(</span><em>data_source</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.sampler.SequentialSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples elements sequentially, always in the same order.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>data_source</strong> (<a class="reference internal" href="index.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><em>Dataset</em></a>) &#8211; dataset to sample from</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.sampler.RandomSampler">
<em class="property">class </em><code class="descclassname">torch.utils.data.sampler.</code><code class="descname">RandomSampler</code><span class="sig-paren">(</span><em>data_source</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.sampler.RandomSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples elements randomly, without replacement.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>data_source</strong> (<a class="reference internal" href="index.html#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><em>Dataset</em></a>) &#8211; dataset to sample from</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.sampler.SubsetRandomSampler">
<em class="property">class </em><code class="descclassname">torch.utils.data.sampler.</code><code class="descname">SubsetRandomSampler</code><span class="sig-paren">(</span><em>indices</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.sampler.SubsetRandomSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples elements randomly from a given list of indices, without replacement.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>indices</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#list" title="(in Python v2.7)"><em>list</em></a>) &#8211; a list of indices</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="torch.utils.data.sampler.WeightedRandomSampler">
<em class="property">class </em><code class="descclassname">torch.utils.data.sampler.</code><code class="descname">WeightedRandomSampler</code><span class="sig-paren">(</span><em>weights</em>, <em>num_samples</em>, <em>replacement=True</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.data.sampler.WeightedRandomSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples elements from [0,..,len(weights)-1] with given probabilities (weights).
:param weights: a list of weights, not necessary summing up to one
:type weights: list
:param num_samples: number of samples to draw
:type num_samples: int</p>
</dd></dl>

</div>
<span id="document-model_zoo"></span><div class="section" id="module-torch.utils.model_zoo">
<span id="torch-utils-model-zoo"></span><h2>torch.utils.model_zoo<a class="headerlink" href="#module-torch.utils.model_zoo" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torch.utils.model_zoo.load_url">
<code class="descclassname">torch.utils.model_zoo.</code><code class="descname">load_url</code><span class="sig-paren">(</span><em>url</em>, <em>model_dir=None</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.utils.model_zoo.load_url" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads the Torch serialized object at the given URL.</p>
<p>If the object is already present in <cite>model_dir</cite>, it&#8217;s deserialied and
returned. The filename part of the URL should follow the naming convention
<code class="docutils literal"><span class="pre">filename-&lt;sha256&gt;.ext</span></code> where <code class="docutils literal"><span class="pre">&lt;sha256&gt;</span></code> is the first eight or more
digits of the SHA256 hash of the contents of the file. The hash is used to
ensure unique names and to verify the contents of the file.</p>
<p>The default value of <cite>model_dir</cite> is <code class="docutils literal"><span class="pre">$TORCH_HOME/models</span></code> where
<code class="docutils literal"><span class="pre">$TORCH_HOME</span></code> defaults to <code class="docutils literal"><span class="pre">~/.torch</span></code>. The default directory can be
overriden with the <code class="docutils literal"><span class="pre">$TORCH_MODEL_ZOO</span></code> environement variable.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>url</strong> (<a class="reference external" href="https://docs.python.org/2/library/string.html#module-string" title="(in Python v2.7)"><em>string</em></a>) &#8211; URL of the object to download</li>
<li><strong>model_dir</strong> (<a class="reference external" href="https://docs.python.org/2/library/string.html#module-string" title="(in Python v2.7)"><em>string</em></a><em>, </em><em>optional</em>) &#8211; directory in which to save the object</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_zoo</span><span class="o">.</span><span class="n">load_url</span><span class="p">(</span><span class="s1">&#39;https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-torchvision/torchvision"></span><div class="section" id="torchvision">
<h2>torchvision<a class="headerlink" href="#torchvision" title="Permalink to this headline">¶</a></h2>
<p>The <code class="xref py py-mod docutils literal"><span class="pre">torchvision</span></code> package consists of popular datasets, model
architectures, and common image transformations for computer vision.</p>
</div>
<span id="document-torchvision/datasets"></span><div class="section" id="torchvision-datasets">
<h2>torchvision.datasets<a class="headerlink" href="#torchvision-datasets" title="Permalink to this headline">¶</a></h2>
<p>The following dataset loaders are available:</p>
<ul class="simple">
<li><a class="reference external" href="#mnist">MNIST</a></li>
<li><a class="reference external" href="#coco">COCO (Captioning and Detection)</a></li>
<li><a class="reference external" href="#lsun">LSUN Classification</a></li>
<li><a class="reference external" href="#imagefolder">ImageFolder</a></li>
<li><a class="reference external" href="#imagenet-12">Imagenet-12</a></li>
<li><a class="reference external" href="#cifar">CIFAR10 and CIFAR100</a></li>
<li><a class="reference external" href="#stl10">STL10</a></li>
</ul>
<p>Datasets have the API:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">__getitem__</span></code></li>
<li><code class="docutils literal"><span class="pre">__len__</span></code>
They all subclass from <code class="docutils literal"><span class="pre">torch.utils.data.Dataset</span></code>
Hence, they can all be multi-threaded (python multiprocessing) using
standard torch.utils.data.DataLoader.</li>
</ul>
<p>For example:</p>
<p><code class="docutils literal"><span class="pre">torch.utils.data.DataLoader(coco_cap,</span> <span class="pre">batch_size=args.batchSize,</span> <span class="pre">shuffle=True,</span> <span class="pre">num_workers=args.nThreads)</span></code></p>
<p>In the constructor, each dataset has a slightly different API as needed,
but they all take the keyword args:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">transform</span></code> - a function that takes in an image and returns a
transformed version</li>
<li>common stuff like <code class="docutils literal"><span class="pre">ToTensor</span></code>, <code class="docutils literal"><span class="pre">RandomCrop</span></code>, etc. These can be
composed together with <code class="docutils literal"><span class="pre">transforms.Compose</span></code> (see transforms section
below)</li>
<li><code class="docutils literal"><span class="pre">target_transform</span></code> - a function that takes in the target and
transforms it. For example, take in the caption string and return a
tensor of word indices.</li>
</ul>
<div class="section" id="mnist">
<h3>MNIST<a class="headerlink" href="#mnist" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal"><span class="pre">dset.MNIST(root,</span> <span class="pre">train=True,</span> <span class="pre">transform=None,</span> <span class="pre">target_transform=None,</span> <span class="pre">download=False)</span></code></p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">root</span></code> : root directory of dataset where <code class="docutils literal"><span class="pre">processed/training.pt</span></code> and  <code class="docutils literal"><span class="pre">processed/test.pt</span></code> exist.</li>
<li><code class="docutils literal"><span class="pre">train</span></code> : <code class="docutils literal"><span class="pre">True</span></code> = Training set, <code class="docutils literal"><span class="pre">False</span></code> = Test set</li>
<li><code class="docutils literal"><span class="pre">download</span></code> : <code class="docutils literal"><span class="pre">True</span></code> = downloads the dataset from the internet and puts it in root directory. If dataset already downloaded, place the processed dataset (function available in mnist.py) in the <code class="docutils literal"><span class="pre">processed</span></code> folder.</li>
</ul>
</div>
<div class="section" id="coco">
<h3>COCO<a class="headerlink" href="#coco" title="Permalink to this headline">¶</a></h3>
<p>This requires the <a class="reference external" href="https://github.com/pdollar/coco/tree/master/PythonAPI">COCO API to be installed</a></p>
<div class="section" id="captions">
<h4>Captions:<a class="headerlink" href="#captions" title="Permalink to this headline">¶</a></h4>
<p><code class="docutils literal"><span class="pre">dset.CocoCaptions(root=&quot;dir</span> <span class="pre">where</span> <span class="pre">images</span> <span class="pre">are&quot;,</span> <span class="pre">annFile=&quot;json</span> <span class="pre">annotation</span> <span class="pre">file&quot;,</span> <span class="pre">[transform,</span> <span class="pre">target_transform])</span></code></p>
<p>Example:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="k">as</span> <span class="nn">dset</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="n">cap</span> <span class="o">=</span> <span class="n">dset</span><span class="o">.</span><span class="n">CocoCaptions</span><span class="p">(</span><span class="n">root</span> <span class="o">=</span> <span class="s1">&#39;dir where images are&#39;</span><span class="p">,</span>
                        <span class="n">annFile</span> <span class="o">=</span> <span class="s1">&#39;json annotation file&#39;</span><span class="p">,</span>
                        <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of samples: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cap</span><span class="p">))</span>
<span class="n">img</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">cap</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="c1"># load 4th sample</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Image Size: &quot;</span><span class="p">,</span> <span class="n">img</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">Number</span> <span class="n">of</span> <span class="n">samples</span><span class="p">:</span> <span class="mi">82783</span>
<span class="n">Image</span> <span class="n">Size</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="n">L</span><span class="p">,</span> <span class="mi">427</span><span class="n">L</span><span class="p">,</span> <span class="mi">640</span><span class="n">L</span><span class="p">)</span>
<span class="p">[</span><span class="s1">u&#39;A plane emitting smoke stream flying over a mountain.&#39;</span><span class="p">,</span>
<span class="s1">u&#39;A plane darts across a bright blue sky behind a mountain covered in snow&#39;</span><span class="p">,</span>
<span class="s1">u&#39;A plane leaves a contrail above the snowy mountain top.&#39;</span><span class="p">,</span>
<span class="s1">u&#39;A mountain that has a plane flying overheard in the distance.&#39;</span><span class="p">,</span>
<span class="s1">u&#39;A mountain view with a plume of smoke in the background&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="detection">
<h4>Detection:<a class="headerlink" href="#detection" title="Permalink to this headline">¶</a></h4>
<p><code class="docutils literal"><span class="pre">dset.CocoDetection(root=&quot;dir</span> <span class="pre">where</span> <span class="pre">images</span> <span class="pre">are&quot;,</span> <span class="pre">annFile=&quot;json</span> <span class="pre">annotation</span> <span class="pre">file&quot;,</span> <span class="pre">[transform,</span> <span class="pre">target_transform])</span></code></p>
</div>
</div>
<div class="section" id="lsun">
<h3>LSUN<a class="headerlink" href="#lsun" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal"><span class="pre">dset.LSUN(db_path,</span> <span class="pre">classes='train',</span> <span class="pre">[transform,</span> <span class="pre">target_transform])</span></code></p>
<ul class="simple">
<li>db_path = root directory for the database files</li>
<li><code class="docutils literal"><span class="pre">classes</span></code> = <code class="docutils literal"><span class="pre">‘train’</span></code> (all categories, training set), <code class="docutils literal"><span class="pre">‘val’</span></code> (all categories, validation set), <code class="docutils literal"><span class="pre">‘test’</span></code> (all categories, test set)</li>
<li>[<code class="docutils literal"><span class="pre">‘bedroom\_train’</span></code>, <code class="docutils literal"><span class="pre">‘church\_train’</span></code>, …] : a list of categories to load</li>
</ul>
</div>
<div class="section" id="imagefolder">
<h3>ImageFolder<a class="headerlink" href="#imagefolder" title="Permalink to this headline">¶</a></h3>
<p>A generic data loader where the images are arranged in this way:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">root</span><span class="o">/</span><span class="n">dog</span><span class="o">/</span><span class="n">xxx</span><span class="o">.</span><span class="n">png</span>
<span class="n">root</span><span class="o">/</span><span class="n">dog</span><span class="o">/</span><span class="n">xxy</span><span class="o">.</span><span class="n">png</span>
<span class="n">root</span><span class="o">/</span><span class="n">dog</span><span class="o">/</span><span class="n">xxz</span><span class="o">.</span><span class="n">png</span>

<span class="n">root</span><span class="o">/</span><span class="n">cat</span><span class="o">/</span><span class="mf">123.</span><span class="n">png</span>
<span class="n">root</span><span class="o">/</span><span class="n">cat</span><span class="o">/</span><span class="n">nsdf3</span><span class="o">.</span><span class="n">png</span>
<span class="n">root</span><span class="o">/</span><span class="n">cat</span><span class="o">/</span><span class="n">asd932_</span><span class="o">.</span><span class="n">png</span>
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">dset.ImageFolder(root=&quot;root</span> <span class="pre">folder</span> <span class="pre">path&quot;,</span> <span class="pre">[transform,</span> <span class="pre">target_transform])</span></code></p>
<p>It has the members:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">self.classes</span></code> - The class names as a list</li>
<li><code class="docutils literal"><span class="pre">self.class_to_idx</span></code> - Corresponding class indices</li>
<li><code class="docutils literal"><span class="pre">self.imgs</span></code> - The list of (image path, class-index) tuples</li>
</ul>
</div>
<div class="section" id="imagenet-12">
<h3>Imagenet-12<a class="headerlink" href="#imagenet-12" title="Permalink to this headline">¶</a></h3>
<p>This is simply implemented with an ImageFolder dataset.</p>
<p>The data is preprocessed <a class="reference external" href="https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset">as described
here</a></p>
<p><a class="reference external" href="https://github.com/pytorch/examples/blob/27e2a46c1d1505324032b1d94fc6ce24d5b67e97/imagenet/main.py#L48-L62">Here is an
example</a>.</p>
</div>
<div class="section" id="cifar">
<h3>CIFAR<a class="headerlink" href="#cifar" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal"><span class="pre">dset.CIFAR10(root,</span> <span class="pre">train=True,</span> <span class="pre">transform=None,</span> <span class="pre">target_transform=None,</span> <span class="pre">download=False)</span></code></p>
<p><code class="docutils literal"><span class="pre">dset.CIFAR100(root,</span> <span class="pre">train=True,</span> <span class="pre">transform=None,</span> <span class="pre">target_transform=None,</span> <span class="pre">download=False)</span></code></p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">root</span></code> : root directory of dataset where there is folder
<code class="docutils literal"><span class="pre">cifar-10-batches-py</span></code></li>
<li><code class="docutils literal"><span class="pre">train</span></code> : <code class="docutils literal"><span class="pre">True</span></code> = Training set, <code class="docutils literal"><span class="pre">False</span></code> = Test set</li>
<li><code class="docutils literal"><span class="pre">download</span></code> : <code class="docutils literal"><span class="pre">True</span></code> = downloads the dataset from the internet and
puts it in root directory. If dataset already downloaded, doesn&#8217;t do anything.</li>
</ul>
</div>
<div class="section" id="stl10">
<h3>STL10<a class="headerlink" href="#stl10" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal"><span class="pre">dset.STL10(root,</span> <span class="pre">split='train',</span> <span class="pre">transform=None,</span> <span class="pre">target_transform=None,</span> <span class="pre">download=False)</span></code></p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">root</span></code> : root directory of dataset where there is folder <code class="docutils literal"><span class="pre">stl10_binary</span></code></li>
<li><code class="docutils literal"><span class="pre">split</span></code> : <code class="docutils literal"><span class="pre">'train'</span></code> = Training set, <code class="docutils literal"><span class="pre">'test'</span></code> = Test set, <code class="docutils literal"><span class="pre">'unlabeled'</span></code> = Unlabeled set,    <code class="docutils literal"><span class="pre">'train+unlabeled'</span></code> = Training + Unlabeled set (missing label marked as <code class="docutils literal"><span class="pre">-1</span></code>)</li>
<li><code class="docutils literal"><span class="pre">download</span></code> : <code class="docutils literal"><span class="pre">True</span></code> = downloads the dataset from the internet and puts it in root directory. If dataset already downloaded, doesn&#8217;t do anything.</li>
</ul>
</div>
</div>
<span id="document-torchvision/models"></span><div class="section" id="torchvision-models">
<h2>torchvision.models<a class="headerlink" href="#torchvision-models" title="Permalink to this headline">¶</a></h2>
</div>
<span id="document-torchvision/transforms"></span><div class="section" id="torchvision-transforms">
<h2>torchvision.transforms<a class="headerlink" href="#torchvision-transforms" title="Permalink to this headline">¶</a></h2>
<div class="section" id="transforms-on-pil-image">
<h3>Transforms on PIL.Image<a class="headerlink" href="#transforms-on-pil-image" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="transforms-on-torch-tensor">
<h3>Transforms on torch.*Tensor<a class="headerlink" href="#transforms-on-torch-tensor" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="conversion-transforms">
<h3>Conversion Transforms<a class="headerlink" href="#conversion-transforms" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="generic-transforms">
<h3>Generic Transforms<a class="headerlink" href="#generic-transforms" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<span id="document-torchvision/utils"></span><div class="section" id="torchvision-utils">
<h2>torchvision.utils<a class="headerlink" href="#torchvision-utils" title="Permalink to this headline">¶</a></h2>
</div>
</div>
</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></li>
<li><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></li>
</ul>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1.11',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>