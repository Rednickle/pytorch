// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#define __STDC_FORMAT_MACROS

#include "ATen/CUDAHalfType.h"

// @generated by aten/src/ATen/gen.py

#include <THC/THC.h>
#include <THC/THCTensor.hpp>
#include <THCUNN/THCUNN.h>
#undef THNN_
#undef THCIndexTensor_
#include "ATen/core/TensorImpl.h"
#include "ATen/CUDAGenerator.h"
#include "ATen/Allocator.h"
#include "ATen/DeviceGuard.h"
#include "ATen/NativeFunctions.h"
#include "ATen/Utils.h"
#include "ATen/WrapDimUtils.h"
#include "ATen/core/Half.h"
#include "ATen/core/TensorImpl.h"
#include "ATen/core/UndefinedTensorImpl.h"
#include "c10/util/Optional.h"

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include "ATen/Config.h"
#include <ATen/cuda/ATenCUDAGeneral.h>
#include <ATen/DeviceGuard.h>
#include <ATen/cuda/CUDADevice.h>
#include <ATen/cuda/CUDATypeDefault.h>

namespace at {

CUDAHalfType::CUDAHalfType()
  : CUDATypeDefault(CUDATensorId(), /*is_variable=*/false, /*is_undefined=*/false) {}

ScalarType CUDAHalfType::scalarType() const {
  return ScalarType::Half;
}

caffe2::TypeMeta CUDAHalfType::typeMeta() const {
    return caffe2::TypeMeta::Make<Half>();
}

Backend CUDAHalfType::backend() const {
  return Backend::CUDA;
}

const char * CUDAHalfType::toString() const {
  return "CUDAHalfType";
}

TypeID CUDAHalfType::ID() const {
  return TypeID::CUDAHalf;
}

size_t CUDAHalfType::elementSizeInBytes() const {
  return sizeof(Half);
}

/* example
Tensor * CUDAHalfType::add(Tensor & a, Tensor & b) {
  std::cout << "add CUDAHalfTensor\n";
  return &a;
}
*/

int64_t CUDAHalfType::_th_storage_offset(const Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    return static_cast<int64_t>(THCudaHalfTensor_storageOffset(globalContext().getTHCState(), self_));
}
int64_t CUDAHalfType::_th_ndimension(const Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    return static_cast<int64_t>(THCudaHalfTensor_nDimension(globalContext().getTHCState(), self_));
}
Tensor & CUDAHalfType::_th_set_(Tensor & self, Storage source) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToDataType(ScalarType::Half));
    THCudaHalfTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
    self_->maybe_zero_dim(false);
    return self;
}
Tensor & CUDAHalfType::_th_set_(Tensor & self, Storage source, int64_t storage_offset, IntList size, IntList stride) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto source_ = checked_storage(source,"source",2, DeviceType::CUDA, at::scalarTypeToDataType(ScalarType::Half));
    THCudaHalfTensor_setStorage(globalContext().getTHCState(), self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
    self_->maybe_zero_dim(size.size() == 0);
    return self;
}
Tensor & CUDAHalfType::_th_set_(Tensor & self, const Tensor & source) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto source_ = checked_tensor_unwrap(source,"source",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_set(globalContext().getTHCState(), self_, source_);
    self_->maybe_zero_dim(source_->dim() == 0);
    return self;
}
Tensor & CUDAHalfType::_th_set_(Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_setStorage(globalContext().getTHCState(), self_, NULL, 0, {0}, {});
    self_->maybe_zero_dim(false);
    return self;
}
Tensor & CUDAHalfType::_fill_(Tensor & self, Scalar value) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto value_ = value.toHalf();
    THCudaHalfTensor_fill(globalContext().getTHCState(), self_, value_);
    return self;
}
Tensor & CUDAHalfType::_fill_(Tensor & self, const Tensor & value) const {
    const DeviceGuard device_guard(self);
    if (value.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_fill_(self, at::_local_scalar(value));
    }
    AT_ERROR("_fill_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
bool CUDAHalfType::_th_is_contiguous(const Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    return THCudaHalfTensor_isContiguous(globalContext().getTHCState(), self_);
}
bool CUDAHalfType::_th_is_set_to(const Tensor & self, const Tensor & tensor) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CUDA, ScalarType::Half);
    return THCudaHalfTensor_isSetTo(globalContext().getTHCState(), self_, tensor_);
}
Tensor & CUDAHalfType::s__th_masked_fill_(Tensor & self, const Tensor & mask, Scalar value) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
    auto value_ = value.toHalf();
    THCudaHalfTensor_maskedFill(globalContext().getTHCState(), self_, mask_, value_);
    return self;
}
Tensor & CUDAHalfType::s__th_masked_fill_(Tensor & self, const Tensor & mask, const Tensor & value) const {
    const DeviceGuard device_guard(self);
    if (value.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_masked_fill_(self, mask, at::_local_scalar(value));
    }
    AT_ERROR("_th_masked_fill_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
Tensor & CUDAHalfType::s__th_masked_scatter_(Tensor & self, const Tensor & mask, const Tensor & source) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
    auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_maskedCopy(globalContext().getTHCState(), self_, mask_, source_);
    return self;
}
Tensor & CUDAHalfType::s__th_masked_select_out(Tensor & result, const Tensor & self, const Tensor & mask) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
    THCudaHalfTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
    result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_masked_select(const Tensor & self, const Tensor & mask) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CUDA, ScalarType::Byte);
    THCudaHalfTensor_maskedSelect(globalContext().getTHCState(), result_, self_, mask_);
    result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_nonzero_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_nonzero(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_nonzero(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_nonzero(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_clone(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaHalfTensor_newClone(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
}
Tensor CUDAHalfType::_th_view(const Tensor & self, IntList size) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaHalfTensor_newView(globalContext().getTHCState(), self_, size))->maybe_zero_dim(size.size() == 0)));
}
Tensor & CUDAHalfType::_th_resize_as_(Tensor & self, const Tensor & the_template) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto the_template_ = checked_tensor_unwrap(the_template,"the_template",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_resizeAs(globalContext().getTHCState(), self_, the_template_);
    self_->maybe_zero_dim(the_template_->dim() == 0);
    return self;
}
Tensor & CUDAHalfType::_th_index_select_out(Tensor & result, const Tensor & self, int64_t dim, const Tensor & index) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
    THCudaHalfTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
    result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_index_select(const Tensor & self, int64_t dim, const Tensor & index) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
    THCudaHalfTensor_indexSelect(globalContext().getTHCState(), result_, self_, dim, index_);
    result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_indexCopy_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
    auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_indexCopy(globalContext().getTHCState(), self_, dim, index_, source_);
    return self;
}
Tensor & CUDAHalfType::_th_take_out(Tensor & result, const Tensor & self, const Tensor & index) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
    THCudaHalfTensor_take(globalContext().getTHCState(), result_, self_, index_);
    result_->maybe_zero_dim(index_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_take(const Tensor & self, const Tensor & index) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
    THCudaHalfTensor_take(globalContext().getTHCState(), result_, self_, index_);
    result_->maybe_zero_dim(index_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_put_(Tensor & self, const Tensor & index, const Tensor & source, bool accumulate) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CUDA, ScalarType::Long);
    auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_put(globalContext().getTHCState(), self_, index_, source_, accumulate);
    return self;
}
Tensor & CUDAHalfType::_th_index_add_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
    auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_indexAdd(globalContext().getTHCState(), self_, dim, index_, source_);
    return self;
}
Tensor & CUDAHalfType::_th_index_fill_(Tensor & self, int64_t dim, const Tensor & index, Scalar value) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
    auto value_ = value.toHalf();
    THCudaHalfTensor_indexFill(globalContext().getTHCState(), self_, dim, index_, value_);
    return self;
}
Tensor & CUDAHalfType::_th_index_fill_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & value) const {
    const DeviceGuard device_guard(self);
    if (value.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_index_fill_(self, dim, index, at::_local_scalar(value));
    }
    AT_ERROR("_th_index_fill_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
Tensor CUDAHalfType::_th_unfold(const Tensor & self, int64_t dimension, int64_t size, int64_t step) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dimension = maybe_wrap_dim(dimension, self_);
    THCudaHalfTensor_unfold(globalContext().getTHCState(), result_, self_, dimension, size, step);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_range_out(Tensor & result, Scalar start, Scalar end, Scalar step) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto start_ = start.toDouble();
    auto end_ = end.toDouble();
    auto step_ = step.toDouble();
    THCudaHalfTensor_range(globalContext().getTHCState(), result_, start_, end_, step_);
    return result;
}
Tensor CUDAHalfType::_range(Scalar start, Scalar end, Scalar step) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto start_ = start.toDouble();
    auto end_ = end.toDouble();
    auto step_ = step.toDouble();
    THCudaHalfTensor_range(globalContext().getTHCState(), result_, start_, end_, step_);
    return result;
}
Tensor & CUDAHalfType::_arange_out(Tensor & result, Scalar start, Scalar end, Scalar step) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto start_ = start.toDouble();
    auto end_ = end.toDouble();
    auto step_ = step.toDouble();
    THCudaHalfTensor_arange(globalContext().getTHCState(), result_, start_, end_, step_);
    return result;
}
Tensor CUDAHalfType::_arange(Scalar start, Scalar end, Scalar step) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto start_ = start.toDouble();
    auto end_ = end.toDouble();
    auto step_ = step.toDouble();
    THCudaHalfTensor_arange(globalContext().getTHCState(), result_, start_, end_, step_);
    return result;
}
Tensor & CUDAHalfType::_arange_out(Tensor & result, Scalar end) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto end_ = end.toDouble();
    THCudaHalfTensor_arange(globalContext().getTHCState(), result_, 0, end_, 1);
    return result;
}
Tensor CUDAHalfType::_arange(Scalar end) const {
    // DeviceGuard omitted
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto end_ = end.toDouble();
    THCudaHalfTensor_arange(globalContext().getTHCState(), result_, 0, end_, 1);
    return result;
}
Tensor & CUDAHalfType::_th_scatter_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) const {
    const DeviceGuard device_guard(self);
    if (src.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_scatter_(self, dim, index, at::_local_scalar(src));
    }
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
    auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_scatter(globalContext().getTHCState(), self_, dim, index_, src_);
    return self;
}
Tensor & CUDAHalfType::_th_scatter_(Tensor & self, int64_t dim, const Tensor & index, Scalar value) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
    auto value_ = value.toHalf();
    THCudaHalfTensor_scatterFill(globalContext().getTHCState(), self_, dim, index_, value_);
    return self;
}
Tensor & CUDAHalfType::_th_scatter_add_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
    auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_scatterAdd(globalContext().getTHCState(), self_, dim, index_, src_);
    return self;
}
Tensor & CUDAHalfType::_th_gather_out(Tensor & result, const Tensor & self, int64_t dim, const Tensor & index) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    result.resize_(index.sizes());
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
    THCudaHalfTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
    result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_gather(const Tensor & self, int64_t dim, const Tensor & index) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    result.resize_(index.sizes());
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CUDA, ScalarType::Long);
    THCudaHalfTensor_gather(globalContext().getTHCState(), result_, self_, dim, index_);
    result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
    return result;
}
void* CUDAHalfType::data_ptr(const Tensor & self) const {
    // DeviceGuard omitted
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    return THCudaHalfTensor_data(globalContext().getTHCState(), self_);
}
bool CUDAHalfType::_th_equal(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    return THCudaHalfTensor_equal(globalContext().getTHCState(), self_, other_);
}
Tensor & CUDAHalfType::__and___out(Tensor & result, const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::__and__(const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_bitand(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s___and___out(Tensor & result, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s___and__(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cbitand(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::__iand__(Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_bitand(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::s___iand__(Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cbitand(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::__or___out(Tensor & result, const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::__or__(const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_bitor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s___or___out(Tensor & result, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s___or__(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cbitor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::__ior__(Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_bitor(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::s___ior__(Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cbitor(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::__xor___out(Tensor & result, const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::__xor__(const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_bitxor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s___xor___out(Tensor & result, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s___xor__(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cbitxor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::__ixor__(Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_bitxor(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::s___ixor__(Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cbitxor(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::__lshift___out(Tensor & result, const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::__lshift__(const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_lshift(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s___lshift___out(Tensor & result, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s___lshift__(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_clshift(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::__ilshift__(Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_lshift(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::s___ilshift__(Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_clshift(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::__rshift___out(Tensor & result, const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::__rshift__(const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_rshift(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s___rshift___out(Tensor & result, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s___rshift__(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_crshift(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::__irshift__(Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_rshift(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::s___irshift__(Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_crshift(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::_th_lt_out(Tensor & result, const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_lt(const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_ltValue(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s__th_lt_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_lt(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_ltTensor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_lt_(Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_ltValueT(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::s__th_lt_(Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_ltTensorT(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::_th_gt_out(Tensor & result, const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_gt(const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_gtValue(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s__th_gt_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_gt(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_gtTensor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_gt_(Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_gtValueT(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::s__th_gt_(Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_gtTensorT(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::_th_le_out(Tensor & result, const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_le(const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_leValue(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s__th_le_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_le(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_leTensor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_le_(Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_leValueT(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::s__th_le_(Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_leTensorT(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::_th_ge_out(Tensor & result, const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_ge(const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_geValue(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s__th_ge_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_ge(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_geTensor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_ge_(Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_geValueT(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::s__th_ge_(Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_geTensorT(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::_th_eq_out(Tensor & result, const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_eq(const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_eqValue(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s__th_eq_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_eq(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_eqTensor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_eq_(Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_eqValueT(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::s__th_eq_(Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_eqTensorT(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::_th_ne_out(Tensor & result, const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_ne(const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_neValue(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s__th_ne_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Byte);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_ne(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Byte), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_neTensor(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_ne_(Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_neValueT(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::s__th_ne_(Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_neTensorT(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::s__th_min_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_min(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cmin(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_min(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    return scalarTensor(convert<Half>(THCudaHalfTensor_minall(globalContext().getTHCState(), self_)));
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::_th_min_out(Tensor & min, Tensor & min_indices, const Tensor & self, int64_t dim, bool keepdim) const {
    const DeviceGuard device_guard(min);
    auto min_ = checked_tensor_unwrap(min,"min",0, false, Backend::CUDA, ScalarType::Half);
    auto min_indices_ = checked_tensor_unwrap(min_indices,"min_indices",0, false, Backend::CUDA, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
    min_->maybe_zero_dim(maybe_scalar);
    min_indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(min, min_indices);
}
std::tuple<Tensor,Tensor> CUDAHalfType::_th_min(const Tensor & self, int64_t dim, bool keepdim) const {
    const DeviceGuard device_guard(self);
    auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
    auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_min(globalContext().getTHCState(), min_, min_indices_, self_, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
    min_->maybe_zero_dim(maybe_scalar);
    min_indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(min, min_indices);
}
Tensor & CUDAHalfType::s__th_max_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_max(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cmax(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_max(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    return scalarTensor(convert<Half>(THCudaHalfTensor_maxall(globalContext().getTHCState(), self_)));
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::_th_max_out(Tensor & max, Tensor & max_indices, const Tensor & self, int64_t dim, bool keepdim) const {
    const DeviceGuard device_guard(max);
    auto max_ = checked_tensor_unwrap(max,"max",0, false, Backend::CUDA, ScalarType::Half);
    auto max_indices_ = checked_tensor_unwrap(max_indices,"max_indices",0, false, Backend::CUDA, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
    max_->maybe_zero_dim(maybe_scalar);
    max_indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(max, max_indices);
}
std::tuple<Tensor,Tensor> CUDAHalfType::_th_max(const Tensor & self, int64_t dim, bool keepdim) const {
    const DeviceGuard device_guard(self);
    auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
    auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_max(globalContext().getTHCState(), max_, max_indices_, self_, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
    max_->maybe_zero_dim(maybe_scalar);
    max_indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(max, max_indices);
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::_th_mode_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool keepdim) const {
    const DeviceGuard device_guard(values);
    auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(values, indices);
}
std::tuple<Tensor,Tensor> CUDAHalfType::_th_mode(const Tensor & self, int64_t dim, bool keepdim) const {
    const DeviceGuard device_guard(self);
    auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
    auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_mode(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(values, indices);
}
Tensor CUDAHalfType::_th_median(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    return scalarTensor(convert<Half>(THCudaHalfTensor_medianall(globalContext().getTHCState(), self_)));
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::_th_median_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool keepdim) const {
    const DeviceGuard device_guard(values);
    auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_median(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(values, indices);
}
std::tuple<Tensor,Tensor> CUDAHalfType::_th_median(const Tensor & self, int64_t dim, bool keepdim) const {
    const DeviceGuard device_guard(self);
    auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
    auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_median(globalContext().getTHCState(), values_, indices_, self_, dim, keepdim);
    bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(values, indices);
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::_th_sort_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool descending) const {
    const DeviceGuard device_guard(values);
    auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
    bool maybe_scalar = self_->dim() == 0;
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(values, indices);
}
std::tuple<Tensor,Tensor> CUDAHalfType::_th_sort(const Tensor & self, int64_t dim, bool descending) const {
    const DeviceGuard device_guard(self);
    auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
    auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_sort(globalContext().getTHCState(), values_, indices_, self_, dim, descending);
    bool maybe_scalar = self_->dim() == 0;
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(values, indices);
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::_th_topk_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) const {
    const DeviceGuard device_guard(values);
    auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CUDA, ScalarType::Long);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
    bool maybe_scalar = self_->dim() == 0;
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(values, indices);
}
std::tuple<Tensor,Tensor> CUDAHalfType::_th_topk(const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) const {
    const DeviceGuard device_guard(self);
    auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
    auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_topk(globalContext().getTHCState(), values_, indices_, self_, k, dim, largest, sorted);
    bool maybe_scalar = self_->dim() == 0;
    values_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(values, indices);
}
Tensor & CUDAHalfType::_abs_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_abs(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_abs(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_abs(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_sigmoid_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_sigmoid(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_sigmoid(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_sigmoid(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_log_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_log(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_log(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_log(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_log10_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_log10(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_log10(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_log10(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_log1p_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_log1p(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_log1p(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_log1p(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_log2_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_log2(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_log2(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_log2(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_lgamma_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_lgamma(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_lgamma(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_lgamma(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_lgamma_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_lgamma(globalContext().getTHCState(), self_, self_);
    return self;
}
Tensor & CUDAHalfType::_th_digamma_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_digamma(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_digamma(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_digamma(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_digamma_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_digamma(globalContext().getTHCState(), self_, self_);
    return self;
}
Tensor & CUDAHalfType::_th_polygamma_out(Tensor & result, int64_t n, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_polygamma(globalContext().getTHCState(), result_, n, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_polygamma(int64_t n, const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_polygamma(globalContext().getTHCState(), result_, n, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_polygamma_(Tensor & self, int64_t n) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_polygamma(globalContext().getTHCState(), self_, n, self_);
    return self;
}
Tensor & CUDAHalfType::_exp_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_exp(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_exp(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_exp(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_expm1_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_expm1(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_expm1(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_expm1(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_cos_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cos(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_cos(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cos(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_acos_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_acos(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_acos(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_acos(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_cosh_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cosh(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_cosh(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cosh(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_sin_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_sin(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_sin(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_sin(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_asin_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_asin(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_asin(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_asin(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_sinh_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_sinh(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_sinh(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_sinh(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_tan_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_tan(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_tan(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_tan(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_atan_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_atan(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_atan(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_atan(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_tanh_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_tanh(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_tanh(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_tanh(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_erf_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_erf(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_erf(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_erf(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_erfc_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_erfc(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_erfc(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_erfc(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_erfinv_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_erfinv(globalContext().getTHCState(), self_, self_);
    return self;
}
Tensor & CUDAHalfType::_th_erfinv_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_erfinv(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_erfinv(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_erfinv(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_sqrt_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_sqrt(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_sqrt(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_sqrt(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_rsqrt_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_rsqrt(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_rsqrt(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_rsqrt(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_ceil_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_ceil(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_ceil(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_ceil(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_floor_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_floor(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_floor(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_floor(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_round_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_round(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_round(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_round(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_trunc_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_trunc(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_trunc(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_trunc(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_frac_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_frac(globalContext().getTHCState(), self_, self_);
    return self;
}
Tensor & CUDAHalfType::_th_frac_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_frac(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_frac(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_frac(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_var_out(Tensor & result, const Tensor & self, int64_t dim, bool unbiased, bool keepdim) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_var(globalContext().getTHCState(), result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
    result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
    return result;
}
Tensor CUDAHalfType::_th_var(const Tensor & self, int64_t dim, bool unbiased, bool keepdim) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_var(globalContext().getTHCState(), result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
    result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
    return result;
}
Tensor CUDAHalfType::_th_var(const Tensor & self, bool unbiased) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    return scalarTensor(convert<Half>(THCudaHalfTensor_varall(globalContext().getTHCState(), self_, (unbiased) ? 0 : 1)));
}
Tensor & CUDAHalfType::_th_std_out(Tensor & result, const Tensor & self, int64_t dim, bool unbiased, bool keepdim) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_std(globalContext().getTHCState(), result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
    result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
    return result;
}
Tensor CUDAHalfType::_th_std(const Tensor & self, int64_t dim, bool unbiased, bool keepdim) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_std(globalContext().getTHCState(), result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
    result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
    return result;
}
Tensor CUDAHalfType::_th_std(const Tensor & self, bool unbiased) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    return scalarTensor(convert<Half>(THCudaHalfTensor_stdall(globalContext().getTHCState(), self_, (unbiased) ? 0 : 1)));
}
Tensor CUDAHalfType::_th_norm(const Tensor & self, Scalar p) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto p_ = p.toHalf();
    return scalarTensor(convert<Half>(THCudaHalfTensor_normall(globalContext().getTHCState(), self_, p_)));
}
Tensor & CUDAHalfType::_th_norm_out(Tensor & result, const Tensor & self, Scalar p, int64_t dim, bool keepdim) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto p_ = p.toHalf();
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_norm(globalContext().getTHCState(), result_, self_, p_, dim, keepdim);
    result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
    return result;
}
Tensor CUDAHalfType::_th_norm(const Tensor & self, Scalar p, int64_t dim, bool keepdim) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto p_ = p.toHalf();
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_norm(globalContext().getTHCState(), result_, self_, p_, dim, keepdim);
    result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
    return result;
}
Tensor & CUDAHalfType::_th_renorm_out(Tensor & result, const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto p_ = p.toHalf();
    dim = maybe_wrap_dim(dim, self_);
    auto maxnorm_ = maxnorm.toHalf();
    THCudaHalfTensor_renorm(globalContext().getTHCState(), result_, self_, p_, dim, maxnorm_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_renorm(const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto p_ = p.toHalf();
    dim = maybe_wrap_dim(dim, self_);
    auto maxnorm_ = maxnorm.toHalf();
    THCudaHalfTensor_renorm(globalContext().getTHCState(), result_, self_, p_, dim, maxnorm_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_renorm_(Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto p_ = p.toHalf();
    dim = maybe_wrap_dim(dim, self_);
    auto maxnorm_ = maxnorm.toHalf();
    THCudaHalfTensor_renorm(globalContext().getTHCState(), self_, self_, p_, dim, maxnorm_);
    return self;
}
Tensor CUDAHalfType::s__th_dist(const Tensor & self, const Tensor & other, Scalar p) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    auto p_ = p.toHalf();
    return scalarTensor(convert<Half>(THCudaHalfTensor_dist(globalContext().getTHCState(), self_, other_, p_)));
}
Tensor & CUDAHalfType::_th_reciprocal_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cinv(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_reciprocal(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cinv(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_reciprocal_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cinv(globalContext().getTHCState(), self_, self_);
    return self;
}
Tensor & CUDAHalfType::_th_neg_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_neg(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_neg(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_neg(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_neg_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_neg(globalContext().getTHCState(), self_, self_);
    return self;
}
Tensor & CUDAHalfType::s__th_atan2_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_atan2(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_atan2(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_atan2(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s__th_atan2_(Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_atan2(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::_th_pow_out(Tensor & result, const Tensor & self, Scalar exponent) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto exponent_ = exponent.toHalf();
    THCudaHalfTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_pow(const Tensor & self, Scalar exponent) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto exponent_ = exponent.toHalf();
    THCudaHalfTensor_pow(globalContext().getTHCState(), result_, self_, exponent_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s__th_pow_out(Tensor & result, const Tensor & self, const Tensor & exponent) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
    result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_pow(const Tensor & self, const Tensor & exponent) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cpow(globalContext().getTHCState(), result_, self_, exponent_);
    result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_pow_out(Tensor & result, Scalar self, const Tensor & exponent) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = self.toHalf();
    auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
    result_->maybe_zero_dim(exponent_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_pow(Scalar self, const Tensor & exponent) const {
    const DeviceGuard device_guard(exponent);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = self.toHalf();
    auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_tpow(globalContext().getTHCState(), result_, self_, exponent_);
    result_->maybe_zero_dim(exponent_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_pow_(Tensor & self, Scalar exponent) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto exponent_ = exponent.toHalf();
    THCudaHalfTensor_pow(globalContext().getTHCState(), self_, self_, exponent_);
    return self;
}
Tensor & CUDAHalfType::s__th_pow_(Tensor & self, const Tensor & exponent) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto exponent_ = checked_tensor_unwrap(exponent,"exponent",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cpow(globalContext().getTHCState(), self_, self_, exponent_);
    return self;
}
Tensor & CUDAHalfType::s__th_lerp_out(Tensor & result, const Tensor & self, const Tensor & end, Scalar weight) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto end_ = checked_tensor_unwrap(end,"end",2, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = weight.toHalf();
    THCudaHalfTensor_lerp(globalContext().getTHCState(), result_, self_, end_, weight_);
    result_->maybe_zero_dim(self_->dim() == 0 && end_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_lerp(const Tensor & self, const Tensor & end, Scalar weight) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto end_ = checked_tensor_unwrap(end,"end",2, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = weight.toHalf();
    THCudaHalfTensor_lerp(globalContext().getTHCState(), result_, self_, end_, weight_);
    result_->maybe_zero_dim(self_->dim() == 0 && end_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s__th_lerp_(Tensor & self, const Tensor & end, Scalar weight) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto end_ = checked_tensor_unwrap(end,"end",3, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = weight.toHalf();
    THCudaHalfTensor_lerp(globalContext().getTHCState(), self_, self_, end_, weight_);
    return self;
}
Tensor & CUDAHalfType::_th_zero_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_zero(globalContext().getTHCState(), self_);
    return self;
}
Tensor & CUDAHalfType::_cumsum_out(Tensor & result, const Tensor & self, int64_t dim) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_cumsum(const Tensor & self, int64_t dim) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_cumsum(globalContext().getTHCState(), result_, self_, dim);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_cumprod_out(Tensor & result, const Tensor & self, int64_t dim) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_cumprod(const Tensor & self, int64_t dim) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    THCudaHalfTensor_cumprod(globalContext().getTHCState(), result_, self_, dim);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_sign_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_sign(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_sign(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_sign(globalContext().getTHCState(), result_, self_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_sign_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_sign(globalContext().getTHCState(), self_, self_);
    return self;
}
Tensor CUDAHalfType::_th_trace(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    return scalarTensor(convert<Half>(THCudaHalfTensor_trace(globalContext().getTHCState(), self_)));
}
Tensor & CUDAHalfType::_th_fmod_out(Tensor & result, const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_fmod(const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_fmod(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s__th_fmod_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_fmod(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cfmod(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_fmod_(Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_fmod(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::s__th_fmod_(Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cfmod(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::_th_remainder_out(Tensor & result, const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_remainder(const Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_remainder(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s__th_remainder_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_remainder(const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cremainder(globalContext().getTHCState(), result_, self_, other_);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_remainder_(Tensor & self, Scalar other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = other.toHalf();
    THCudaHalfTensor_remainder(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::s__th_remainder_(Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cremainder(globalContext().getTHCState(), self_, self_, other_);
    return self;
}
Tensor & CUDAHalfType::_th_clamp_out(Tensor & result, const Tensor & self, Scalar min, Scalar max) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto min_ = min.toHalf();
    auto max_ = max.toHalf();
    THCudaHalfTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_clamp(const Tensor & self, Scalar min, Scalar max) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto min_ = min.toHalf();
    auto max_ = max.toHalf();
    THCudaHalfTensor_clamp(globalContext().getTHCState(), result_, self_, min_, max_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_clamp_min_out(Tensor & result, const Tensor & self, Scalar min) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto min_ = min.toHalf();
    THCudaHalfTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_clamp_min(const Tensor & self, Scalar min) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto min_ = min.toHalf();
    THCudaHalfTensor_cmaxValue(globalContext().getTHCState(), result_, self_, min_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_clamp_max_out(Tensor & result, const Tensor & self, Scalar max) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto max_ = max.toHalf();
    THCudaHalfTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_clamp_max(const Tensor & self, Scalar max) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto max_ = max.toHalf();
    THCudaHalfTensor_cminValue(globalContext().getTHCState(), result_, self_, max_);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_dot(const Tensor & self, const Tensor & tensor) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CUDA, ScalarType::Half);
    return scalarTensor(convert<Half>(THCudaHalfTensor_dot(globalContext().getTHCState(), self_, tensor_)));
}
Tensor & CUDAHalfType::_th_tril_out(Tensor & result, const Tensor & self, int64_t diagonal) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_tril(globalContext().getTHCState(), result_, self_, diagonal);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_tril(const Tensor & self, int64_t diagonal) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_tril(globalContext().getTHCState(), result_, self_, diagonal);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_tril_(Tensor & self, int64_t diagonal) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_tril(globalContext().getTHCState(), self_, self_, diagonal);
    return self;
}
Tensor & CUDAHalfType::_th_triu_out(Tensor & result, const Tensor & self, int64_t diagonal) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_triu(globalContext().getTHCState(), result_, self_, diagonal);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_triu(const Tensor & self, int64_t diagonal) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_triu(globalContext().getTHCState(), result_, self_, diagonal);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_triu_(Tensor & self, int64_t diagonal) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_triu(globalContext().getTHCState(), self_, self_, diagonal);
    return self;
}
Tensor & CUDAHalfType::_th_cross_out(Tensor & result, const Tensor & self, const Tensor & other, int64_t dim) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cross(globalContext().getTHCState(), result_, self_, other_, dim);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_cross(const Tensor & self, const Tensor & other, int64_t dim) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_cross(globalContext().getTHCState(), result_, self_, other_, dim);
    result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_diag_out(Tensor & result, const Tensor & self, int64_t diagonal) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    if (self_->dim() == 0) {
      throw std::runtime_error("Input must be 1-d or 2-d");
    }
    THCudaHalfTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_diag(const Tensor & self, int64_t diagonal) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    if (self_->dim() == 0) {
      throw std::runtime_error("Input must be 1-d or 2-d");
    }
    THCudaHalfTensor_diag(globalContext().getTHCState(), result_, self_, diagonal);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s_th_addmm_out(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto beta_ = beta.toHalf();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto alpha_ = alpha.toHalf();
    auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Half);
    auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
    result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s_th_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto beta_ = beta.toHalf();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto alpha_ = alpha.toHalf();
    auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CUDA, ScalarType::Half);
    auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat1_, mat2_);
    result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::th_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto beta_ = beta.toHalf();
    auto alpha_ = alpha.toHalf();
    auto mat1_ = checked_tensor_unwrap(mat1,"mat1",5, false, Backend::CUDA, ScalarType::Half);
    auto mat2_ = checked_tensor_unwrap(mat2,"mat2",6, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat1_, mat2_);
    return self;
}
Tensor & CUDAHalfType::s__addmv_out(Tensor & result, const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto beta_ = beta.toHalf();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto alpha_ = alpha.toHalf();
    auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Half);
    auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
    result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__addmv(const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto beta_ = beta.toHalf();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto alpha_ = alpha.toHalf();
    auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CUDA, ScalarType::Half);
    auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addmv(globalContext().getTHCState(), result_, beta_, self_, alpha_, mat_, vec_);
    result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_addmv_(Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto beta_ = beta.toHalf();
    auto alpha_ = alpha.toHalf();
    auto mat_ = checked_tensor_unwrap(mat,"mat",5, false, Backend::CUDA, ScalarType::Half);
    auto vec_ = checked_tensor_unwrap(vec,"vec",6, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addmv(globalContext().getTHCState(), self_, beta_, self_, alpha_, mat_, vec_);
    return self;
}
Tensor & CUDAHalfType::s__addr_out(Tensor & result, const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto beta_ = beta.toHalf();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto alpha_ = alpha.toHalf();
    auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Half);
    auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
    result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__addr(const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto beta_ = beta.toHalf();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto alpha_ = alpha.toHalf();
    auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CUDA, ScalarType::Half);
    auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addr(globalContext().getTHCState(), result_, beta_, self_, alpha_, vec1_, vec2_);
    result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_addr_(Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto beta_ = beta.toHalf();
    auto alpha_ = alpha.toHalf();
    auto vec1_ = checked_tensor_unwrap(vec1,"vec1",5, false, Backend::CUDA, ScalarType::Half);
    auto vec2_ = checked_tensor_unwrap(vec2,"vec2",6, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addr(globalContext().getTHCState(), self_, beta_, self_, alpha_, vec1_, vec2_);
    return self;
}
Tensor & CUDAHalfType::_ger_out(Tensor & result, const Tensor & self, const Tensor & vec2) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addr(globalContext().getTHCState(), result_, convert<at::Half,double>(0), result_, convert<at::Half,double>(1), self_, vec2_);
    result_->maybe_zero_dim(false);
    return result;
}
Tensor CUDAHalfType::_ger(const Tensor & self, const Tensor & vec2) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addr(globalContext().getTHCState(), result_, convert<at::Half,double>(0), result_, convert<at::Half,double>(1), self_, vec2_);
    result_->maybe_zero_dim(false);
    return result;
}
Tensor & CUDAHalfType::_mv_out(Tensor & result, const Tensor & self, const Tensor & vec) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    result.resize_({ self.size(0) });
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addmv(globalContext().getTHCState(), result_, convert<at::Half,double>(0), result_, convert<at::Half,double>(1), self_, vec_);
    result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_mv(const Tensor & self, const Tensor & vec) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    result.resize_({ self.size(0) });
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addmv(globalContext().getTHCState(), result_, convert<at::Half,double>(0), result_, convert<at::Half,double>(1), self_, vec_);
    result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_mm_out(Tensor & result, const Tensor & self, const Tensor & mat2) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    result.resize_({ self.size(0),mat2.size(1) });
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addmm(globalContext().getTHCState(), result_, convert<at::Half,double>(0), result_, convert<at::Half,double>(1), self_, mat2_);
    result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_mm(const Tensor & self, const Tensor & mat2) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    result.resize_({ self.size(0),mat2.size(1) });
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addmm(globalContext().getTHCState(), result_, convert<at::Half,double>(0), result_, convert<at::Half,double>(1), self_, mat2_);
    result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_bmm_out(Tensor & result, const Tensor & self, const Tensor & mat2) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    result.resize_({ self.size(0),self.size(1),mat2.size(2) });
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_baddbmm(globalContext().getTHCState(), result_, convert<at::Half,double>(0), result_, convert<at::Half,double>(1), self_, mat2_);
    result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_bmm(const Tensor & self, const Tensor & mat2) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    result.resize_({ self.size(0),self.size(1),mat2.size(2) });
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_baddbmm(globalContext().getTHCState(), result_, convert<at::Half,double>(0), result_, convert<at::Half,double>(1), self_, mat2_);
    result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s_addbmm_out(Tensor & result, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto beta_ = beta.toHalf();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto alpha_ = alpha.toHalf();
    auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Half);
    auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
    result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s_addbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto beta_ = beta.toHalf();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto alpha_ = alpha.toHalf();
    auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Half);
    auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
    result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_addbmm_(Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto beta_ = beta.toHalf();
    auto alpha_ = alpha.toHalf();
    auto batch1_ = checked_tensor_unwrap(batch1,"batch1",5, false, Backend::CUDA, ScalarType::Half);
    auto batch2_ = checked_tensor_unwrap(batch2,"batch2",6, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addbmm(globalContext().getTHCState(), self_, beta_, self_, alpha_, batch1_, batch2_);
    return self;
}
Tensor & CUDAHalfType::s__th_baddbmm_out(Tensor & result, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto beta_ = beta.toHalf();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto alpha_ = alpha.toHalf();
    auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Half);
    auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
    result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_baddbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto beta_ = beta.toHalf();
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto alpha_ = alpha.toHalf();
    auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CUDA, ScalarType::Half);
    auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_baddbmm(globalContext().getTHCState(), result_, beta_, self_, alpha_, batch1_, batch2_);
    result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s__th_addcmul_out(Tensor & result, const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto value_ = value.toHalf();
    auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Half);
    auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
    result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_addcmul(const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto value_ = value.toHalf();
    auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Half);
    auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addcmul(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
    result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s__th_addcmul_(Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto value_ = value.toHalf();
    auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Half);
    auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addcmul(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
    return self;
}
Tensor & CUDAHalfType::s__th_addcdiv_out(Tensor & result, const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto value_ = value.toHalf();
    auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Half);
    auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
    result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::s__th_addcdiv(const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto value_ = value.toHalf();
    auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CUDA, ScalarType::Half);
    auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addcdiv(globalContext().getTHCState(), result_, self_, value_, tensor1_, tensor2_);
    result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::s__th_addcdiv_(Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto value_ = value.toHalf();
    auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CUDA, ScalarType::Half);
    auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_addcdiv(globalContext().getTHCState(), self_, self_, value_, tensor1_, tensor2_);
    return self;
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::_th_btrifact_out(Tensor & result, Tensor & pivots, const Tensor & self, bool pivot) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto pivots_ = checked_tensor_unwrap(pivots,"pivots",0, false, Backend::CUDA, ScalarType::Int);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_btrifact(globalContext().getTHCState(), result_, pivots_, NULL, pivot, self_);
    bool maybe_scalar = self_->dim() == 0;
    result_->maybe_zero_dim(maybe_scalar);
    pivots_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(result, pivots);
}
std::tuple<Tensor,Tensor> CUDAHalfType::_th_btrifact(const Tensor & self, bool pivot) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto pivots_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Int), allocator(), false).release();
    auto pivots = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(pivots_));
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_btrifact(globalContext().getTHCState(), result_, pivots_, NULL, pivot, self_);
    bool maybe_scalar = self_->dim() == 0;
    result_->maybe_zero_dim(maybe_scalar);
    pivots_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(result, pivots);
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAHalfType::_th_btrifact_with_info_out(Tensor & result, Tensor & pivots, Tensor & info, const Tensor & self, bool pivot) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto pivots_ = checked_tensor_unwrap(pivots,"pivots",0, false, Backend::CUDA, ScalarType::Int);
    auto info_ = checked_tensor_unwrap(info,"info",0, false, Backend::CUDA, ScalarType::Int);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_btrifact(globalContext().getTHCState(), result_, pivots_, info_, pivot, self_);
    bool maybe_scalar = self_->dim() == 0;
    result_->maybe_zero_dim(maybe_scalar);
    pivots_->maybe_zero_dim(maybe_scalar);
    info_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &, Tensor &>(result, pivots, info);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::_th_btrifact_with_info(const Tensor & self, bool pivot) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto pivots_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Int), allocator(), false).release();
    auto pivots = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(pivots_));
    auto info_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Int), allocator(), false).release();
    auto info = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(info_));
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_btrifact(globalContext().getTHCState(), result_, pivots_, info_, pivot, self_);
    bool maybe_scalar = self_->dim() == 0;
    result_->maybe_zero_dim(maybe_scalar);
    pivots_->maybe_zero_dim(maybe_scalar);
    info_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor, Tensor>(result, pivots, info);
}
Tensor & CUDAHalfType::_th_btrisolve_out(Tensor & result, const Tensor & self, const Tensor & LU_data, const Tensor & LU_pivots) const {
    const DeviceGuard device_guard(result);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto LU_data_ = checked_tensor_unwrap(LU_data,"LU_data",2, false, Backend::CUDA, ScalarType::Half);
    auto LU_pivots_ = checked_tensor_unwrap(LU_pivots,"LU_pivots",3, false, Backend::CUDA, ScalarType::Int);
    THCudaHalfTensor_btrisolve(globalContext().getTHCState(), result_, self_, LU_data_, LU_pivots_);
    result_->maybe_zero_dim(self_->dim() == 0 && LU_data_->dim() == 0 && LU_pivots_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_btrisolve(const Tensor & self, const Tensor & LU_data, const Tensor & LU_pivots) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto LU_data_ = checked_tensor_unwrap(LU_data,"LU_data",2, false, Backend::CUDA, ScalarType::Half);
    auto LU_pivots_ = checked_tensor_unwrap(LU_pivots,"LU_pivots",3, false, Backend::CUDA, ScalarType::Int);
    THCudaHalfTensor_btrisolve(globalContext().getTHCState(), result_, self_, LU_data_, LU_pivots_);
    result_->maybe_zero_dim(self_->dim() == 0 && LU_data_->dim() == 0 && LU_pivots_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_random_(Tensor & self, int64_t from, int64_t to, Generator * generator) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    THCudaHalfTensor_clampedRandom(globalContext().getTHCState(), self_, from, to);
    return self;
}
Tensor & CUDAHalfType::_th_random_(Tensor & self, int64_t to, Generator * generator) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    THCudaHalfTensor_cappedRandom(globalContext().getTHCState(), self_, to);
    return self;
}
Tensor & CUDAHalfType::_th_random_(Tensor & self, Generator * generator) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    THCudaHalfTensor_random(globalContext().getTHCState(), self_);
    return self;
}
Tensor & CUDAHalfType::_th_multinomial_out(Tensor & result, const Tensor & self, int64_t num_samples, bool replacement, Generator * generator) const {
    const DeviceGuard device_guard(self);
    auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CUDA, ScalarType::Long);
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_multinomial(globalContext().getTHCState(), result_, self_, num_samples, replacement);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor CUDAHalfType::_th_multinomial(const Tensor & self, int64_t num_samples, bool replacement, Generator * generator) const {
    const DeviceGuard device_guard(self);
    auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_multinomial(globalContext().getTHCState(), result_, self_, num_samples, replacement);
    result_->maybe_zero_dim(self_->dim() == 0);
    return result;
}
Tensor & CUDAHalfType::_th_uniform_(Tensor & self, double from, double to, Generator * generator) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    THCudaHalfTensor_uniform(globalContext().getTHCState(), self_, from, to);
    return self;
}
Tensor & CUDAHalfType::_th_normal_out(Tensor & output, const Tensor & mean, double std, Generator * generator) const {
    const DeviceGuard device_guard(output);
    auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CUDA, ScalarType::Half);
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_normal_means(globalContext().getTHCState(), output_, mean_, std);
    output_->maybe_zero_dim(mean_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::_th_normal(const Tensor & mean, double std, Generator * generator) const {
    const DeviceGuard device_guard(mean);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_normal_means(globalContext().getTHCState(), output_, mean_, std);
    output_->maybe_zero_dim(mean_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::_th_normal_out(Tensor & output, double mean, const Tensor & std, Generator * generator) const {
    const DeviceGuard device_guard(output);
    auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CUDA, ScalarType::Half);
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_normal_stddevs(globalContext().getTHCState(), output_, mean, std_);
    output_->maybe_zero_dim(std_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::_th_normal(double mean, const Tensor & std, Generator * generator) const {
    const DeviceGuard device_guard(std);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_normal_stddevs(globalContext().getTHCState(), output_, mean, std_);
    output_->maybe_zero_dim(std_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::_th_normal_out(Tensor & output, const Tensor & mean, const Tensor & std, Generator * generator) const {
    const DeviceGuard device_guard(output);
    auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CUDA, ScalarType::Half);
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CUDA, ScalarType::Half);
    auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_normal_means_stddevs(globalContext().getTHCState(), output_, mean_, std_);
    output_->maybe_zero_dim(mean_->dim() == 0 && std_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::_th_normal(const Tensor & mean, const Tensor & std, Generator * generator) const {
    const DeviceGuard device_guard(mean);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CUDA, ScalarType::Half);
    auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_normal_means_stddevs(globalContext().getTHCState(), output_, mean_, std_);
    output_->maybe_zero_dim(mean_->dim() == 0 && std_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::_th_normal_(Tensor & self, double mean, double std, Generator * generator) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    THCudaHalfTensor_normal(globalContext().getTHCState(), self_, mean, std);
    return self;
}
Tensor & CUDAHalfType::_th_cauchy_(Tensor & self, double median, double sigma, Generator * generator) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    THCudaHalfTensor_cauchy(globalContext().getTHCState(), self_, median, sigma);
    return self;
}
Tensor & CUDAHalfType::_th_log_normal_(Tensor & self, double mean, double std, Generator * generator) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    THCudaHalfTensor_logNormal(globalContext().getTHCState(), self_, mean, std);
    return self;
}
Tensor & CUDAHalfType::_th_exponential_(Tensor & self, double lambd, Generator * generator) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    THCudaHalfTensor_exponential(globalContext().getTHCState(), self_, lambd);
    return self;
}
Tensor & CUDAHalfType::_th_geometric_(Tensor & self, double p, Generator * generator) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    THCudaHalfTensor_geometric(globalContext().getTHCState(), self_, p);
    return self;
}
Tensor CUDAHalfType::tensor(Storage storage, int64_t storageOffset, IntList size, IntList stride) const {
    // DeviceGuard omitted
    auto storage_ = checked_storage(storage,"storage",1, DeviceType::CUDA, at::scalarTypeToDataType(ScalarType::Half));
    return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaHalfTensor_newWithStorage(globalContext().getTHCState(), storage_.unsafeGetStorageImpl(), storageOffset, size, stride))->maybe_zero_dim(size.size() == 0)));
}
Tensor CUDAHalfType::tensor(IntList size, IntList stride) const {
    // DeviceGuard omitted
    return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaHalfTensor_newWithSize(globalContext().getTHCState(), size, stride))->maybe_zero_dim(size.size() == 0)));
}
Tensor CUDAHalfType::_th_alias(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCudaHalfTensor_newWithTensor(globalContext().getTHCState(), self_))->maybe_zero_dim(self_->dim() == 0)));
}
Tensor & CUDAHalfType::_copy_ignoring_overlaps_(Tensor & self, const Tensor & src) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto src_ = checked_tensor_unwrap(src,"src",2, false, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_copyIgnoringOverlaps(globalContext().getTHCState(), self_, src_);
    return self;
}
Tensor & CUDAHalfType::_cat_out(Tensor & self, TensorList tensors, int64_t dim) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",0, false, Backend::CUDA, ScalarType::Half);
    auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
    return self;
}
Tensor CUDAHalfType::_cat(TensorList tensors, int64_t dim) const {
    const DeviceGuard device_guard(tensors);
    auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
    auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CUDA, ScalarType::Half);
    THCudaHalfTensor_catArray(globalContext().getTHCState(), self_, tensors_.data(), tensors_.size(), dim);
    return self;
}
Tensor & CUDAHalfType::binary_cross_entropy_forward_out(Tensor & output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Half);
    auto output_ = checked_tensor_unwrap(output,"output",4, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::binary_cross_entropy_forward(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Half);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::binary_cross_entropy_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::binary_cross_entropy_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::l1_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
    auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfAbsCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::l1_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfAbsCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::l1_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfAbsCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::l1_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfAbsCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::mse_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
    auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfMSECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::mse_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfMSECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::mse_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfMSECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::mse_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfMSECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::multi_margin_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
    auto p_ = p.toDouble();
    auto margin_ = margin.toDouble();
    auto weight_ = checked_tensor_unwrap(weight,"weight",5, true, Backend::CUDA, ScalarType::Half);
    auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfMultiMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, p_, weight_ ? weight_ : NULL, margin_);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::multi_margin_loss_forward(const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
    auto p_ = p.toDouble();
    auto margin_ = margin.toDouble();
    auto weight_ = checked_tensor_unwrap(weight,"weight",5, true, Backend::CUDA, ScalarType::Half);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfMultiMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, p_, weight_ ? weight_ : NULL, margin_);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::multi_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
    auto p_ = p.toDouble();
    auto margin_ = margin.toDouble();
    auto weight_ = checked_tensor_unwrap(weight,"weight",6, true, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfMultiMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, p_, weight_ ? weight_ : NULL, margin_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::multi_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
    auto p_ = p.toDouble();
    auto margin_ = margin.toDouble();
    auto weight_ = checked_tensor_unwrap(weight,"weight",6, true, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfMultiMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, p_, weight_ ? weight_ : NULL, margin_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::multilabel_margin_loss_forward_out(Tensor & output, Tensor & is_target, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
    auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
    auto is_target_ = checked_tensor_unwrap(is_target,"is_target",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfMultiLabelMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, is_target_, reduction);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    is_target_->maybe_zero_dim(target_->dim() == 0);
    return std::tuple<Tensor &, Tensor &>(output, is_target);
}
std::tuple<Tensor,Tensor> CUDAHalfType::multilabel_margin_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto is_target_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto is_target = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(is_target_));
    THNN_CudaHalfMultiLabelMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, is_target_, reduction);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    is_target_->maybe_zero_dim(target_->dim() == 0);
    return std::tuple<Tensor, Tensor>(output, is_target);
}
Tensor & CUDAHalfType::multilabel_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction, const Tensor & is_target) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
    auto is_target_ = checked_tensor_unwrap(is_target,"is_target",5, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfMultiLabelMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, is_target_, reduction);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::multilabel_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction, const Tensor & is_target) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
    auto is_target_ = checked_tensor_unwrap(is_target,"is_target",5, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfMultiLabelMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, is_target_, reduction);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::nll_loss_forward_out(Tensor & output, Tensor & total_weight, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Half);
    auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Half);
    auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",5, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    total_weight_->maybe_zero_dim(true);
    return std::tuple<Tensor &, Tensor &>(output, total_weight);
}
std::tuple<Tensor,Tensor> CUDAHalfType::nll_loss_forward(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Half);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto total_weight_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto total_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(total_weight_));
    THNN_CudaHalfClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    total_weight_->maybe_zero_dim(true);
    return std::tuple<Tensor, Tensor>(output, total_weight);
}
Tensor & CUDAHalfType::nll_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
    auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Half);
    auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::nll_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
    auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Half);
    auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::nll_loss2d_forward_out(Tensor & output, Tensor & total_weight, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Half);
    auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Half);
    auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",5, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    total_weight_->maybe_zero_dim(true);
    return std::tuple<Tensor &, Tensor &>(output, total_weight);
}
std::tuple<Tensor,Tensor> CUDAHalfType::nll_loss2d_forward(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Long);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CUDA, ScalarType::Half);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto total_weight_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto total_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(total_weight_));
    THNN_CudaHalfSpatialClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    total_weight_->maybe_zero_dim(true);
    return std::tuple<Tensor, Tensor>(output, total_weight);
}
Tensor & CUDAHalfType::nll_loss2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
    auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Half);
    auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::nll_loss2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Long);
    auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CUDA, ScalarType::Half);
    auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfSpatialClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::smooth_l1_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
    auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSmoothL1Criterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::smooth_l1_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfSmoothL1Criterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::smooth_l1_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSmoothL1Criterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::smooth_l1_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfSmoothL1Criterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::soft_margin_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
    auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSoftMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::soft_margin_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CUDA, ScalarType::Half);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfSoftMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
    output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::soft_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSoftMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::soft_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfSoftMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::elu_forward_out(Tensor & output, const Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto alpha_ = alpha.toDouble();
    auto scale_ = scale.toDouble();
    auto input_scale_ = input_scale.toDouble();
    auto output_ = checked_tensor_unwrap(output,"output",4, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfELU_updateOutput(globalContext().getTHCState(), self_, output_, alpha_, scale_, input_scale_, false);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::elu_forward(const Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto alpha_ = alpha.toDouble();
    auto scale_ = scale.toDouble();
    auto input_scale_ = input_scale.toDouble();
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfELU_updateOutput(globalContext().getTHCState(), self_, output_, alpha_, scale_, input_scale_, false);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::elu_backward_out(Tensor & grad_input, const Tensor & grad_output, Scalar alpha, Scalar scale, Scalar input_scale, const Tensor & output) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto alpha_ = alpha.toDouble();
    auto scale_ = scale.toDouble();
    auto input_scale_ = input_scale.toDouble();
    auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfELU_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_, alpha_, scale_, input_scale_);
    grad_input_->maybe_zero_dim(output_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::elu_backward(const Tensor & grad_output, Scalar alpha, Scalar scale, Scalar input_scale, const Tensor & output) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto alpha_ = alpha.toDouble();
    auto scale_ = scale.toDouble();
    auto input_scale_ = input_scale.toDouble();
    auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfELU_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_, alpha_, scale_, input_scale_);
    grad_input_->maybe_zero_dim(output_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::elu_forward_(Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto alpha_ = alpha.toDouble();
    auto scale_ = scale.toDouble();
    auto input_scale_ = input_scale.toDouble();
    THNN_CudaHalfELU_updateOutput(globalContext().getTHCState(), self_, self_, alpha_, scale_, input_scale_, true);
    return self;
}
Tensor & CUDAHalfType::glu_forward_out(Tensor & output, const Tensor & self, int64_t dim) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
    output_->maybe_zero_dim(false);
    return output;
}
Tensor CUDAHalfType::glu_forward(const Tensor & self, int64_t dim) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
    output_->maybe_zero_dim(false);
    return output;
}
Tensor & CUDAHalfType::glu_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, int64_t dim) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::glu_backward(const Tensor & grad_output, const Tensor & self, int64_t dim) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    dim = maybe_wrap_dim(dim, self_);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::hardtanh_forward_out(Tensor & output, const Tensor & self, Scalar min_val, Scalar max_val) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto min_val_ = min_val.toDouble();
    auto max_val_ = max_val.toDouble();
    auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfHardTanh_updateOutput(globalContext().getTHCState(), self_, output_, min_val_, max_val_, false);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::hardtanh_forward(const Tensor & self, Scalar min_val, Scalar max_val) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto min_val_ = min_val.toDouble();
    auto max_val_ = max_val.toDouble();
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfHardTanh_updateOutput(globalContext().getTHCState(), self_, output_, min_val_, max_val_, false);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::hardtanh_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar min_val, Scalar max_val) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto min_val_ = min_val.toDouble();
    auto max_val_ = max_val.toDouble();
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfHardTanh_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, min_val_, max_val_, false);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::hardtanh_backward(const Tensor & grad_output, const Tensor & self, Scalar min_val, Scalar max_val) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto min_val_ = min_val.toDouble();
    auto max_val_ = max_val.toDouble();
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfHardTanh_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, min_val_, max_val_, false);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::hardtanh_forward_(Tensor & self, Scalar min_val, Scalar max_val) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto min_val_ = min_val.toDouble();
    auto max_val_ = max_val.toDouble();
    THNN_CudaHalfHardTanh_updateOutput(globalContext().getTHCState(), self_, self_, min_val_, max_val_, true);
    return self;
}
Tensor & CUDAHalfType::leaky_relu_forward_out(Tensor & output, const Tensor & self, Scalar negative_slope) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto negative_slope_ = negative_slope.toDouble();
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfLeakyReLU_updateOutput(globalContext().getTHCState(), self_, output_, negative_slope_, false);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::leaky_relu_forward(const Tensor & self, Scalar negative_slope) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto negative_slope_ = negative_slope.toDouble();
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfLeakyReLU_updateOutput(globalContext().getTHCState(), self_, output_, negative_slope_, false);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::leaky_relu_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar negative_slope) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto negative_slope_ = negative_slope.toDouble();
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfLeakyReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, negative_slope_, false);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::leaky_relu_backward(const Tensor & grad_output, const Tensor & self, Scalar negative_slope) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto negative_slope_ = negative_slope.toDouble();
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfLeakyReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, negative_slope_, false);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::leaky_relu_forward_(Tensor & self, Scalar negative_slope) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto negative_slope_ = negative_slope.toDouble();
    THNN_CudaHalfLeakyReLU_updateOutput(globalContext().getTHCState(), self_, self_, negative_slope_, true);
    return self;
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::log_sigmoid_forward_out(Tensor & output, Tensor & buffer, const Tensor & self) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_ = checked_tensor_unwrap(output,"output",1, false, Backend::CUDA, ScalarType::Half);
    auto buffer_ = checked_tensor_unwrap(buffer,"buffer",1, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
    output_->maybe_zero_dim(self_->dim() == 0);
    buffer_->maybe_zero_dim(self_->dim() == 0);
    return std::tuple<Tensor &, Tensor &>(output, buffer);
}
std::tuple<Tensor,Tensor> CUDAHalfType::log_sigmoid_forward(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto buffer_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto buffer = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(buffer_));
    THNN_CudaHalfLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
    output_->maybe_zero_dim(self_->dim() == 0);
    buffer_->maybe_zero_dim(self_->dim() == 0);
    return std::tuple<Tensor, Tensor>(output, buffer);
}
Tensor & CUDAHalfType::log_sigmoid_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & buffer) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto buffer_ = checked_tensor_unwrap(buffer,"buffer",3, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::log_sigmoid_backward(const Tensor & grad_output, const Tensor & self, const Tensor & buffer) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto buffer_ = checked_tensor_unwrap(buffer,"buffer",3, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::rrelu_with_noise_forward_out(Tensor & output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, Generator * generator) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto noise_ = checked_tensor_unwrap(noise,"noise",2, false, Backend::CUDA, ScalarType::Half);
    auto lower_ = lower.toDouble();
    auto upper_ = upper.toDouble();
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfRReLU_updateOutput(globalContext().getTHCState(), self_, output_, noise_, lower_, upper_, training, false, NULL);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::rrelu_with_noise_forward(const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, Generator * generator) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto noise_ = checked_tensor_unwrap(noise,"noise",2, false, Backend::CUDA, ScalarType::Half);
    auto lower_ = lower.toDouble();
    auto upper_ = upper.toDouble();
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfRReLU_updateOutput(globalContext().getTHCState(), self_, output_, noise_, lower_, upper_, training, false, NULL);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::rrelu_with_noise_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto noise_ = checked_tensor_unwrap(noise,"noise",3, false, Backend::CUDA, ScalarType::Half);
    auto lower_ = lower.toDouble();
    auto upper_ = upper.toDouble();
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",6, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfRReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, noise_, lower_, upper_, training, false);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::rrelu_with_noise_backward(const Tensor & grad_output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto noise_ = checked_tensor_unwrap(noise,"noise",3, false, Backend::CUDA, ScalarType::Half);
    auto lower_ = lower.toDouble();
    auto upper_ = upper.toDouble();
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfRReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, noise_, lower_, upper_, training, false);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::rrelu_with_noise_forward_(Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, Generator * generator) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto noise_ = checked_tensor_unwrap(noise,"noise",2, false, Backend::CUDA, ScalarType::Half);
    auto lower_ = lower.toDouble();
    auto upper_ = upper.toDouble();
    auto generator_ = check_generator<CUDAGenerator>(generator, &globalContext().defaultGenerator(device_type()));
    (void) generator_; //silence unused warning
    THNN_CudaHalfRReLU_updateOutput(globalContext().getTHCState(), self_, self_, noise_, lower_, upper_, training, true, NULL);
    return self;
}
Tensor & CUDAHalfType::softplus_forward_out(Tensor & output, const Tensor & self, Scalar beta, Scalar threshold) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto beta_ = beta.toDouble();
    auto threshold_ = threshold.toDouble();
    auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::softplus_forward(const Tensor & self, Scalar beta, Scalar threshold) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto beta_ = beta.toDouble();
    auto threshold_ = threshold.toDouble();
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::softplus_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar beta, Scalar threshold, const Tensor & output) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto beta_ = beta.toDouble();
    auto threshold_ = threshold.toDouble();
    auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::softplus_backward(const Tensor & grad_output, const Tensor & self, Scalar beta, Scalar threshold, const Tensor & output) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto beta_ = beta.toDouble();
    auto threshold_ = threshold.toDouble();
    auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::softshrink_forward_out(Tensor & output, const Tensor & self, Scalar lambd) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto lambd_ = lambd.toDouble();
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSoftShrink_updateOutput(globalContext().getTHCState(), self_, output_, lambd_);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::softshrink_forward(const Tensor & self, Scalar lambd) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto lambd_ = lambd.toDouble();
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfSoftShrink_updateOutput(globalContext().getTHCState(), self_, output_, lambd_);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::softshrink_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar lambd) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto lambd_ = lambd.toDouble();
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSoftShrink_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, lambd_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::softshrink_backward(const Tensor & grad_output, const Tensor & self, Scalar lambd) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto lambd_ = lambd.toDouble();
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfSoftShrink_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, lambd_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::threshold_forward_out(Tensor & output, const Tensor & self, Scalar threshold, Scalar value) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto threshold_ = threshold.toDouble();
    auto value_ = value.toDouble();
    auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfThreshold_updateOutput(globalContext().getTHCState(), self_, output_, threshold_, value_, false);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::threshold_forward(const Tensor & self, Scalar threshold, Scalar value) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto threshold_ = threshold.toDouble();
    auto value_ = value.toDouble();
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfThreshold_updateOutput(globalContext().getTHCState(), self_, output_, threshold_, value_, false);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::threshold_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar threshold, Scalar value) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto threshold_ = threshold.toDouble();
    auto value_ = value.toDouble();
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfThreshold_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, threshold_, value_, false);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::threshold_backward(const Tensor & grad_output, const Tensor & self, Scalar threshold, Scalar value) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto threshold_ = threshold.toDouble();
    auto value_ = value.toDouble();
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfThreshold_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, threshold_, value_, false);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::threshold_forward_(Tensor & self, Scalar threshold, Scalar value) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto threshold_ = threshold.toDouble();
    auto value_ = value.toDouble();
    THNN_CudaHalfThreshold_updateOutput(globalContext().getTHCState(), self_, self_, threshold_, value_, true);
    return self;
}
Tensor & CUDAHalfType::adaptive_avg_pool2d_forward_out(Tensor & output, const Tensor & self, IntList output_size) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialAdaptiveAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[1], output_size_[0]);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::adaptive_avg_pool2d_forward(const Tensor & self, IntList output_size) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfSpatialAdaptiveAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[1], output_size_[0]);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::adaptive_avg_pool2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialAdaptiveAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::adaptive_avg_pool2d_backward(const Tensor & grad_output, const Tensor & self) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfSpatialAdaptiveAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::adaptive_avg_pool3d_forward_out(Tensor & output, const Tensor & self, IntList output_size) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfVolumetricAdaptiveAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[2], output_size_[1]);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::adaptive_avg_pool3d_forward(const Tensor & self, IntList output_size) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfVolumetricAdaptiveAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[2], output_size_[1]);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::adaptive_avg_pool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfVolumetricAdaptiveAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::adaptive_avg_pool3d_backward(const Tensor & grad_output, const Tensor & self) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfVolumetricAdaptiveAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::adaptive_max_pool2d_forward_out(Tensor & output, Tensor & indices, const Tensor & self, IntList output_size) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
    THNN_CudaHalfSpatialAdaptiveMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[1], output_size_[0]);
    bool maybe_scalar = self_->dim() == 0;
    output_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(output, indices);
}
std::tuple<Tensor,Tensor> CUDAHalfType::adaptive_max_pool2d_forward(const Tensor & self, IntList output_size) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
    THNN_CudaHalfSpatialAdaptiveMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[1], output_size_[0]);
    bool maybe_scalar = self_->dim() == 0;
    output_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(output, indices);
}
Tensor & CUDAHalfType::adaptive_max_pool2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialAdaptiveMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::adaptive_max_pool2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfSpatialAdaptiveMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::adaptive_max_pool3d_forward_out(Tensor & output, Tensor & indices, const Tensor & self, IntList output_size) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
    THNN_CudaHalfVolumetricAdaptiveMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[0], output_size_[2], output_size_[1]);
    bool maybe_scalar = self_->dim() == 0;
    output_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(output, indices);
}
std::tuple<Tensor,Tensor> CUDAHalfType::adaptive_max_pool3d_forward(const Tensor & self, IntList output_size) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
    THNN_CudaHalfVolumetricAdaptiveMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[0], output_size_[2], output_size_[1]);
    bool maybe_scalar = self_->dim() == 0;
    output_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(output, indices);
}
Tensor & CUDAHalfType::adaptive_max_pool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfVolumetricAdaptiveMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::adaptive_max_pool3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfVolumetricAdaptiveMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::avg_pool2d_forward_out(Tensor & output, const Tensor & self, IntList kernel_size, IntList stride, IntList padding, bool ceil_mode, bool count_include_pad) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
    auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
    auto padding_ = check_intlist<2>(padding, "padding", 4);
    auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::avg_pool2d_forward(const Tensor & self, IntList kernel_size, IntList stride, IntList padding, bool ceil_mode, bool count_include_pad) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
    auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
    auto padding_ = check_intlist<2>(padding, "padding", 4);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfSpatialAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::avg_pool2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntList kernel_size, IntList stride, IntList padding, bool ceil_mode, bool count_include_pad) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
    auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
    auto padding_ = check_intlist<2>(padding, "padding", 5);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::avg_pool2d_backward(const Tensor & grad_output, const Tensor & self, IntList kernel_size, IntList stride, IntList padding, bool ceil_mode, bool count_include_pad) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
    auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
    auto padding_ = check_intlist<2>(padding, "padding", 5);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfSpatialAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::avg_pool3d_forward_out(Tensor & output, const Tensor & self, IntList kernel_size, IntList stride, IntList padding, bool ceil_mode, bool count_include_pad) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
    auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
    auto padding_ = check_intlist<3>(padding, "padding", 4);
    auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfVolumetricAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::avg_pool3d_forward(const Tensor & self, IntList kernel_size, IntList stride, IntList padding, bool ceil_mode, bool count_include_pad) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
    auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
    auto padding_ = check_intlist<3>(padding, "padding", 4);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfVolumetricAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::avg_pool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntList kernel_size, IntList stride, IntList padding, bool ceil_mode, bool count_include_pad) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
    auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
    auto padding_ = check_intlist<3>(padding, "padding", 5);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfVolumetricAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::avg_pool3d_backward(const Tensor & grad_output, const Tensor & self, IntList kernel_size, IntList stride, IntList padding, bool ceil_mode, bool count_include_pad) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
    auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
    auto padding_ = check_intlist<3>(padding, "padding", 5);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfVolumetricAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::fractional_max_pool2d_forward_out(Tensor & output, Tensor & indices, const Tensor & self, IntList kernel_size, IntList output_size, const Tensor & random_samples) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 3);
    auto random_samples_ = checked_tensor_unwrap(random_samples,"random_samples",4, false, Backend::CUDA, ScalarType::Half);
    auto output_ = checked_tensor_unwrap(output,"output",4, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",4, false, Backend::CUDA, ScalarType::Long);
    THNN_CudaHalfSpatialFractionalMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[1], output_size_[0], kernel_size_[1], kernel_size_[0], indices_, random_samples_);
    output_->maybe_zero_dim(false);
    return std::tuple<Tensor &, Tensor &>(output, indices);
}
std::tuple<Tensor,Tensor> CUDAHalfType::fractional_max_pool2d_forward(const Tensor & self, IntList kernel_size, IntList output_size, const Tensor & random_samples) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 3);
    auto random_samples_ = checked_tensor_unwrap(random_samples,"random_samples",4, false, Backend::CUDA, ScalarType::Half);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
    THNN_CudaHalfSpatialFractionalMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[1], output_size_[0], kernel_size_[1], kernel_size_[0], indices_, random_samples_);
    output_->maybe_zero_dim(false);
    return std::tuple<Tensor, Tensor>(output, indices);
}
Tensor & CUDAHalfType::fractional_max_pool2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntList kernel_size, IntList output_size, const Tensor & indices) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 4);
    auto indices_ = checked_tensor_unwrap(indices,"indices",5, false, Backend::CUDA, ScalarType::Long);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialFractionalMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_size_[1], output_size_[0], kernel_size_[1], kernel_size_[0], indices_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::fractional_max_pool2d_backward(const Tensor & grad_output, const Tensor & self, IntList kernel_size, IntList output_size, const Tensor & indices) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 4);
    auto indices_ = checked_tensor_unwrap(indices,"indices",5, false, Backend::CUDA, ScalarType::Long);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfSpatialFractionalMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_size_[1], output_size_[0], kernel_size_[1], kernel_size_[0], indices_);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::max_pool2d_with_indices_forward_out(Tensor & output, Tensor & indices, const Tensor & self, IntList kernel_size, IntList stride, IntList padding, IntList dilation, bool ceil_mode) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
    auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
    auto padding_ = check_intlist<2>(padding, "padding", 4);
    auto dilation_ = check_intlist<2>(dilation, "dilation", 5);
    auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",6, false, Backend::CUDA, ScalarType::Long);
    THNN_CudaHalfSpatialDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
    bool maybe_scalar = self_->dim() == 0;
    output_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(output, indices);
}
std::tuple<Tensor,Tensor> CUDAHalfType::max_pool2d_with_indices_forward(const Tensor & self, IntList kernel_size, IntList stride, IntList padding, IntList dilation, bool ceil_mode) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
    auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
    auto padding_ = check_intlist<2>(padding, "padding", 4);
    auto dilation_ = check_intlist<2>(dilation, "dilation", 5);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
    THNN_CudaHalfSpatialDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
    bool maybe_scalar = self_->dim() == 0;
    output_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(output, indices);
}
Tensor & CUDAHalfType::max_pool2d_with_indices_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntList kernel_size, IntList stride, IntList padding, IntList dilation, bool ceil_mode, const Tensor & indices) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
    auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
    auto padding_ = check_intlist<2>(padding, "padding", 5);
    auto dilation_ = check_intlist<2>(dilation, "dilation", 6);
    auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CUDA, ScalarType::Long);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",8, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::max_pool2d_with_indices_backward(const Tensor & grad_output, const Tensor & self, IntList kernel_size, IntList stride, IntList padding, IntList dilation, bool ceil_mode, const Tensor & indices) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
    auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
    auto padding_ = check_intlist<2>(padding, "padding", 5);
    auto dilation_ = check_intlist<2>(dilation, "dilation", 6);
    auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CUDA, ScalarType::Long);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfSpatialDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::max_pool3d_with_indices_forward_out(Tensor & output, Tensor & indices, const Tensor & self, IntList kernel_size, IntList stride, IntList padding, IntList dilation, bool ceil_mode) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
    auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
    auto padding_ = check_intlist<3>(padding, "padding", 4);
    auto dilation_ = check_intlist<3>(dilation, "dilation", 5);
    auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",6, false, Backend::CUDA, ScalarType::Long);
    THNN_CudaHalfVolumetricDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
    bool maybe_scalar = self_->dim() == 0;
    output_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &>(output, indices);
}
std::tuple<Tensor,Tensor> CUDAHalfType::max_pool3d_with_indices_forward(const Tensor & self, IntList kernel_size, IntList stride, IntList padding, IntList dilation, bool ceil_mode) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
    auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
    auto padding_ = check_intlist<3>(padding, "padding", 4);
    auto dilation_ = check_intlist<3>(dilation, "dilation", 5);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), scalarTypeToTypeMeta(ScalarType::Long), allocator(), false).release();
    auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
    THNN_CudaHalfVolumetricDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
    bool maybe_scalar = self_->dim() == 0;
    output_->maybe_zero_dim(maybe_scalar);
    indices_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor>(output, indices);
}
Tensor & CUDAHalfType::max_pool3d_with_indices_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntList kernel_size, IntList stride, IntList padding, IntList dilation, bool ceil_mode, const Tensor & indices) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
    auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
    auto padding_ = check_intlist<3>(padding, "padding", 5);
    auto dilation_ = check_intlist<3>(dilation, "dilation", 6);
    auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CUDA, ScalarType::Long);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",8, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfVolumetricDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::max_pool3d_with_indices_backward(const Tensor & grad_output, const Tensor & self, IntList kernel_size, IntList stride, IntList padding, IntList dilation, bool ceil_mode, const Tensor & indices) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
    auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
    auto padding_ = check_intlist<3>(padding, "padding", 5);
    auto dilation_ = check_intlist<3>(dilation, "dilation", 6);
    auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CUDA, ScalarType::Long);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfVolumetricDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::max_unpool2d_forward_out(Tensor & output, const Tensor & self, const Tensor & indices, IntList output_size) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 3);
    auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[1], output_size_[0]);
    output_->maybe_zero_dim(self_->dim() == 0 && indices_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::max_unpool2d_forward(const Tensor & self, const Tensor & indices, IntList output_size) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 3);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfSpatialMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[1], output_size_[0]);
    output_->maybe_zero_dim(self_->dim() == 0 && indices_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::max_unpool2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntList output_size) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 4);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[1], output_size_[0]);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::max_unpool2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntList output_size) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 4);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfSpatialMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[1], output_size_[0]);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::max_unpool3d_forward_out(Tensor & output, const Tensor & self, const Tensor & indices, IntList output_size, IntList stride, IntList padding) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
    auto output_size_ = check_intlist<3>(output_size, "output_size", 3);
    auto stride_ = check_intlist<3>(stride, "stride", 4);
    auto padding_ = check_intlist<3>(padding, "padding", 5);
    auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfVolumetricMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
    output_->maybe_zero_dim(self_->dim() == 0 && indices_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::max_unpool3d_forward(const Tensor & self, const Tensor & indices, IntList output_size, IntList stride, IntList padding) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CUDA, ScalarType::Long);
    auto output_size_ = check_intlist<3>(output_size, "output_size", 3);
    auto stride_ = check_intlist<3>(stride, "stride", 4);
    auto padding_ = check_intlist<3>(padding, "padding", 5);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfVolumetricMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
    output_->maybe_zero_dim(self_->dim() == 0 && indices_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::max_unpool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntList output_size, IntList stride, IntList padding) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
    auto output_size_ = check_intlist<3>(output_size, "output_size", 4);
    auto stride_ = check_intlist<3>(stride, "stride", 5);
    auto padding_ = check_intlist<3>(padding, "padding", 6);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",6, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfVolumetricMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::max_unpool3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntList output_size, IntList stride, IntList padding) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CUDA, ScalarType::Long);
    auto output_size_ = check_intlist<3>(output_size, "output_size", 4);
    auto stride_ = check_intlist<3>(stride, "stride", 5);
    auto padding_ = check_intlist<3>(padding, "padding", 6);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfVolumetricMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::reflection_pad1d_forward_out(Tensor & output, const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<2>(padding, "padding", 2);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfTemporalReflectionPadding_updateOutput(globalContext().getTHCState(), self_, output_, padding_[0], padding_[1]);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::reflection_pad1d_forward(const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<2>(padding, "padding", 2);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfTemporalReflectionPadding_updateOutput(globalContext().getTHCState(), self_, output_, padding_[0], padding_[1]);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::reflection_pad1d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<2>(padding, "padding", 3);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfTemporalReflectionPadding_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, padding_[0], padding_[1]);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::reflection_pad1d_backward(const Tensor & grad_output, const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<2>(padding, "padding", 3);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfTemporalReflectionPadding_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, padding_[0], padding_[1]);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::reflection_pad2d_forward_out(Tensor & output, const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<4>(padding, "padding", 2);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialReflectionPadding_updateOutput(globalContext().getTHCState(), self_, output_, padding_[0], padding_[1], padding_[2], padding_[3]);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::reflection_pad2d_forward(const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<4>(padding, "padding", 2);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfSpatialReflectionPadding_updateOutput(globalContext().getTHCState(), self_, output_, padding_[0], padding_[1], padding_[2], padding_[3]);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::reflection_pad2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<4>(padding, "padding", 3);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialReflectionPadding_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, padding_[0], padding_[1], padding_[2], padding_[3]);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::reflection_pad2d_backward(const Tensor & grad_output, const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<4>(padding, "padding", 3);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfSpatialReflectionPadding_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, padding_[0], padding_[1], padding_[2], padding_[3]);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::replication_pad1d_forward_out(Tensor & output, const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<2>(padding, "padding", 2);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfTemporalReplicationPadding_updateOutput(globalContext().getTHCState(), self_, output_, padding_[0], padding_[1]);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::replication_pad1d_forward(const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<2>(padding, "padding", 2);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfTemporalReplicationPadding_updateOutput(globalContext().getTHCState(), self_, output_, padding_[0], padding_[1]);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::replication_pad1d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<2>(padding, "padding", 3);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfTemporalReplicationPadding_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, padding_[0], padding_[1]);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::replication_pad1d_backward(const Tensor & grad_output, const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<2>(padding, "padding", 3);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfTemporalReplicationPadding_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, padding_[0], padding_[1]);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::replication_pad2d_forward_out(Tensor & output, const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<4>(padding, "padding", 2);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialReplicationPadding_updateOutput(globalContext().getTHCState(), self_, output_, padding_[0], padding_[1], padding_[2], padding_[3]);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::replication_pad2d_forward(const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<4>(padding, "padding", 2);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfSpatialReplicationPadding_updateOutput(globalContext().getTHCState(), self_, output_, padding_[0], padding_[1], padding_[2], padding_[3]);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::replication_pad2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<4>(padding, "padding", 3);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialReplicationPadding_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, padding_[0], padding_[1], padding_[2], padding_[3]);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::replication_pad2d_backward(const Tensor & grad_output, const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<4>(padding, "padding", 3);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfSpatialReplicationPadding_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, padding_[0], padding_[1], padding_[2], padding_[3]);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::replication_pad3d_forward_out(Tensor & output, const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<6>(padding, "padding", 2);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfVolumetricReplicationPadding_updateOutput(globalContext().getTHCState(), self_, output_, padding_[0], padding_[1], padding_[2], padding_[3], padding_[4], padding_[5]);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::replication_pad3d_forward(const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<6>(padding, "padding", 2);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfVolumetricReplicationPadding_updateOutput(globalContext().getTHCState(), self_, output_, padding_[0], padding_[1], padding_[2], padding_[3], padding_[4], padding_[5]);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::replication_pad3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<6>(padding, "padding", 3);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfVolumetricReplicationPadding_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, padding_[0], padding_[1], padding_[2], padding_[3], padding_[4], padding_[5]);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::replication_pad3d_backward(const Tensor & grad_output, const Tensor & self, IntList padding) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto padding_ = check_intlist<6>(padding, "padding", 3);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfVolumetricReplicationPadding_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, padding_[0], padding_[1], padding_[2], padding_[3], padding_[4], padding_[5]);
    grad_input_->maybe_zero_dim(self_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::upsample_linear1d_forward_out(Tensor & output, const Tensor & self, IntList output_size, bool align_corners) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
    auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfTemporalUpSamplingLinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], align_corners);
    return output;
}
Tensor CUDAHalfType::upsample_linear1d_forward(const Tensor & self, IntList output_size, bool align_corners) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfTemporalUpSamplingLinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], align_corners);
    return output;
}
Tensor & CUDAHalfType::upsample_linear1d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntList output_size, IntList input_size, bool align_corners) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
    auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfTemporalUpSamplingLinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0], align_corners);
    grad_input_->maybe_zero_dim(false);
    return grad_input;
}
Tensor CUDAHalfType::upsample_linear1d_backward(const Tensor & grad_output, IntList output_size, IntList input_size, bool align_corners) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
    auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfTemporalUpSamplingLinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0], align_corners);
    grad_input_->maybe_zero_dim(false);
    return grad_input;
}
Tensor & CUDAHalfType::upsample_bilinear2d_forward_out(Tensor & output, const Tensor & self, IntList output_size, bool align_corners) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
    auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialUpSamplingBilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
    return output;
}
Tensor CUDAHalfType::upsample_bilinear2d_forward(const Tensor & self, IntList output_size, bool align_corners) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfSpatialUpSamplingBilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
    return output;
}
Tensor & CUDAHalfType::upsample_bilinear2d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntList output_size, IntList input_size, bool align_corners) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
    auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialUpSamplingBilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
    grad_input_->maybe_zero_dim(false);
    return grad_input;
}
Tensor CUDAHalfType::upsample_bilinear2d_backward(const Tensor & grad_output, IntList output_size, IntList input_size, bool align_corners) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
    auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfSpatialUpSamplingBilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
    grad_input_->maybe_zero_dim(false);
    return grad_input;
}
Tensor & CUDAHalfType::upsample_trilinear3d_forward_out(Tensor & output, const Tensor & self, IntList output_size, bool align_corners) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
    auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfVolumetricUpSamplingTrilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2], align_corners);
    return output;
}
Tensor CUDAHalfType::upsample_trilinear3d_forward(const Tensor & self, IntList output_size, bool align_corners) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfVolumetricUpSamplingTrilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2], align_corners);
    return output;
}
Tensor & CUDAHalfType::upsample_trilinear3d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntList output_size, IntList input_size, bool align_corners) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
    auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfVolumetricUpSamplingTrilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2], align_corners);
    grad_input_->maybe_zero_dim(false);
    return grad_input;
}
Tensor CUDAHalfType::upsample_trilinear3d_backward(const Tensor & grad_output, IntList output_size, IntList input_size, bool align_corners) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
    auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfVolumetricUpSamplingTrilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2], align_corners);
    grad_input_->maybe_zero_dim(false);
    return grad_input;
}
Tensor & CUDAHalfType::upsample_nearest1d_forward_out(Tensor & output, const Tensor & self, IntList output_size) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfTemporalUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0]);
    return output;
}
Tensor CUDAHalfType::upsample_nearest1d_forward(const Tensor & self, IntList output_size) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfTemporalUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0]);
    return output;
}
Tensor & CUDAHalfType::upsample_nearest1d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntList output_size, IntList input_size) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
    auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfTemporalUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0]);
    grad_input_->maybe_zero_dim(false);
    return grad_input;
}
Tensor CUDAHalfType::upsample_nearest1d_backward(const Tensor & grad_output, IntList output_size, IntList input_size) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
    auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfTemporalUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0]);
    grad_input_->maybe_zero_dim(false);
    return grad_input;
}
Tensor & CUDAHalfType::upsample_nearest2d_forward_out(Tensor & output, const Tensor & self, IntList output_size) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1]);
    return output;
}
Tensor CUDAHalfType::upsample_nearest2d_forward(const Tensor & self, IntList output_size) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfSpatialUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1]);
    return output;
}
Tensor & CUDAHalfType::upsample_nearest2d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntList output_size, IntList input_size) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
    auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1]);
    grad_input_->maybe_zero_dim(false);
    return grad_input;
}
Tensor CUDAHalfType::upsample_nearest2d_backward(const Tensor & grad_output, IntList output_size, IntList input_size) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
    auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfSpatialUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1]);
    grad_input_->maybe_zero_dim(false);
    return grad_input;
}
Tensor & CUDAHalfType::upsample_nearest3d_forward_out(Tensor & output, const Tensor & self, IntList output_size) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfVolumetricUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2]);
    return output;
}
Tensor CUDAHalfType::upsample_nearest3d_forward(const Tensor & self, IntList output_size) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfVolumetricUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2]);
    return output;
}
Tensor & CUDAHalfType::upsample_nearest3d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntList output_size, IntList input_size) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
    auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfVolumetricUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2]);
    grad_input_->maybe_zero_dim(false);
    return grad_input;
}
Tensor CUDAHalfType::upsample_nearest3d_backward(const Tensor & grad_output, IntList output_size, IntList input_size) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
    auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfVolumetricUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2]);
    grad_input_->maybe_zero_dim(false);
    return grad_input;
}
Tensor & CUDAHalfType::_sigmoid_forward_out(Tensor & output, const Tensor & self) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_ = checked_tensor_unwrap(output,"output",1, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSigmoid_updateOutput(globalContext().getTHCState(), self_, output_);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::_sigmoid_forward(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfSigmoid_updateOutput(globalContext().getTHCState(), self_, output_);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::_sigmoid_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & output) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSigmoid_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
    grad_input_->maybe_zero_dim(output_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::_sigmoid_backward(const Tensor & grad_output, const Tensor & output) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfSigmoid_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
    grad_input_->maybe_zero_dim(output_->dim() == 0);
    return grad_input;
}
Tensor & CUDAHalfType::_tanh_forward_out(Tensor & output, const Tensor & self) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_ = checked_tensor_unwrap(output,"output",1, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfTanh_updateOutput(globalContext().getTHCState(), self_, output_);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor CUDAHalfType::_tanh_forward(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfTanh_updateOutput(globalContext().getTHCState(), self_, output_);
    output_->maybe_zero_dim(self_->dim() == 0);
    return output;
}
Tensor & CUDAHalfType::_tanh_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & output) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",2, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfTanh_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
    grad_input_->maybe_zero_dim(output_->dim() == 0);
    return grad_input;
}
Tensor CUDAHalfType::_tanh_backward(const Tensor & grad_output, const Tensor & output) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
    THNN_CudaHalfTanh_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
    grad_input_->maybe_zero_dim(output_->dim() == 0);
    return grad_input;
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAHalfType::thnn_conv_transpose2d_forward_out(Tensor & output, Tensor & columns, Tensor & ones, const Tensor & self, const Tensor & weight, IntList kernel_size, const Tensor & bias, IntList stride, IntList padding, IntList output_padding, IntList dilation) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
    auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
    auto stride_ = check_intlist<2>(stride, "stride", 5);
    auto padding_ = check_intlist<2>(padding, "padding", 6);
    auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
    auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
    auto output_ = checked_tensor_unwrap(output,"output",8, false, Backend::CUDA, ScalarType::Half);
    auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Half);
    auto ones_ = checked_tensor_unwrap(ones,"ones",8, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
    bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
    output_->maybe_zero_dim(maybe_scalar);
    columns_->maybe_zero_dim(maybe_scalar);
    ones_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &, Tensor &>(output, columns, ones);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::thnn_conv_transpose2d_forward(const Tensor & self, const Tensor & weight, IntList kernel_size, const Tensor & bias, IntList stride, IntList padding, IntList output_padding, IntList dilation) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
    auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
    auto stride_ = check_intlist<2>(stride, "stride", 5);
    auto padding_ = check_intlist<2>(padding, "padding", 6);
    auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
    auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto columns_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto columns = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(columns_));
    auto ones_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto ones = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(ones_));
    THNN_CudaHalfSpatialFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
    bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
    output_->maybe_zero_dim(maybe_scalar);
    columns_->maybe_zero_dim(maybe_scalar);
    ones_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor, Tensor>(output, columns, ones);
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAHalfType::thnn_conv_transpose2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntList kernel_size, IntList stride, IntList padding, IntList output_padding, IntList dilation, const Tensor & columns, const Tensor & ones) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
    auto stride_ = check_intlist<2>(stride, "stride", 5);
    auto padding_ = check_intlist<2>(padding, "padding", 6);
    auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
    auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
    auto columns_ = checked_tensor_unwrap(columns,"columns",9, false, Backend::CUDA, ScalarType::Half);
    auto ones_ = checked_tensor_unwrap(ones,"ones",10, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",10, true, Backend::CUDA, ScalarType::Half);
    auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",10, true, Backend::CUDA, ScalarType::Half);
    if (grad_weight.defined()) {
        grad_weight.resize_(weight.sizes());
        grad_weight.zero_();
    }
    auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",10, true, Backend::CUDA, ScalarType::Half);
    if (grad_bias.defined()) {
        grad_bias.resize_({ weight.size(1) });
        grad_bias.zero_();
    }
    if (grad_input_) THNN_CudaHalfSpatialFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
    if (grad_weight_ || grad_bias_) THNN_CudaHalfSpatialFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0], 1);
    if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
    return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::thnn_conv_transpose2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntList kernel_size, IntList stride, IntList padding, IntList output_padding, IntList dilation, const Tensor & columns, const Tensor & ones, std::array<bool,3> output_mask) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
    auto stride_ = check_intlist<2>(stride, "stride", 5);
    auto padding_ = check_intlist<2>(padding, "padding", 6);
    auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
    auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
    auto columns_ = checked_tensor_unwrap(columns,"columns",9, false, Backend::CUDA, ScalarType::Half);
    auto ones_ = checked_tensor_unwrap(ones,"ones",10, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
    auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
    if (grad_weight.defined()) {
        grad_weight.resize_(weight.sizes());
        grad_weight.zero_();
    }
    auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
    if (grad_bias.defined()) {
        grad_bias.resize_({ weight.size(1) });
        grad_bias.zero_();
    }
    if (grad_input_) THNN_CudaHalfSpatialFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
    if (grad_weight_ || grad_bias_) THNN_CudaHalfSpatialFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0], 1);
    if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
    return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAHalfType::thnn_conv_transpose3d_forward_out(Tensor & output, Tensor & finput, Tensor & fgrad_input, const Tensor & self, const Tensor & weight, IntList kernel_size, const Tensor & bias, IntList stride, IntList padding, IntList output_padding, IntList dilation) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
    auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
    auto stride_ = check_intlist<3>(stride, "stride", 5);
    auto padding_ = check_intlist<3>(padding, "padding", 6);
    auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
    auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
    auto output_ = checked_tensor_unwrap(output,"output",8, false, Backend::CUDA, ScalarType::Half);
    auto finput_ = checked_tensor_unwrap(finput,"finput",8, false, Backend::CUDA, ScalarType::Half);
    auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfVolumetricFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
    bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
    output_->maybe_zero_dim(maybe_scalar);
    finput_->maybe_zero_dim(maybe_scalar);
    fgrad_input_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &, Tensor &>(output, finput, fgrad_input);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::thnn_conv_transpose3d_forward(const Tensor & self, const Tensor & weight, IntList kernel_size, const Tensor & bias, IntList stride, IntList padding, IntList output_padding, IntList dilation) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
    auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
    auto stride_ = check_intlist<3>(stride, "stride", 5);
    auto padding_ = check_intlist<3>(padding, "padding", 6);
    auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
    auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto finput_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto finput = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(finput_));
    auto fgrad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto fgrad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(fgrad_input_));
    THNN_CudaHalfVolumetricFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
    bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
    output_->maybe_zero_dim(maybe_scalar);
    finput_->maybe_zero_dim(maybe_scalar);
    fgrad_input_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor, Tensor>(output, finput, fgrad_input);
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAHalfType::thnn_conv_transpose3d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntList kernel_size, IntList stride, IntList padding, IntList output_padding, IntList dilation, const Tensor & finput, const Tensor & fgrad_input) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
    auto stride_ = check_intlist<3>(stride, "stride", 5);
    auto padding_ = check_intlist<3>(padding, "padding", 6);
    auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
    auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
    auto finput_ = checked_tensor_unwrap(finput,"finput",9, false, Backend::CUDA, ScalarType::Half);
    auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",10, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",10, true, Backend::CUDA, ScalarType::Half);
    auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",10, true, Backend::CUDA, ScalarType::Half);
    if (grad_weight.defined()) {
        grad_weight.resize_(weight.sizes());
        grad_weight.zero_();
    }
    auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",10, true, Backend::CUDA, ScalarType::Half);
    if (grad_bias.defined()) {
        grad_bias.resize_({ weight.size(1) });
        grad_bias.zero_();
    }
    if (grad_input_) THNN_CudaHalfVolumetricFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
    if (grad_weight_ || grad_bias_) THNN_CudaHalfVolumetricFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1], 1);
    if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
    return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::thnn_conv_transpose3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntList kernel_size, IntList stride, IntList padding, IntList output_padding, IntList dilation, const Tensor & finput, const Tensor & fgrad_input, std::array<bool,3> output_mask) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
    auto stride_ = check_intlist<3>(stride, "stride", 5);
    auto padding_ = check_intlist<3>(padding, "padding", 6);
    auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
    auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
    auto finput_ = checked_tensor_unwrap(finput,"finput",9, false, Backend::CUDA, ScalarType::Half);
    auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",10, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
    auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
    if (grad_weight.defined()) {
        grad_weight.resize_(weight.sizes());
        grad_weight.zero_();
    }
    auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
    if (grad_bias.defined()) {
        grad_bias.resize_({ weight.size(1) });
        grad_bias.zero_();
    }
    if (grad_input_) THNN_CudaHalfVolumetricFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
    if (grad_weight_ || grad_bias_) THNN_CudaHalfVolumetricFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1], 1);
    if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
    return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAHalfType::thnn_conv2d_forward_out(Tensor & output, Tensor & finput, Tensor & fgrad_input, const Tensor & self, const Tensor & weight, IntList kernel_size, const Tensor & bias, IntList stride, IntList padding) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
    auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
    auto stride_ = check_intlist<2>(stride, "stride", 5);
    auto padding_ = check_intlist<2>(padding, "padding", 6);
    auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CUDA, ScalarType::Half);
    auto finput_ = checked_tensor_unwrap(finput,"finput",6, false, Backend::CUDA, ScalarType::Half);
    auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",6, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
    bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
    output_->maybe_zero_dim(maybe_scalar);
    finput_->maybe_zero_dim(maybe_scalar);
    fgrad_input_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &, Tensor &>(output, finput, fgrad_input);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::thnn_conv2d_forward(const Tensor & self, const Tensor & weight, IntList kernel_size, const Tensor & bias, IntList stride, IntList padding) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
    auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
    auto stride_ = check_intlist<2>(stride, "stride", 5);
    auto padding_ = check_intlist<2>(padding, "padding", 6);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto finput_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto finput = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(finput_));
    auto fgrad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto fgrad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(fgrad_input_));
    THNN_CudaHalfSpatialConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
    bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
    output_->maybe_zero_dim(maybe_scalar);
    finput_->maybe_zero_dim(maybe_scalar);
    fgrad_input_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor, Tensor>(output, finput, fgrad_input);
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAHalfType::thnn_conv2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntList kernel_size, IntList stride, IntList padding, const Tensor & finput, const Tensor & fgrad_input) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
    auto stride_ = check_intlist<2>(stride, "stride", 5);
    auto padding_ = check_intlist<2>(padding, "padding", 6);
    auto finput_ = checked_tensor_unwrap(finput,"finput",7, false, Backend::CUDA, ScalarType::Half);
    auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",8, true, Backend::CUDA, ScalarType::Half);
    auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",8, true, Backend::CUDA, ScalarType::Half);
    if (grad_weight.defined()) {
        grad_weight.resize_(weight.sizes());
        grad_weight.zero_();
    }
    auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",8, true, Backend::CUDA, ScalarType::Half);
    if (grad_bias.defined()) {
        grad_bias.resize_({ weight.size(0) });
        grad_bias.zero_();
    }
    if (grad_input_) THNN_CudaHalfSpatialConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
    if (grad_weight_ || grad_bias_) THNN_CudaHalfSpatialConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], 1);
    if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
    return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::thnn_conv2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntList kernel_size, IntList stride, IntList padding, const Tensor & finput, const Tensor & fgrad_input, std::array<bool,3> output_mask) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
    auto stride_ = check_intlist<2>(stride, "stride", 5);
    auto padding_ = check_intlist<2>(padding, "padding", 6);
    auto finput_ = checked_tensor_unwrap(finput,"finput",7, false, Backend::CUDA, ScalarType::Half);
    auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
    auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
    if (grad_weight.defined()) {
        grad_weight.resize_(weight.sizes());
        grad_weight.zero_();
    }
    auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
    if (grad_bias.defined()) {
        grad_bias.resize_({ weight.size(0) });
        grad_bias.zero_();
    }
    if (grad_input_) THNN_CudaHalfSpatialConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
    if (grad_weight_ || grad_bias_) THNN_CudaHalfSpatialConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], 1);
    if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
    return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
}
Tensor & CUDAHalfType::thnn_conv_depthwise2d_forward_out(Tensor & output, const Tensor & self, const Tensor & weight, IntList kernel_size, const Tensor & bias, IntList stride, IntList padding, IntList dilation) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
    auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
    auto stride_ = check_intlist<2>(stride, "stride", 5);
    auto padding_ = check_intlist<2>(padding, "padding", 6);
    auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
    auto output_ = checked_tensor_unwrap(output,"output",7, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialDepthwiseConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
    output_->maybe_zero_dim(self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0));
    return output;
}
Tensor CUDAHalfType::thnn_conv_depthwise2d_forward(const Tensor & self, const Tensor & weight, IntList kernel_size, const Tensor & bias, IntList stride, IntList padding, IntList dilation) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
    auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
    auto stride_ = check_intlist<2>(stride, "stride", 5);
    auto padding_ = check_intlist<2>(padding, "padding", 6);
    auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    THNN_CudaHalfSpatialDepthwiseConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
    output_->maybe_zero_dim(self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0));
    return output;
}
std::tuple<Tensor &,Tensor &> CUDAHalfType::thnn_conv_depthwise2d_backward_out(Tensor & grad_input, Tensor & grad_weight, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntList kernel_size, IntList stride, IntList padding, IntList dilation) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
    auto stride_ = check_intlist<2>(stride, "stride", 5);
    auto padding_ = check_intlist<2>(padding, "padding", 6);
    auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, true, Backend::CUDA, ScalarType::Half);
    auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",7, true, Backend::CUDA, ScalarType::Half);
    if (grad_weight.defined()) {
        grad_weight.resize_(weight.sizes());
        grad_weight.zero_();
    }
    if (grad_input_) THNN_CudaHalfSpatialDepthwiseConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
    if (grad_weight_) THNN_CudaHalfSpatialDepthwiseConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
    if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
    return std::tuple<Tensor &, Tensor &>(grad_input, grad_weight);
}
std::tuple<Tensor,Tensor> CUDAHalfType::thnn_conv_depthwise2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntList kernel_size, IntList stride, IntList padding, IntList dilation, std::array<bool,2> output_mask) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
    auto stride_ = check_intlist<2>(stride, "stride", 5);
    auto padding_ = check_intlist<2>(padding, "padding", 6);
    auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
    auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
    auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
    if (grad_weight.defined()) {
        grad_weight.resize_(weight.sizes());
        grad_weight.zero_();
    }
    if (grad_input_) THNN_CudaHalfSpatialDepthwiseConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
    if (grad_weight_) THNN_CudaHalfSpatialDepthwiseConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
    if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
    return std::tuple<Tensor, Tensor>(grad_input, grad_weight);
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAHalfType::thnn_conv_dilated2d_forward_out(Tensor & output, Tensor & columns, Tensor & ones, const Tensor & self, const Tensor & weight, IntList kernel_size, const Tensor & bias, IntList stride, IntList padding, IntList dilation) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
    auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
    auto stride_ = check_intlist<2>(stride, "stride", 5);
    auto padding_ = check_intlist<2>(padding, "padding", 6);
    auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
    auto output_ = checked_tensor_unwrap(output,"output",7, false, Backend::CUDA, ScalarType::Half);
    auto columns_ = checked_tensor_unwrap(columns,"columns",7, false, Backend::CUDA, ScalarType::Half);
    auto ones_ = checked_tensor_unwrap(ones,"ones",7, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfSpatialDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
    bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
    output_->maybe_zero_dim(maybe_scalar);
    columns_->maybe_zero_dim(maybe_scalar);
    ones_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &, Tensor &>(output, columns, ones);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::thnn_conv_dilated2d_forward(const Tensor & self, const Tensor & weight, IntList kernel_size, const Tensor & bias, IntList stride, IntList padding, IntList dilation) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
    auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
    auto stride_ = check_intlist<2>(stride, "stride", 5);
    auto padding_ = check_intlist<2>(padding, "padding", 6);
    auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto columns_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto columns = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(columns_));
    auto ones_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto ones = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(ones_));
    THNN_CudaHalfSpatialDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
    bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
    output_->maybe_zero_dim(maybe_scalar);
    columns_->maybe_zero_dim(maybe_scalar);
    ones_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor, Tensor>(output, columns, ones);
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAHalfType::thnn_conv_dilated2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntList kernel_size, IntList stride, IntList padding, IntList dilation, const Tensor & columns, const Tensor & ones) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
    auto stride_ = check_intlist<2>(stride, "stride", 5);
    auto padding_ = check_intlist<2>(padding, "padding", 6);
    auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
    auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Half);
    auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",9, true, Backend::CUDA, ScalarType::Half);
    auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",9, true, Backend::CUDA, ScalarType::Half);
    if (grad_weight.defined()) {
        grad_weight.resize_(weight.sizes());
        grad_weight.zero_();
    }
    auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",9, true, Backend::CUDA, ScalarType::Half);
    if (grad_bias.defined()) {
        grad_bias.resize_({ weight.size(0) });
        grad_bias.zero_();
    }
    if (grad_input_) THNN_CudaHalfSpatialDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
    if (grad_weight_ || grad_bias_) THNN_CudaHalfSpatialDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], 1);
    if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
    return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::thnn_conv_dilated2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntList kernel_size, IntList stride, IntList padding, IntList dilation, const Tensor & columns, const Tensor & ones, std::array<bool,3> output_mask) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
    auto stride_ = check_intlist<2>(stride, "stride", 5);
    auto padding_ = check_intlist<2>(padding, "padding", 6);
    auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
    auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Half);
    auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
    auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
    if (grad_weight.defined()) {
        grad_weight.resize_(weight.sizes());
        grad_weight.zero_();
    }
    auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
    if (grad_bias.defined()) {
        grad_bias.resize_({ weight.size(0) });
        grad_bias.zero_();
    }
    if (grad_input_) THNN_CudaHalfSpatialDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
    if (grad_weight_ || grad_bias_) THNN_CudaHalfSpatialDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], 1);
    if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
    return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAHalfType::thnn_conv_dilated3d_forward_out(Tensor & output, Tensor & columns, Tensor & ones, const Tensor & self, const Tensor & weight, IntList kernel_size, const Tensor & bias, IntList stride, IntList padding, IntList dilation) const {
    const DeviceGuard device_guard(output);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
    auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
    auto stride_ = check_intlist<3>(stride, "stride", 5);
    auto padding_ = check_intlist<3>(padding, "padding", 6);
    auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
    auto output_ = checked_tensor_unwrap(output,"output",7, false, Backend::CUDA, ScalarType::Half);
    auto columns_ = checked_tensor_unwrap(columns,"columns",7, false, Backend::CUDA, ScalarType::Half);
    auto ones_ = checked_tensor_unwrap(ones,"ones",7, false, Backend::CUDA, ScalarType::Half);
    THNN_CudaHalfVolumetricDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
    bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
    output_->maybe_zero_dim(maybe_scalar);
    columns_->maybe_zero_dim(maybe_scalar);
    ones_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor &, Tensor &, Tensor &>(output, columns, ones);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::thnn_conv_dilated3d_forward(const Tensor & self, const Tensor & weight, IntList kernel_size, const Tensor & bias, IntList stride, IntList padding, IntList dilation) const {
    const DeviceGuard device_guard(self);
    auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
    auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CUDA, ScalarType::Half);
    auto stride_ = check_intlist<3>(stride, "stride", 5);
    auto padding_ = check_intlist<3>(padding, "padding", 6);
    auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
    auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
    auto columns_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto columns = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(columns_));
    auto ones_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release();
    auto ones = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(ones_));
    THNN_CudaHalfVolumetricDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
    bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
    output_->maybe_zero_dim(maybe_scalar);
    columns_->maybe_zero_dim(maybe_scalar);
    ones_->maybe_zero_dim(maybe_scalar);
    return std::tuple<Tensor, Tensor, Tensor>(output, columns, ones);
}
std::tuple<Tensor &,Tensor &,Tensor &> CUDAHalfType::thnn_conv_dilated3d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntList kernel_size, IntList stride, IntList padding, IntList dilation, const Tensor & columns, const Tensor & ones) const {
    const DeviceGuard device_guard(grad_input);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
    auto stride_ = check_intlist<3>(stride, "stride", 5);
    auto padding_ = check_intlist<3>(padding, "padding", 6);
    auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
    auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Half);
    auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",9, true, Backend::CUDA, ScalarType::Half);
    auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",9, true, Backend::CUDA, ScalarType::Half);
    if (grad_weight.defined()) {
        grad_weight.resize_(weight.sizes());
        grad_weight.zero_();
    }
    auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",9, true, Backend::CUDA, ScalarType::Half);
    if (grad_bias.defined()) {
        grad_bias.resize_({ weight.size(0) });
        grad_bias.zero_();
    }
    if (grad_input_) THNN_CudaHalfVolumetricDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
    if (grad_weight_ || grad_bias_) THNN_CudaHalfVolumetricDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], 1);
    if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
    return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::thnn_conv_dilated3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntList kernel_size, IntList stride, IntList padding, IntList dilation, const Tensor & columns, const Tensor & ones, std::array<bool,3> output_mask) const {
    const DeviceGuard device_guard(grad_output);
    auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CUDA, ScalarType::Half);
    auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CUDA, ScalarType::Half);
    auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CUDA, ScalarType::Half);
    auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
    auto stride_ = check_intlist<3>(stride, "stride", 5);
    auto padding_ = check_intlist<3>(padding, "padding", 6);
    auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
    auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CUDA, ScalarType::Half);
    auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CUDA, ScalarType::Half);
    auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
    auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
    if (grad_weight.defined()) {
        grad_weight.resize_(weight.sizes());
        grad_weight.zero_();
    }
    auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(CUDATensorId(), caffe2::TypeMeta::Make<Half>(), allocator(), false).release() : nullptr;
    auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
    if (grad_bias.defined()) {
        grad_bias.resize_({ weight.size(0) });
        grad_bias.zero_();
    }
    if (grad_input_) THNN_CudaHalfVolumetricDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
    if (grad_weight_ || grad_bias_) THNN_CudaHalfVolumetricDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], 1);
    if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
    return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
}
std::tuple<Tensor,Tensor> CUDAHalfType::_cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntList input_lengths, IntList target_lengths, int64_t blank, bool deterministic) const {
    const DeviceGuard device_guard(log_probs);
    return at::native::_cudnn_ctc_loss(/* actuals */ log_probs, targets, input_lengths, target_lengths, blank, deterministic);
}
Tensor CUDAHalfType::_cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const {
    const DeviceGuard device_guard(weight_arr);
    return at::native::_cudnn_rnn_flatten_weight(/* actuals */ weight_arr, weight_stride0, input_size, mode, hidden_size, num_layers, batch_first, bidirectional);
}
std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CUDAHalfType::_cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntList batch_sizes, const Tensor & dropout_state) const {
    const DeviceGuard device_guard(input);
    return at::native::_cudnn_rnn(/* actuals */ input, weight, weight_stride0, weight_buf, hx, cx, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state);
}
std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> CUDAHalfType::_cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntList batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const {
    const DeviceGuard device_guard(input);
    return at::native::_cudnn_rnn_backward(/* actuals */ input, weight, weight_stride0, weight_buf, hx, cx, output, grad_output, grad_hy, grad_cy, mode, hidden_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state, reserve, output_mask);
}
Tensor CUDAHalfType::_cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::_cudnn_init_dropout_state(/* actuals */ dropout, train, dropout_seed, options);
}
std::tuple<Tensor,Tensor> CUDAHalfType::_fused_dropout(const Tensor & self, double p, Generator * generator) const {
    const DeviceGuard device_guard(self);
    return at::native::fused_dropout_cuda(/* actuals */ self, p, generator);
}
Tensor CUDAHalfType::_masked_scale(const Tensor & self, const Tensor & mask, double scale) const {
    const DeviceGuard device_guard(self);
    return at::native::masked_scale_cuda(/* actuals */ self, mask, scale);
}
Tensor & CUDAHalfType::abs_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_abs__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::abs_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_abs_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::acos_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_acos__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::acos_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_acos_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::asin_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_asin__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::asin_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_asin_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::atan_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_atan__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::atan_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_atan_out_cuda(/* actuals */ result, self);
}
Tensor CUDAHalfType::baddbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(self);
    return at::native::baddbmm_cuda(/* actuals */ self, batch1, batch2, beta, alpha);
}
Tensor & CUDAHalfType::baddbmm_(Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(self);
    return at::native::baddbmm__cuda(/* actuals */ self, batch1, batch2, beta, alpha);
}
Tensor & CUDAHalfType::baddbmm_out(Tensor & result, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(result);
    return at::native::baddbmm_out_cuda(/* actuals */ result, self, batch1, batch2, beta, alpha);
}
Tensor & CUDAHalfType::bernoulli_(Tensor & self, const Tensor & p, Generator * generator) const {
    const DeviceGuard device_guard(self);
    return at::native::bernoulli_tensor_cuda_(/* actuals */ self, p, generator);
}
Tensor & CUDAHalfType::bernoulli_(Tensor & self, double p, Generator * generator) const {
    const DeviceGuard device_guard(self);
    return at::native::bernoulli_scalar_cuda_(/* actuals */ self, p, generator);
}
Tensor CUDAHalfType::bincount(const Tensor & self, const Tensor & weights, int64_t minlength) const {
    const DeviceGuard device_guard(self);
    return at::native::_bincount_cuda(/* actuals */ self, weights, minlength);
}
Tensor CUDAHalfType::bmm(const Tensor & self, const Tensor & mat2) const {
    const DeviceGuard device_guard(self);
    return at::native::bmm_cuda(/* actuals */ self, mat2);
}
Tensor & CUDAHalfType::bmm_out(Tensor & result, const Tensor & self, const Tensor & mat2) const {
    const DeviceGuard device_guard(result);
    return at::native::bmm_out_cuda(/* actuals */ result, self, mat2);
}
Tensor & CUDAHalfType::ceil_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_ceil__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::ceil_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_ceil_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::clamp_(Tensor & self, c10::optional<Scalar> min, c10::optional<Scalar> max) const {
    const DeviceGuard device_guard(self);
    return at::native::_clamp__cuda(/* actuals */ self, min, max);
}
Tensor & CUDAHalfType::clamp_out(Tensor & result, const Tensor & self, c10::optional<Scalar> min, c10::optional<Scalar> max) const {
    const DeviceGuard device_guard(result);
    return at::native::_clamp_out_cuda(/* actuals */ result, self, min, max);
}
Tensor & CUDAHalfType::clamp_max_(Tensor & self, Scalar max) const {
    const DeviceGuard device_guard(self);
    return at::native::_clamp_max__cuda(/* actuals */ self, max);
}
Tensor & CUDAHalfType::clamp_max_out(Tensor & result, const Tensor & self, Scalar max) const {
    const DeviceGuard device_guard(result);
    return at::native::_clamp_max_out_cuda(/* actuals */ result, self, max);
}
Tensor & CUDAHalfType::clamp_min_(Tensor & self, Scalar min) const {
    const DeviceGuard device_guard(self);
    return at::native::_clamp_min__cuda(/* actuals */ self, min);
}
Tensor & CUDAHalfType::clamp_min_out(Tensor & result, const Tensor & self, Scalar min) const {
    const DeviceGuard device_guard(result);
    return at::native::_clamp_min_out_cuda(/* actuals */ result, self, min);
}
Tensor & CUDAHalfType::cos_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_cos__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::cos_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_cos_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::cosh_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_cosh__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::cosh_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_cosh_out_cuda(/* actuals */ result, self);
}
Tensor CUDAHalfType::cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const {
    const DeviceGuard device_guard(theta);
    return at::native::cudnn_affine_grid_generator_forward(/* actuals */ theta, N, C, H, W);
}
Tensor CUDAHalfType::cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const {
    const DeviceGuard device_guard(grad);
    return at::native::cudnn_affine_grid_generator_backward(/* actuals */ grad, N, C, H, W);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
    const DeviceGuard device_guard(input);
    return at::native::cudnn_batch_norm(/* actuals */ input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
    const DeviceGuard device_guard(input);
    return at::native::cudnn_batch_norm_backward(/* actuals */ input, grad_output, weight, running_mean, running_var, save_mean, save_var, epsilon);
}
Tensor CUDAHalfType::cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const DeviceGuard device_guard(self);
    return at::native::cudnn_convolution(/* actuals */ self, weight, bias, padding, stride, dilation, groups, benchmark, deterministic);
}
Tensor CUDAHalfType::cudnn_convolution_backward_input(IntList self_size, const Tensor & grad_output, const Tensor & weight, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::cudnn_convolution_backward_input(/* actuals */ self_size, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    const DeviceGuard device_guard(self);
    return at::native::cudnn_convolution_backward(/* actuals */ self, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, output_mask);
}
Tensor CUDAHalfType::cudnn_convolution_backward_bias(const Tensor & grad_output) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::cudnn_convolution_backward_bias(/* actuals */ grad_output);
}
Tensor CUDAHalfType::cudnn_convolution_backward_weight(IntList weight_size, const Tensor & grad_output, const Tensor & self, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::cudnn_convolution_backward_weight(/* actuals */ weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic);
}
Tensor CUDAHalfType::cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntList padding, IntList output_padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const DeviceGuard device_guard(self);
    return at::native::cudnn_convolution_transpose(/* actuals */ self, weight, bias, padding, output_padding, stride, dilation, groups, benchmark, deterministic);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntList padding, IntList output_padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    const DeviceGuard device_guard(self);
    return at::native::cudnn_convolution_transpose_backward(/* actuals */ self, grad_output, weight, padding, output_padding, stride, dilation, groups, benchmark, deterministic, output_mask);
}
Tensor CUDAHalfType::cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::cudnn_convolution_backward_bias(/* actuals */ grad_output);
}
Tensor CUDAHalfType::cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::cudnn_convolution_transpose_backward_input(/* actuals */ grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
}
Tensor CUDAHalfType::cudnn_convolution_transpose_backward_weight(IntList weight_size, const Tensor & grad_output, const Tensor & self, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::cudnn_convolution_transpose_backward_weight(/* actuals */ weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic);
}
Tensor CUDAHalfType::cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const {
    const DeviceGuard device_guard(self);
    return at::native::cudnn_grid_sampler_forward(/* actuals */ self, grid);
}
std::tuple<Tensor,Tensor> CUDAHalfType::cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const {
    const DeviceGuard device_guard(self);
    return at::native::cudnn_grid_sampler_backward(/* actuals */ self, grid, grad_output);
}
std::tuple<Tensor,Tensor> CUDAHalfType::_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntList input_lengths, IntList target_lengths, int64_t blank) const {
    const DeviceGuard device_guard(log_probs);
    return at::native::ctc_loss_gpu(/* actuals */ log_probs, targets, input_lengths, target_lengths, blank);
}
Tensor CUDAHalfType::_ctc_loss_backward(const Tensor & grad, const Tensor & log_probs, const Tensor & targets, IntList input_lengths, IntList target_lengths, const Tensor & neg_log_likelihood, const Tensor & log_alpha, int64_t blank) const {
    const DeviceGuard device_guard(grad);
    return at::native::ctc_loss_backward_gpu(/* actuals */ grad, log_probs, targets, input_lengths, target_lengths, neg_log_likelihood, log_alpha, blank);
}
Tensor CUDAHalfType::embedding_dense_backward(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) const {
    const DeviceGuard device_guard(grad);
    return at::native::embedding_dense_backward_cuda(/* actuals */ grad, indices, num_weights, padding_idx, scale_grad_by_freq);
}
Tensor & CUDAHalfType::embedding_renorm_(Tensor & self, const Tensor & indices, double max_norm, double norm_type) const {
    const DeviceGuard device_guard(self);
    return at::native::embedding_renorm_cuda_(/* actuals */ self, indices, max_norm, norm_type);
}
std::tuple<Tensor,Tensor,Tensor,Tensor> CUDAHalfType::_embedding_bag(const Tensor & weight, const Tensor & indices, const Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse) const {
    const DeviceGuard device_guard(weight);
    return at::native::_embedding_bag_cuda(/* actuals */ weight, indices, offsets, scale_grad_by_freq, mode, sparse);
}
Tensor CUDAHalfType::_embedding_bag_dense_backward(const Tensor & grad, const Tensor & indices, const Tensor & offsets, const Tensor & offset2bag, const Tensor & bag_size, const Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode) const {
    const DeviceGuard device_guard(grad);
    return at::native::_embedding_bag_dense_backward_cuda(/* actuals */ grad, indices, offsets, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode);
}
Tensor CUDAHalfType::empty(IntList size, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::empty_cuda(/* actuals */ size, options);
}
Tensor & CUDAHalfType::resize_(Tensor & self, IntList size) const {
    // DeviceGuard omitted
    return at::native::resize_cuda_(/* actuals */ self, size);
}
Tensor & CUDAHalfType::erf_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_erf__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::erf_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_erf_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::erfc_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_erfc__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::erfc_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_erfc_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::exp_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_exp__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::exp_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_exp_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::expm1_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_expm1__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::expm1_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_expm1_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::eye_out(Tensor & result, int64_t n) const {
    const DeviceGuard device_guard(result);
    return at::native::eye_out_cuda(/* actuals */ result, n);
}
Tensor & CUDAHalfType::eye_out(Tensor & result, int64_t n, int64_t m) const {
    const DeviceGuard device_guard(result);
    return at::native::eye_out_cuda(/* actuals */ result, n, m);
}
Tensor & CUDAHalfType::floor_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_floor__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::floor_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_floor_out_cuda(/* actuals */ result, self);
}
Tensor CUDAHalfType::grid_sampler_2d(const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const DeviceGuard device_guard(input);
    return at::native::grid_sampler_2d_cuda(/* actuals */ input, grid, interpolation_mode, padding_mode);
}
std::tuple<Tensor,Tensor> CUDAHalfType::grid_sampler_2d_backward(const Tensor & grad_output, const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::grid_sampler_2d_backward_cuda(/* actuals */ grad_output, input, grid, interpolation_mode, padding_mode);
}
Tensor CUDAHalfType::grid_sampler_3d(const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const DeviceGuard device_guard(input);
    return at::native::grid_sampler_3d_cuda(/* actuals */ input, grid, interpolation_mode, padding_mode);
}
std::tuple<Tensor,Tensor> CUDAHalfType::grid_sampler_3d_backward(const Tensor & grad_output, const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::grid_sampler_3d_backward_cuda(/* actuals */ grad_output, input, grid, interpolation_mode, padding_mode);
}
std::tuple<Tensor,Tensor> CUDAHalfType::_gesv_helper(const Tensor & self, const Tensor & A) const {
    const DeviceGuard device_guard(self);
    return at::native::_gesv_helper_cuda(/* actuals */ self, A);
}
Tensor CUDAHalfType::_fft_with_size(const Tensor & self, int64_t signal_ndim, bool complex_input, bool complex_output, bool inverse, IntList checked_signal_sizes, bool normalized, bool onesided, IntList output_sizes) const {
    const DeviceGuard device_guard(self);
    return at::native::_fft_cufft(/* actuals */ self, signal_ndim, complex_input, complex_output, inverse, checked_signal_sizes, normalized, onesided, output_sizes);
}
Tensor CUDAHalfType::kl_div_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::kl_div_backward_cuda(/* actuals */ grad_output, self, target, reduction);
}
Tensor & CUDAHalfType::log_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_log__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::log_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_log_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::log10_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_log10__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::log10_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_log10_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::log1p_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_log1p__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::log1p_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_log1p_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::log2_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_log2__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::log2_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_log2_out_cuda(/* actuals */ result, self);
}
Tensor CUDAHalfType::_log_softmax(const Tensor & self, int64_t dim, bool half_to_float) const {
    const DeviceGuard device_guard(self);
    return at::native::log_softmax_cuda(/* actuals */ self, dim, half_to_float);
}
Tensor CUDAHalfType::_log_softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::log_softmax_backward_cuda(/* actuals */ grad_output, output, dim, self);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
    const DeviceGuard device_guard(input);
    return at::native::miopen_batch_norm(/* actuals */ input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
    const DeviceGuard device_guard(input);
    return at::native::miopen_batch_norm_backward(/* actuals */ input, grad_output, weight, running_mean, running_var, save_mean, save_var, epsilon);
}
Tensor CUDAHalfType::miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const DeviceGuard device_guard(self);
    return at::native::miopen_convolution(/* actuals */ self, weight, bias, padding, stride, dilation, groups, benchmark, deterministic);
}
Tensor CUDAHalfType::miopen_convolution_backward_input(IntList self_size, const Tensor & grad_output, const Tensor & weight, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::miopen_convolution_backward_input(/* actuals */ self_size, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    const DeviceGuard device_guard(self);
    return at::native::miopen_convolution_backward(/* actuals */ self, grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic, output_mask);
}
Tensor CUDAHalfType::miopen_convolution_backward_bias(const Tensor & grad_output) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::miopen_convolution_backward_bias(/* actuals */ grad_output);
}
Tensor CUDAHalfType::miopen_convolution_backward_weight(IntList weight_size, const Tensor & grad_output, const Tensor & self, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::miopen_convolution_backward_weight(/* actuals */ weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic);
}
Tensor CUDAHalfType::miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntList padding, IntList output_padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const DeviceGuard device_guard(self);
    return at::native::miopen_convolution_transpose(/* actuals */ self, weight, bias, padding, output_padding, stride, dilation, groups, benchmark, deterministic);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntList padding, IntList output_padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    const DeviceGuard device_guard(self);
    return at::native::miopen_convolution_transpose_backward(/* actuals */ self, grad_output, weight, padding, output_padding, stride, dilation, groups, benchmark, deterministic, output_mask);
}
Tensor CUDAHalfType::miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::miopen_convolution_transpose_backward_input(/* actuals */ grad_output, weight, padding, stride, dilation, groups, benchmark, deterministic);
}
Tensor CUDAHalfType::miopen_convolution_transpose_backward_weight(IntList weight_size, const Tensor & grad_output, const Tensor & self, IntList padding, IntList stride, IntList dilation, int64_t groups, bool benchmark, bool deterministic) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::miopen_convolution_transpose_backward_weight(/* actuals */ weight_size, grad_output, self, padding, stride, dilation, groups, benchmark, deterministic);
}
Tensor CUDAHalfType::narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const {
    const DeviceGuard device_guard(self);
    return at::native::narrow_copy_dense(/* actuals */ self, dim, start, length);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::native_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps) const {
    const DeviceGuard device_guard(input);
    return at::native::batch_norm_cuda(/* actuals */ input, weight, bias, running_mean, running_var, training, momentum, eps);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const {
    const DeviceGuard device_guard(grad_out);
    return at::native::batch_norm_backward_cuda(/* actuals */ grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
}
Tensor & CUDAHalfType::randperm_out(Tensor & result, int64_t n, Generator * generator) const {
    const DeviceGuard device_guard(result);
    return at::native::randperm_out_cuda(/* actuals */ result, n, generator);
}
std::tuple<Tensor,Tensor> CUDAHalfType::RoiPooling2d_forward(const Tensor & input, const Tensor & rois, int64_t pooledHeight, int64_t pooledWidth, double spatialScale) const {
    const DeviceGuard device_guard(input);
    return at::native::RoiPooling2d_forward_cuda(/* actuals */ input, rois, pooledHeight, pooledWidth, spatialScale);
}
Tensor CUDAHalfType::RoiPooling2d_backward(const Tensor & input, const Tensor & rois, int64_t pooledHeight, int64_t pooledWidth, double spatialScale, const Tensor & gradOutput, const Tensor & argmaxes) const {
    const DeviceGuard device_guard(input);
    return at::native::RoiPooling2d_backward_cuda(/* actuals */ input, rois, pooledHeight, pooledWidth, spatialScale, gradOutput, argmaxes);
}
Tensor & CUDAHalfType::round_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_round__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::round_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_round_out_cuda(/* actuals */ result, self);
}
Tensor CUDAHalfType::prelu(const Tensor & self, const Tensor & weight) const {
    const DeviceGuard device_guard(self);
    return at::native::prelu_cuda(/* actuals */ self, weight);
}
std::tuple<Tensor,Tensor> CUDAHalfType::prelu_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::prelu_backward_cuda(/* actuals */ grad_output, self, weight);
}
Tensor CUDAHalfType::hardshrink(const Tensor & self, Scalar lambd) const {
    const DeviceGuard device_guard(self);
    return at::native::hardshrink_cuda(/* actuals */ self, lambd);
}
Tensor CUDAHalfType::hardshrink_backward(const Tensor & grad_out, const Tensor & self, Scalar lambd) const {
    const DeviceGuard device_guard(grad_out);
    return at::native::hardshrink_backward_cuda(/* actuals */ grad_out, self, lambd);
}
Tensor & CUDAHalfType::rsqrt_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_rsqrt__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::rsqrt_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_rsqrt_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::sigmoid_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_sigmoid__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::sigmoid_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_sigmoid_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::sin_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_sin__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::sin_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_sin_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::sinh_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_sinh__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::sinh_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_sinh_out_cuda(/* actuals */ result, self);
}
Tensor CUDAHalfType::_softmax(const Tensor & self, int64_t dim, bool half_to_float) const {
    const DeviceGuard device_guard(self);
    return at::native::softmax_cuda(/* actuals */ self, dim, half_to_float);
}
Tensor CUDAHalfType::_softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const {
    const DeviceGuard device_guard(grad_output);
    return at::native::softmax_backward_cuda(/* actuals */ grad_output, output, dim, self);
}
Tensor & CUDAHalfType::_sparse_add_out(Tensor & result, const Tensor & self, const Tensor & other, Scalar alpha) const {
    AT_ERROR("_sparse_add_out not supported on CUDAHalfType");
}
Tensor & CUDAHalfType::_sparse_dense_add_out(Tensor & result, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
    const DeviceGuard device_guard(result);
    return at::native::add_out_dense_sparse_cuda(/* actuals */ result, self, other, alpha);
}
Tensor & CUDAHalfType::_sparse_div_zerodim_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    AT_ERROR("_sparse_div_zerodim_out not supported on CUDAHalfType");
}
Tensor & CUDAHalfType::_sparse_div_scalar_out(Tensor & result, const Tensor & self, Scalar other) const {
    AT_ERROR("_sparse_div_scalar_out not supported on CUDAHalfType");
}
Tensor & CUDAHalfType::_sparse_mul_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    AT_ERROR("_sparse_mul_out not supported on CUDAHalfType");
}
Tensor & CUDAHalfType::_sparse_mul_zerodim_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    AT_ERROR("_sparse_mul_zerodim_out not supported on CUDAHalfType");
}
Tensor & CUDAHalfType::_sparse_mul_scalar_out(Tensor & result, const Tensor & self, Scalar other) const {
    AT_ERROR("_sparse_mul_scalar_out not supported on CUDAHalfType");
}
Tensor & CUDAHalfType::sspaddmm_out(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(result);
    return at::native::_sspaddmm_out_only_sparse_cuda(/* actuals */ result, self, mat1, mat2, beta, alpha);
}
Tensor & CUDAHalfType::sqrt_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_sqrt__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::sqrt_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_sqrt_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::tan_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_tan__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::tan_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_tan_out_cuda(/* actuals */ result, self);
}
Tensor & CUDAHalfType::tanh_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_tanh__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::tanh_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_tanh_out_cuda(/* actuals */ result, self);
}
Tensor CUDAHalfType::flip(const Tensor & self, IntList dims) const {
    const DeviceGuard device_guard(self);
    return at::native::flip_cuda(/* actuals */ self, dims);
}
Tensor & CUDAHalfType::trunc_(Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_trunc__cuda(/* actuals */ self);
}
Tensor & CUDAHalfType::trunc_out(Tensor & result, const Tensor & self) const {
    const DeviceGuard device_guard(result);
    return at::native::_trunc_out_cuda(/* actuals */ result, self);
}
std::tuple<Tensor,Tensor> CUDAHalfType::_unique(const Tensor & self, bool sorted, bool return_inverse) const {
    const DeviceGuard device_guard(self);
    return at::native::_unique_cuda(/* actuals */ self, sorted, return_inverse);
}
std::tuple<Tensor,Tensor> CUDAHalfType::_unique_dim(const Tensor & self, int64_t dim, bool sorted, bool return_inverse) const {
    const DeviceGuard device_guard(self);
    return at::native::_unique_dim_cuda(/* actuals */ self, dim, sorted, return_inverse);
}
Tensor CUDAHalfType::_s_where(const Tensor & condition, const Tensor & self, const Tensor & other) const {
    const DeviceGuard device_guard(self);
    return at::native::_s_where_cuda(/* actuals */ condition, self, other);
}
std::tuple<Tensor,Tensor> CUDAHalfType::_weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const {
    const DeviceGuard device_guard(v);
    return at::native::weight_norm_cuda(/* actuals */ v, g, dim);
}
std::tuple<Tensor,Tensor> CUDAHalfType::_weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const {
    const DeviceGuard device_guard(grad_w);
    return at::native::weight_norm_cuda_backward(/* actuals */ grad_w, saved_v, saved_g, saved_norms, dim);
}
Tensor CUDAHalfType::_standard_gamma_grad(const Tensor & self, const Tensor & output) const {
    const DeviceGuard device_guard(self);
    return at::native::_standard_gamma_grad_cuda(/* actuals */ self, output);
}
Tensor CUDAHalfType::_standard_gamma(const Tensor & self, Generator * generator) const {
    const DeviceGuard device_guard(self);
    return at::native::_s_gamma_cuda(/* actuals */ self, generator);
}
Tensor CUDAHalfType::poisson(const Tensor & self, Generator * generator) const {
    const DeviceGuard device_guard(self);
    return at::native::_s_poisson_cuda(/* actuals */ self, generator);
}
Tensor CUDAHalfType::native_norm(const Tensor & self, Scalar p) const {
    AT_ERROR("native_norm not supported on CUDAHalfType");
}
Tensor CUDAHalfType::native_clone(const Tensor & self) const {
    AT_ERROR("native_clone not supported on CUDAHalfType");
}
Tensor & CUDAHalfType::native_resize_as_(Tensor & self, const Tensor & the_template) const {
    AT_ERROR("native_resize_as_ not supported on CUDAHalfType");
}
Tensor & CUDAHalfType::native_pow_out(Tensor & result, const Tensor & self, Scalar exponent) const {
    AT_ERROR("native_pow_out not supported on CUDAHalfType");
}
Tensor CUDAHalfType::native_pow(const Tensor & self, Scalar exponent) const {
    AT_ERROR("native_pow not supported on CUDAHalfType");
}
Tensor & CUDAHalfType::native_zero_(Tensor & self) const {
    AT_ERROR("native_zero_ not supported on CUDAHalfType");
}
Tensor & CUDAHalfType::s_native_addmm_out(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(result);
    return at::native::s_addmm_out_sparse_dense_cuda(/* actuals */ result, self, mat1, mat2, beta, alpha);
}
Tensor CUDAHalfType::s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(self);
    return at::native::s_addmm_sparse_dense_cuda(/* actuals */ self, mat1, mat2, beta, alpha);
}
Tensor & CUDAHalfType::s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const DeviceGuard device_guard(self);
    return at::native::s_addmm_sparse_dense_cuda_(/* actuals */ self, mat1, mat2, beta, alpha);
}
Tensor CUDAHalfType::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntList size, const TensorOptions & options) const {
    AT_ERROR("_sparse_coo_tensor_with_dims not supported on CUDAHalfType");
}
Tensor CUDAHalfType::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntList size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors not supported on CUDAHalfType");
}
Tensor & CUDAHalfType::sparse_resize_(Tensor & self, IntList size, int64_t sparse_dim, int64_t dense_dim) const {
    AT_ERROR("sparse_resize_ not supported on CUDAHalfType");
}
Tensor & CUDAHalfType::sparse_resize_and_clear_(Tensor & self, IntList size, int64_t sparse_dim, int64_t dense_dim) const {
    AT_ERROR("sparse_resize_and_clear_ not supported on CUDAHalfType");
}
Tensor CUDAHalfType::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
    const DeviceGuard device_guard(self);
    return at::native::sparse_mask_cuda(/* actuals */ self, mask);
}
Tensor CUDAHalfType::to_dense(const Tensor & self) const {
    AT_ERROR("to_dense not supported on CUDAHalfType");
}
int64_t CUDAHalfType::sparse_dim(const Tensor & self) const {
    AT_ERROR("sparse_dim not supported on CUDAHalfType");
}
int64_t CUDAHalfType::dense_dim(const Tensor & self) const {
    AT_ERROR("dense_dim not supported on CUDAHalfType");
}
int64_t CUDAHalfType::_nnz(const Tensor & self) const {
    AT_ERROR("_nnz not supported on CUDAHalfType");
}
Tensor CUDAHalfType::coalesce(const Tensor & self) const {
    AT_ERROR("coalesce not supported on CUDAHalfType");
}
bool CUDAHalfType::is_coalesced(const Tensor & self) const {
    AT_ERROR("is_coalesced not supported on CUDAHalfType");
}
Tensor CUDAHalfType::_indices(const Tensor & self) const {
    AT_ERROR("_indices not supported on CUDAHalfType");
}
Tensor CUDAHalfType::_values(const Tensor & self) const {
    AT_ERROR("_values not supported on CUDAHalfType");
}
Tensor & CUDAHalfType::_coalesced_(Tensor & self, bool coalesced) const {
    AT_ERROR("_coalesced_ not supported on CUDAHalfType");
}
Tensor CUDAHalfType::indices(const Tensor & self) const {
    AT_ERROR("indices not supported on CUDAHalfType");
}
Tensor CUDAHalfType::values(const Tensor & self) const {
    AT_ERROR("values not supported on CUDAHalfType");
}
Tensor & CUDAHalfType::hspmm_out(Tensor & result, const Tensor & mat1, const Tensor & mat2) const {
    AT_ERROR("hspmm_out not supported on CUDAHalfType");
}
Tensor CUDAHalfType::hspmm(const Tensor & mat1, const Tensor & mat2) const {
    AT_ERROR("hspmm not supported on CUDAHalfType");
}
Tensor & CUDAHalfType::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
    AT_ERROR("copy_sparse_to_sparse_ not supported on CUDAHalfType");
}
Scalar CUDAHalfType::_local_scalar_dense(const Tensor & self) const {
    const DeviceGuard device_guard(self);
    return at::native::_local_scalar_dense_cuda(/* actuals */ self);
}
std::tuple<Tensor,Tensor,Tensor> CUDAHalfType::_thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const {
    const DeviceGuard device_guard(input_gates);
    return at::native::_thnn_fused_lstm_cell_cuda(/* actuals */ input_gates, hidden_gates, cx, input_bias, hidden_bias);
}
std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CUDAHalfType::_thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const {
    const DeviceGuard device_guard(grad_hy);
    return at::native::_thnn_fused_lstm_cell_backward_cuda(/* actuals */ grad_hy, grad_cy, cx, cy, workspace, has_bias);
}
std::tuple<Tensor,Tensor> CUDAHalfType::_thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const {
    const DeviceGuard device_guard(input_gates);
    return at::native::_thnn_fused_gru_cell_cuda(/* actuals */ input_gates, hidden_gates, hx, input_bias, hidden_bias);
}
std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> CUDAHalfType::_thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const {
    const DeviceGuard device_guard(grad_hy);
    return at::native::_thnn_fused_gru_cell_backward_cuda(/* actuals */ grad_hy, workspace, has_bias);
}

}
