// @generated by aten/src/ATen/copy_wrapper.py

#include "ATen/Config.h"

#include "TH/TH.h"
#undef THNN_
#include "THC/THC.h"
#include "ATen/Utils.h"
#include "ATen/CPUByteType.h"
#include "ATen/CPUCharType.h"
#include "ATen/CPUDoubleType.h"
#include "ATen/CPUFloatType.h"
#include "ATen/CPUIntType.h"
#include "ATen/CPULongType.h"
#include "ATen/CPUShortType.h"
#include "ATen/CPUHalfType.h"
#include "ATen/SparseCPUByteType.h"
#include "ATen/SparseCPUCharType.h"
#include "ATen/SparseCPUDoubleType.h"
#include "ATen/SparseCPUFloatType.h"
#include "ATen/SparseCPUIntType.h"
#include "ATen/SparseCPULongType.h"
#include "ATen/SparseCPUShortType.h"
#include "ATen/CUDAByteType.h"
#include "ATen/CUDACharType.h"
#include "ATen/CUDADoubleType.h"
#include "ATen/CUDAFloatType.h"
#include "ATen/CUDAIntType.h"
#include "ATen/CUDALongType.h"
#include "ATen/CUDAShortType.h"
#include "ATen/CUDAHalfType.h"
#include "ATen/SparseCUDAByteType.h"
#include "ATen/SparseCUDACharType.h"
#include "ATen/SparseCUDADoubleType.h"
#include "ATen/SparseCUDAFloatType.h"
#include "ATen/SparseCUDAIntType.h"
#include "ATen/SparseCUDALongType.h"
#include "ATen/SparseCUDAShortType.h"
#include "ATen/core/TensorImpl.h"

namespace at {

Tensor & CUDAByteType::s_copy_(Tensor & dst, const Tensor & src, bool non_blocking) const {
  // code generated by copy_wrapper
  checked_tensor_unwrap(dst, "dst", 0, false, Backend::CUDA, ScalarType::Byte);
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        if (non_blocking) {
            THCudaByteTensor_copyAsyncCPU(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
            break;
        }
        THCudaByteTensor_copyByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUChar:
        THCudaByteTensor_copyChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUDouble:
        THCudaByteTensor_copyDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUFloat:
        THCudaByteTensor_copyFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUInt:
        THCudaByteTensor_copyInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPULong:
        THCudaByteTensor_copyLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUShort:
        THCudaByteTensor_copyShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUHalf:
        THCudaByteTensor_copyHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAByte:
        THCudaByteTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAChar:
        THCudaByteTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDADouble:
        THCudaByteTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAFloat:
        THCudaByteTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAInt:
        THCudaByteTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDALong:
        THCudaByteTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAShort:
        THCudaByteTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAHalf:
        THCudaByteTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    default:
      AT_ERROR("copy does not support ", src.type().toString(), " to ", toString(), " copy.");
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst;
}
Tensor & CUDAByteType::_s_copy_from(const Tensor & src, Tensor & dst, bool non_blocking) const {
  // code generated by copy_wrapper
  checked_tensor_unwrap(src, "src", 0, false, Backend::CUDA, ScalarType::Byte);
  switch (dst.type().ID()) {
    case TypeID::CPUByte:
        if (non_blocking) {
            THByteTensor_copyAsyncCuda(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
            break;
        }
        THByteTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUChar:
        THCharTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUDouble:
        THDoubleTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUFloat:
        THFloatTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUInt:
        THIntTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPULong:
        THLongTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUShort:
        THShortTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUHalf:
        THHalfTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAByte:
        THCudaByteTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAChar:
        THCudaCharTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDADouble:
        THCudaDoubleTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAFloat:
        THCudaTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAInt:
        THCudaIntTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDALong:
        THCudaLongTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAShort:
        THCudaShortTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAHalf:
        THCudaHalfTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    default:
      AT_ERROR("copy does not support ", toString(), " to ", dst.type().toString(), " copy.");
      break;
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst; // NB! dst
}
Tensor & CUDACharType::s_copy_(Tensor & dst, const Tensor & src, bool non_blocking) const {
  // code generated by copy_wrapper
  checked_tensor_unwrap(dst, "dst", 0, false, Backend::CUDA, ScalarType::Char);
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THCudaCharTensor_copyByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUChar:
        if (non_blocking) {
            THCudaCharTensor_copyAsyncCPU(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
            break;
        }
        THCudaCharTensor_copyChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUDouble:
        THCudaCharTensor_copyDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUFloat:
        THCudaCharTensor_copyFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUInt:
        THCudaCharTensor_copyInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPULong:
        THCudaCharTensor_copyLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUShort:
        THCudaCharTensor_copyShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUHalf:
        THCudaCharTensor_copyHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAByte:
        THCudaCharTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAChar:
        THCudaCharTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDADouble:
        THCudaCharTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAFloat:
        THCudaCharTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAInt:
        THCudaCharTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDALong:
        THCudaCharTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAShort:
        THCudaCharTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAHalf:
        THCudaCharTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    default:
      AT_ERROR("copy does not support ", src.type().toString(), " to ", toString(), " copy.");
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst;
}
Tensor & CUDACharType::_s_copy_from(const Tensor & src, Tensor & dst, bool non_blocking) const {
  // code generated by copy_wrapper
  checked_tensor_unwrap(src, "src", 0, false, Backend::CUDA, ScalarType::Char);
  switch (dst.type().ID()) {
    case TypeID::CPUByte:
        THByteTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUChar:
        if (non_blocking) {
            THCharTensor_copyAsyncCuda(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
            break;
        }
        THCharTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUDouble:
        THDoubleTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUFloat:
        THFloatTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUInt:
        THIntTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPULong:
        THLongTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUShort:
        THShortTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUHalf:
        THHalfTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAByte:
        THCudaByteTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAChar:
        THCudaCharTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDADouble:
        THCudaDoubleTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAFloat:
        THCudaTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAInt:
        THCudaIntTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDALong:
        THCudaLongTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAShort:
        THCudaShortTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAHalf:
        THCudaHalfTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    default:
      AT_ERROR("copy does not support ", toString(), " to ", dst.type().toString(), " copy.");
      break;
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst; // NB! dst
}
Tensor & CUDADoubleType::s_copy_(Tensor & dst, const Tensor & src, bool non_blocking) const {
  // code generated by copy_wrapper
  checked_tensor_unwrap(dst, "dst", 0, false, Backend::CUDA, ScalarType::Double);
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THCudaDoubleTensor_copyByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUChar:
        THCudaDoubleTensor_copyChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUDouble:
        if (non_blocking) {
            THCudaDoubleTensor_copyAsyncCPU(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
            break;
        }
        THCudaDoubleTensor_copyDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUFloat:
        THCudaDoubleTensor_copyFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUInt:
        THCudaDoubleTensor_copyInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPULong:
        THCudaDoubleTensor_copyLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUShort:
        THCudaDoubleTensor_copyShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUHalf:
        THCudaDoubleTensor_copyHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAByte:
        THCudaDoubleTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAChar:
        THCudaDoubleTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDADouble:
        THCudaDoubleTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAFloat:
        THCudaDoubleTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAInt:
        THCudaDoubleTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDALong:
        THCudaDoubleTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAShort:
        THCudaDoubleTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAHalf:
        THCudaDoubleTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    default:
      AT_ERROR("copy does not support ", src.type().toString(), " to ", toString(), " copy.");
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst;
}
Tensor & CUDADoubleType::_s_copy_from(const Tensor & src, Tensor & dst, bool non_blocking) const {
  // code generated by copy_wrapper
  checked_tensor_unwrap(src, "src", 0, false, Backend::CUDA, ScalarType::Double);
  switch (dst.type().ID()) {
    case TypeID::CPUByte:
        THByteTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUChar:
        THCharTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUDouble:
        if (non_blocking) {
            THDoubleTensor_copyAsyncCuda(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
            break;
        }
        THDoubleTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUFloat:
        THFloatTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUInt:
        THIntTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPULong:
        THLongTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUShort:
        THShortTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUHalf:
        THHalfTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAByte:
        THCudaByteTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAChar:
        THCudaCharTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDADouble:
        THCudaDoubleTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAFloat:
        THCudaTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAInt:
        THCudaIntTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDALong:
        THCudaLongTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAShort:
        THCudaShortTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAHalf:
        THCudaHalfTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    default:
      AT_ERROR("copy does not support ", toString(), " to ", dst.type().toString(), " copy.");
      break;
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst; // NB! dst
}
Tensor & CUDAFloatType::s_copy_(Tensor & dst, const Tensor & src, bool non_blocking) const {
  // code generated by copy_wrapper
  checked_tensor_unwrap(dst, "dst", 0, false, Backend::CUDA, ScalarType::Float);
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THCudaTensor_copyByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUChar:
        THCudaTensor_copyChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUDouble:
        THCudaTensor_copyDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUFloat:
        if (non_blocking) {
            THCudaTensor_copyAsyncCPU(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
            break;
        }
        THCudaTensor_copyFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUInt:
        THCudaTensor_copyInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPULong:
        THCudaTensor_copyLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUShort:
        THCudaTensor_copyShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUHalf:
        THCudaTensor_copyHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAByte:
        THCudaTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAChar:
        THCudaTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDADouble:
        THCudaTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAFloat:
        THCudaTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAInt:
        THCudaTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDALong:
        THCudaTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAShort:
        THCudaTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAHalf:
        THCudaTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    default:
      AT_ERROR("copy does not support ", src.type().toString(), " to ", toString(), " copy.");
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst;
}
Tensor & CUDAFloatType::_s_copy_from(const Tensor & src, Tensor & dst, bool non_blocking) const {
  // code generated by copy_wrapper
  checked_tensor_unwrap(src, "src", 0, false, Backend::CUDA, ScalarType::Float);
  switch (dst.type().ID()) {
    case TypeID::CPUByte:
        THByteTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUChar:
        THCharTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUDouble:
        THDoubleTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUFloat:
        if (non_blocking) {
            THFloatTensor_copyAsyncCuda(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
            break;
        }
        THFloatTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUInt:
        THIntTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPULong:
        THLongTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUShort:
        THShortTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUHalf:
        THHalfTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAByte:
        THCudaByteTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAChar:
        THCudaCharTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDADouble:
        THCudaDoubleTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAFloat:
        THCudaTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAInt:
        THCudaIntTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDALong:
        THCudaLongTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAShort:
        THCudaShortTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAHalf:
        THCudaHalfTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    default:
      AT_ERROR("copy does not support ", toString(), " to ", dst.type().toString(), " copy.");
      break;
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst; // NB! dst
}
Tensor & CUDAIntType::s_copy_(Tensor & dst, const Tensor & src, bool non_blocking) const {
  // code generated by copy_wrapper
  checked_tensor_unwrap(dst, "dst", 0, false, Backend::CUDA, ScalarType::Int);
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THCudaIntTensor_copyByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUChar:
        THCudaIntTensor_copyChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUDouble:
        THCudaIntTensor_copyDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUFloat:
        THCudaIntTensor_copyFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUInt:
        if (non_blocking) {
            THCudaIntTensor_copyAsyncCPU(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
            break;
        }
        THCudaIntTensor_copyInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPULong:
        THCudaIntTensor_copyLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUShort:
        THCudaIntTensor_copyShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUHalf:
        THCudaIntTensor_copyHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAByte:
        THCudaIntTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAChar:
        THCudaIntTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDADouble:
        THCudaIntTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAFloat:
        THCudaIntTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAInt:
        THCudaIntTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDALong:
        THCudaIntTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAShort:
        THCudaIntTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAHalf:
        THCudaIntTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    default:
      AT_ERROR("copy does not support ", src.type().toString(), " to ", toString(), " copy.");
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst;
}
Tensor & CUDAIntType::_s_copy_from(const Tensor & src, Tensor & dst, bool non_blocking) const {
  // code generated by copy_wrapper
  checked_tensor_unwrap(src, "src", 0, false, Backend::CUDA, ScalarType::Int);
  switch (dst.type().ID()) {
    case TypeID::CPUByte:
        THByteTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUChar:
        THCharTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUDouble:
        THDoubleTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUFloat:
        THFloatTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUInt:
        if (non_blocking) {
            THIntTensor_copyAsyncCuda(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
            break;
        }
        THIntTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPULong:
        THLongTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUShort:
        THShortTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUHalf:
        THHalfTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAByte:
        THCudaByteTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAChar:
        THCudaCharTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDADouble:
        THCudaDoubleTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAFloat:
        THCudaTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAInt:
        THCudaIntTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDALong:
        THCudaLongTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAShort:
        THCudaShortTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAHalf:
        THCudaHalfTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    default:
      AT_ERROR("copy does not support ", toString(), " to ", dst.type().toString(), " copy.");
      break;
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst; // NB! dst
}
Tensor & CUDALongType::s_copy_(Tensor & dst, const Tensor & src, bool non_blocking) const {
  // code generated by copy_wrapper
  checked_tensor_unwrap(dst, "dst", 0, false, Backend::CUDA, ScalarType::Long);
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THCudaLongTensor_copyByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUChar:
        THCudaLongTensor_copyChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUDouble:
        THCudaLongTensor_copyDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUFloat:
        THCudaLongTensor_copyFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUInt:
        THCudaLongTensor_copyInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPULong:
        if (non_blocking) {
            THCudaLongTensor_copyAsyncCPU(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
            break;
        }
        THCudaLongTensor_copyLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUShort:
        THCudaLongTensor_copyShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUHalf:
        THCudaLongTensor_copyHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAByte:
        THCudaLongTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAChar:
        THCudaLongTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDADouble:
        THCudaLongTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAFloat:
        THCudaLongTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAInt:
        THCudaLongTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDALong:
        THCudaLongTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAShort:
        THCudaLongTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAHalf:
        THCudaLongTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    default:
      AT_ERROR("copy does not support ", src.type().toString(), " to ", toString(), " copy.");
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst;
}
Tensor & CUDALongType::_s_copy_from(const Tensor & src, Tensor & dst, bool non_blocking) const {
  // code generated by copy_wrapper
  checked_tensor_unwrap(src, "src", 0, false, Backend::CUDA, ScalarType::Long);
  switch (dst.type().ID()) {
    case TypeID::CPUByte:
        THByteTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUChar:
        THCharTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUDouble:
        THDoubleTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUFloat:
        THFloatTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUInt:
        THIntTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPULong:
        if (non_blocking) {
            THLongTensor_copyAsyncCuda(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
            break;
        }
        THLongTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUShort:
        THShortTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUHalf:
        THHalfTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAByte:
        THCudaByteTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAChar:
        THCudaCharTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDADouble:
        THCudaDoubleTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAFloat:
        THCudaTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAInt:
        THCudaIntTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDALong:
        THCudaLongTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAShort:
        THCudaShortTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAHalf:
        THCudaHalfTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    default:
      AT_ERROR("copy does not support ", toString(), " to ", dst.type().toString(), " copy.");
      break;
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst; // NB! dst
}
Tensor & CUDAShortType::s_copy_(Tensor & dst, const Tensor & src, bool non_blocking) const {
  // code generated by copy_wrapper
  checked_tensor_unwrap(dst, "dst", 0, false, Backend::CUDA, ScalarType::Short);
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THCudaShortTensor_copyByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUChar:
        THCudaShortTensor_copyChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUDouble:
        THCudaShortTensor_copyDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUFloat:
        THCudaShortTensor_copyFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUInt:
        THCudaShortTensor_copyInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPULong:
        THCudaShortTensor_copyLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUShort:
        if (non_blocking) {
            THCudaShortTensor_copyAsyncCPU(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
            break;
        }
        THCudaShortTensor_copyShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUHalf:
        THCudaShortTensor_copyHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAByte:
        THCudaShortTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAChar:
        THCudaShortTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDADouble:
        THCudaShortTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAFloat:
        THCudaShortTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAInt:
        THCudaShortTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDALong:
        THCudaShortTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAShort:
        THCudaShortTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAHalf:
        THCudaShortTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    default:
      AT_ERROR("copy does not support ", src.type().toString(), " to ", toString(), " copy.");
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst;
}
Tensor & CUDAShortType::_s_copy_from(const Tensor & src, Tensor & dst, bool non_blocking) const {
  // code generated by copy_wrapper
  checked_tensor_unwrap(src, "src", 0, false, Backend::CUDA, ScalarType::Short);
  switch (dst.type().ID()) {
    case TypeID::CPUByte:
        THByteTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUChar:
        THCharTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUDouble:
        THDoubleTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUFloat:
        THFloatTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUInt:
        THIntTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPULong:
        THLongTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUShort:
        if (non_blocking) {
            THShortTensor_copyAsyncCuda(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
            break;
        }
        THShortTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUHalf:
        THHalfTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAByte:
        THCudaByteTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAChar:
        THCudaCharTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDADouble:
        THCudaDoubleTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAFloat:
        THCudaTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAInt:
        THCudaIntTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDALong:
        THCudaLongTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAShort:
        THCudaShortTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAHalf:
        THCudaHalfTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    default:
      AT_ERROR("copy does not support ", toString(), " to ", dst.type().toString(), " copy.");
      break;
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst; // NB! dst
}
Tensor & CUDAHalfType::s_copy_(Tensor & dst, const Tensor & src, bool non_blocking) const {
  // code generated by copy_wrapper
  checked_tensor_unwrap(dst, "dst", 0, false, Backend::CUDA, ScalarType::Half);
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THCudaHalfTensor_copyByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUChar:
        THCudaHalfTensor_copyChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUDouble:
        THCudaHalfTensor_copyDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUFloat:
        THCudaHalfTensor_copyFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUInt:
        THCudaHalfTensor_copyInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPULong:
        THCudaHalfTensor_copyLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUShort:
        THCudaHalfTensor_copyShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUHalf:
        if (non_blocking) {
            THCudaHalfTensor_copyAsyncCPU(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
            break;
        }
        THCudaHalfTensor_copyHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAByte:
        THCudaHalfTensor_copyCudaByte(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAChar:
        THCudaHalfTensor_copyCudaChar(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDADouble:
        THCudaHalfTensor_copyCudaDouble(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAFloat:
        THCudaHalfTensor_copyCudaFloat(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAInt:
        THCudaHalfTensor_copyCudaInt(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDALong:
        THCudaHalfTensor_copyCudaLong(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAShort:
        THCudaHalfTensor_copyCudaShort(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAHalf:
        THCudaHalfTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    default:
      AT_ERROR("copy does not support ", src.type().toString(), " to ", toString(), " copy.");
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst;
}
Tensor & CUDAHalfType::_s_copy_from(const Tensor & src, Tensor & dst, bool non_blocking) const {
  // code generated by copy_wrapper
  checked_tensor_unwrap(src, "src", 0, false, Backend::CUDA, ScalarType::Half);
  switch (dst.type().ID()) {
    case TypeID::CPUByte:
        THByteTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUChar:
        THCharTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUDouble:
        THDoubleTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUFloat:
        THFloatTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUInt:
        THIntTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPULong:
        THLongTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUShort:
        THShortTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CPUHalf:
        if (non_blocking) {
            THHalfTensor_copyAsyncCuda(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
            break;
        }
        THHalfTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAByte:
        THCudaByteTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAChar:
        THCudaCharTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDADouble:
        THCudaDoubleTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAFloat:
        THCudaTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAInt:
        THCudaIntTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDALong:
        THCudaLongTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAShort:
        THCudaShortTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    case TypeID::CUDAHalf:
        THCudaHalfTensor_copyCudaHalf(globalContext().getTHCState(), dst.unsafeGetTensorImpl(), src.unsafeGetTensorImpl());
        break;
    default:
      AT_ERROR("copy does not support ", toString(), " to ", dst.type().toString(), " copy.");
      break;
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst; // NB! dst
}
Tensor & SparseCUDAByteType::s_copy_(Tensor & dst, const Tensor & src, bool non_blocking) const {
  // code generated by copy_wrapper

  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support ", src.type().toString(), " to ", toString(), " copy.");
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst;
}
Tensor & SparseCUDAByteType::_s_copy_from(const Tensor & src, Tensor & dst, bool non_blocking) const {
  // code generated by copy_wrapper

  switch (dst.type().ID()) {

    default:
      AT_ERROR("copy does not support ", toString(), " to ", dst.type().toString(), " copy.");
      break;
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst; // NB! dst
}
Tensor & SparseCUDACharType::s_copy_(Tensor & dst, const Tensor & src, bool non_blocking) const {
  // code generated by copy_wrapper

  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support ", src.type().toString(), " to ", toString(), " copy.");
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst;
}
Tensor & SparseCUDACharType::_s_copy_from(const Tensor & src, Tensor & dst, bool non_blocking) const {
  // code generated by copy_wrapper

  switch (dst.type().ID()) {

    default:
      AT_ERROR("copy does not support ", toString(), " to ", dst.type().toString(), " copy.");
      break;
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst; // NB! dst
}
Tensor & SparseCUDADoubleType::s_copy_(Tensor & dst, const Tensor & src, bool non_blocking) const {
  // code generated by copy_wrapper

  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support ", src.type().toString(), " to ", toString(), " copy.");
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst;
}
Tensor & SparseCUDADoubleType::_s_copy_from(const Tensor & src, Tensor & dst, bool non_blocking) const {
  // code generated by copy_wrapper

  switch (dst.type().ID()) {

    default:
      AT_ERROR("copy does not support ", toString(), " to ", dst.type().toString(), " copy.");
      break;
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst; // NB! dst
}
Tensor & SparseCUDAFloatType::s_copy_(Tensor & dst, const Tensor & src, bool non_blocking) const {
  // code generated by copy_wrapper

  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support ", src.type().toString(), " to ", toString(), " copy.");
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst;
}
Tensor & SparseCUDAFloatType::_s_copy_from(const Tensor & src, Tensor & dst, bool non_blocking) const {
  // code generated by copy_wrapper

  switch (dst.type().ID()) {

    default:
      AT_ERROR("copy does not support ", toString(), " to ", dst.type().toString(), " copy.");
      break;
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst; // NB! dst
}
Tensor & SparseCUDAIntType::s_copy_(Tensor & dst, const Tensor & src, bool non_blocking) const {
  // code generated by copy_wrapper

  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support ", src.type().toString(), " to ", toString(), " copy.");
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst;
}
Tensor & SparseCUDAIntType::_s_copy_from(const Tensor & src, Tensor & dst, bool non_blocking) const {
  // code generated by copy_wrapper

  switch (dst.type().ID()) {

    default:
      AT_ERROR("copy does not support ", toString(), " to ", dst.type().toString(), " copy.");
      break;
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst; // NB! dst
}
Tensor & SparseCUDALongType::s_copy_(Tensor & dst, const Tensor & src, bool non_blocking) const {
  // code generated by copy_wrapper

  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support ", src.type().toString(), " to ", toString(), " copy.");
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst;
}
Tensor & SparseCUDALongType::_s_copy_from(const Tensor & src, Tensor & dst, bool non_blocking) const {
  // code generated by copy_wrapper

  switch (dst.type().ID()) {

    default:
      AT_ERROR("copy does not support ", toString(), " to ", dst.type().toString(), " copy.");
      break;
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst; // NB! dst
}
Tensor & SparseCUDAShortType::s_copy_(Tensor & dst, const Tensor & src, bool non_blocking) const {
  // code generated by copy_wrapper

  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support ", src.type().toString(), " to ", toString(), " copy.");
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst;
}
Tensor & SparseCUDAShortType::_s_copy_from(const Tensor & src, Tensor & dst, bool non_blocking) const {
  // code generated by copy_wrapper

  switch (dst.type().ID()) {

    default:
      AT_ERROR("copy does not support ", toString(), " to ", dst.type().toString(), " copy.");
      break;
  }
  dst.unsafeGetTensorImpl()->maybe_zero_dim(src.dim() == 0);
  return dst; // NB! dst
}

}
